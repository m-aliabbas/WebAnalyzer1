{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import FireCrawlLoader\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "import threading\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator, root_validator\n",
    "# Load HTML\n",
    "from langchain_community.document_loaders import FireCrawlLoader\n",
    "from typing import List\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import os\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import List\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import  load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(dotenv_path='../src/.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url,mode='scrape',ignore_pages=[],max_pages=4):\n",
    "    if mode == 'scrape':\n",
    "        loader = FireCrawlLoader(\n",
    "        api_key=os.environ['FIRECRAWL_API_KEY'], url=url, mode=mode\n",
    "            )\n",
    "    else:\n",
    "        crawl_params = {\n",
    "                'crawlerOptions': {\n",
    "                    'excludes': [\n",
    "                        'blog/*', 'login/*', 'account/*', 'user/*', 'profile/*',\n",
    "                        'admin/*', 'dashboard/*', 'search/*', 'filter/*',\n",
    "                        'checkout/*', 'payment/*', 'cart/*', 'css/*', 'js/*',\n",
    "                        'images/*', 'assets/*', 'temp/*', 'under-construction/*',\n",
    "                        'api/*', '404', '500', 'downloads/*', 'files/*', 'pdfs/*',\n",
    "                        'archive/*', 'old/*', 'version/*', 'forum/*', 'comments/*',\n",
    "                        'reviews/*', 'external/*', 'outbound/*', 'product/*', 'shop/*',\n",
    "                        'category/*', 'promo/*', 'deals/*', 'offers/*', 'help/*',\n",
    "                        'support/*', 'news/*', 'press/*', 'events/*',\n",
    "                        'calendar/*', 'subscribe/*', 'signup/*', 'test/*', 'staging/*',\n",
    "                        'wp-admin/*', 'backend/*', 'admin-panel/*', 'management/*'\n",
    "                    ],\n",
    "                    'includes': [], # leave empty for all pages\n",
    "                    'limit': max_pages,\n",
    "                }\n",
    "            }\n",
    "\n",
    "        loader = FireCrawlLoader(\n",
    "        api_key=os.environ['FIRECRAWL_API_KEY'], url=url, mode=mode,\n",
    "        params=crawl_params\n",
    "            ) \n",
    "    docs = loader.load()\n",
    "    # print(docs)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting firecrawl-py\n",
      "  Using cached firecrawl_py-0.0.20-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: requests in /Users/ali/anaconda3/envs/graph_rag/lib/python3.10/site-packages (from firecrawl-py) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ali/anaconda3/envs/graph_rag/lib/python3.10/site-packages (from requests->firecrawl-py) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ali/anaconda3/envs/graph_rag/lib/python3.10/site-packages (from requests->firecrawl-py) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ali/anaconda3/envs/graph_rag/lib/python3.10/site-packages (from requests->firecrawl-py) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ali/anaconda3/envs/graph_rag/lib/python3.10/site-packages (from requests->firecrawl-py) (2024.7.4)\n",
      "Using cached firecrawl_py-0.0.20-py3-none-any.whl (9.5 kB)\n",
      "Installing collected packages: firecrawl-py\n",
      "Successfully installed firecrawl-py-0.0.20\n"
     ]
    }
   ],
   "source": [
    "!pip install firecrawl-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content = get_html(\"https://thelondondentalcentre.co.uk/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'By using this website, you agree to our use of cookies. We use cookies to provide you with a great experience and to help our website run effectively.\\n\\nOK\\n\\n[0](/cart)\\n\\n[](https://thelondondentalcentre.co.uk/more-services/0-ldc-finance)\\n\\nInterest-Free Finance Available\\n\\n\\n\\n[![London Dental Centre](https://thelondondentalcentre.co.uk/images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/8c5f5d2f-957b-403a-9403-9bec057add03/Web+elements+%281920+x+500+px%29.png?format=1500w)](/)\\n\\n[About Us](/about-us)\\n\\n[Treatments](/treatments)\\n\\n[Fees](/fees)\\n\\n[020 3667 7070](tel:02036677070)\\n\\n[Book Online Now](https://uk.dentalhub.online/soe/new/London%20Dental%20Centre?pid=UKLZC01)\\n\\nOpen Menu Close Menu\\n\\n[![London Dental Centre](https://thelondondentalcentre.co.uk/images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/8c5f5d2f-957b-403a-9403-9bec057add03/Web+elements+%281920+x+500+px%29.png?format=1500w)](/)\\n\\n[About Us](/about-us)\\n\\n[Treatments](/treatments)\\n\\n[Fees](/fees)\\n\\n[020 3667 7070](tel:02036677070)\\n\\n[Book Online Now](https://uk.dentalhub.online/soe/new/London%20Dental%20Centre?pid=UKLZC01)\\n\\nOpen Menu Close Menu\\n\\n[About Us](/about-us)\\n\\n[Treatments](/treatments)\\n\\n[Fees](/fees)\\n\\n[020 3667 7070](tel:02036677070)\\n\\n[Book Online Now](https://uk.dentalhub.online/soe/new/London%20Dental%20Centre?pid=UKLZC01)\\n\\n![](https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/488b16fe-93ed-4555-b77c-a7b19b994539/1.png)\\n\\nYour Trusted Dentists in the City of London\\n===========================================\\n\\nWe’re dedicated to personalised care that makes you feel valued, using world-class techniques to enhance your smile and boost your confidence.\\n\\n% buffered00:00\\n\\nPlay\\n\\nVideo is not available or format is not supported. Try a different browser.\\n\\n[Contact Us](/contact-us)\\n\\n[Book Now](https://uk.dentalhub.online/soe/new/London%20Dental%20Centre?pid=UKLZC01)\\n\\n![](https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/0d182fef-92bb-40ec-a686-c2677e3b7c94/Web+elements+%281%29.png)\\n\\nWhere **_expertise_** meets **_elegance_**.\\n-------------------------------------------\\n\\n![](https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/cb0835ec-09aa-41f8-bf47-78f3f4ca7a57/Waiting+room+%281%29.jpg)\\n\\n![](https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/4e653fd0-3db6-47bf-8095-6b0d6f9aa306/scanner+%281%29.jpg)\\n\\n![](https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/152a823f-be2b-41a2-8375-0fcdd34a626f/image.jpg)\\n\\n### **We all need a dentist.** Picking the right one can be a major decision. Here’s why **you should choose us**.\\n\\nWe are committed to providing a smooth and relaxed dental experience.\\n\\nWe offer [all types of dentistry](/treatments)\\n including general dentistry, cosmetic dentistry, Invisalign orthodontics and implants.\\n\\nBased in Islington, in the City of London, we are easily accessible. We offer flexible appointments, as well as [Emergency Dental Care](/emergency-dentist)\\n in the heart of London.\\n\\nEstablished in 2010, we provide top-quality private dentistry tailored to your needs. Using state-of-the-art technology, our modern and friendly practice in Old Street is here for you.\\n\\n109 Lever Street, Islington, EC1V 3RQ\\n\\n[We are accepting new patients](/dentist-london)\\n\\n### **Testimonials**\\n\\n![](https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/b25f190c-615f-4421-b17b-9abfe94e57bf/Untitled+%281920+x+500+px%29.png)\\n\\n“Was looking for a dentist in Old Street and so glad I found this place! They made me feel really calm and relaxed, really happy with the experience”\\n\\n![](https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/b25f190c-615f-4421-b17b-9abfe94e57bf/Untitled+%281920+x+500+px%29.png)\\n\\nFriendly, professional and genuinely cared for my wellbeing. I just moved into the neighborhood and will make this place my local dentist.\\n\\n**Zainab**\\n\\n![](https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/e2a91744-66c6-4dfe-9ce6-1fccc604404c/Z+logo.png)\\n\\n![](https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/72c59bd5-a94f-41e4-ba98-f21c6567edc4/jim.png)\\n\\n**Jim**\\n\\n![](https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/b25f190c-615f-4421-b17b-9abfe94e57bf/Untitled+%281920+x+500+px%29.png)\\n\\nIt’s been 4 months since I finished my Invisalign treatment at LDC with Dr. Leila and I am still just as happy! I would highly recommend treatment at LDC, the team are all very friendly and accommodating, thank you! :)\\n\\n**Zoe**\\n\\n![](https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/f36073ad-3706-4c08-a2e2-30b241588598/zoe.png)\\n\\n[100+ 5-Star Reviews](https://www.google.com/search?gs_ssp=eJzj4tZP1zcsyUguKjNMNmC0UjWoMLEwNzNMMrUwMU41MjIxM7YyqEgyM7RMS0tOSzI2tTA2sUz2EsnJz0vJz1NISc0rScxRSAZSRakADBIWRA&q=london+dental+centre&rlz=1C5CHFA_enGB889GB894&oq=lon&gs_lcrp=EgZjaHJvbWUqFQgCEC4YJxivARjHARiABBiKBRiOBTIGCAAQRRg5MgwIARAjGCcYgAQYigUyFQgCEC4YJxivARjHARiABBiKBRiOBTIMCAMQABhDGIAEGIoFMg0IBBAuGIMBGLEDGIAEMgwIBRAAGEMYgAQYigUyBggGEEUYQTIGCAcQRRhB0gEINDcwNWowajeoAgCwAgA&sourceid=chrome&ie=UTF-8#lrd=0x48761b5843e22463:0xb619ffcfb358349c,1,,,,)\\n\\nCosmetic\\n\\n### [**Composite Bonding**](/cosmetic-bonding)\\n\\nOur skilled dentists can help to re-shape your teeth to achieve the smile that helps you feel confident.\\n\\nCosmetic\\n\\n### [**Teeth Whitening**](/treatment/teeth-whitening)\\n\\nWe help you to achieve a brighter smile with our professional teeth whitening kits.\\n\\n![](https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/64d96c63-26af-4739-b9b9-b582c14caede/Composite+Bond+%281%29.jpg)\\n\\n![](https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/27f20f47-9548-4cb1-9f04-40d9f5141cdc/smiling+%281%29.jpg)\\n\\nCosmetic\\n\\n### [**Smile Makeover**](/cosmetic)\\n\\nSpeak to one of cosmetic dentists about how you can achieve your perfect smile with a range of treatments.\\n\\nOral Surgery\\n\\n### [**Dental Implants**](/treatment/implants)\\n\\nTalk to one of our master implantologists about dental implants and how they can restore you smile.\\n\\n![](https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/ec6df589-632a-49fc-af35-ace34a12fb7f/Scanning+Leila+%281%29.jpeg)\\n\\n![](https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/58e716fe-9c8f-4da4-969b-a37e35c4ebe4/dpa+%281%29+%281%29.png)\\n\\n[Explore all treatments](/treatments)\\n\\n[Book Online](https://uk.dentalhub.online/soe/new/London%20Dental%20Centre?pid=UKLZC01)\\n\\n![Invisalign Clear Aligners](https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/a85c15a5-4bef-4346-84f8-2b9165720059/image+%283%29.png)\\n\\n**INVISALIGN**\\n==============\\n\\n### Straighter teeth, clear aligners, no braces\\n\\n#### We offer Invisalign Full and Lite\\n\\n### **Learn more about** [**Invisalign**](/treatment/invisalign)\\n\\nFrom £2,600\\n\\n[0% finance available](/more-services/0-ldc-finance)\\n\\n![Invisalign Clear Aligners for straight teeth](https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/70be4291-c0bd-4e88-8a07-de4e219f8e4f/invis+retainers.png)\\n\\n![Cosmetic Dentist London](https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/282f11f7-aea7-4c16-9d39-b9a6496ff001/invisalign+%281%29.jpg)\\n\\n**Cosmetic Dentistry, Transforming** **Smiles**\\n-----------------------------------------------\\n\\nAt LDC, we are dedicated to going above and beyond to make you smile. Our highly skilled team has transformed thousands of smiles and continuously seeks new ways to innovate. From teeth bonding, veneers, and straightening to implants and whitening, ur cosmetic dentists are committed to giving you a smile that enhances your life.\\n\\n### **Meet Our Dentists**\\n\\n[![](https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/370aef6e-180a-4c4b-b561-fe9c8256f2cc/Dr%2BKatya+%281%29.jpg)](/dr-katya)\\n\\n[![](https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/46579ff7-4b1b-4956-931b-dd00d47adc7e/Leila+%281%29.jpeg)](/dr-leila-nebia)\\n\\nDr Leila, General & Cosmetic Dentist\\n\\nDr Katya, General & Cosmetic Dentist\\n\\n[![](https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/121e030e-9d19-4eed-b0e2-f9015b45ebd0/zola+%281%29.png)](/dr-zola-banh)\\n\\n[![](https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/b95c29e4-49e7-4284-b67f-291c05cf7ec1/Arthie+%281%29.jpg)](/drarthie)\\n\\nDr Arthie, General & Cosmetic Dentist\\n\\nDr Zola, General & Cosmetic Dentist\\n\\n[![periodontist london](https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/855ecb7c-2707-46af-91df-fa1846981d47/dr-Ioannis-plastagaris.jpeg)](/dr-ioannis-plastargias)\\n\\n[![](https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/fa07fbda-d2c7-4e23-a1b0-3b4a92fb772a/marios.jpg)](/drmarios)\\n\\nDr Marios, Oral Surgeon\\n\\nDr Ioannis, Periodontist\\n\\nAre you making the most of your **dental insurance**?\\n-----------------------------------------------------\\n\\nMost dental insurance can be used with us. Speak to a member of our team about getting the most out of your insurance.\\n\\nMaximise your oral health, minimise the costs.\\n\\n[Contact us](/contact-us)\\n\\n[Dental Insurance FAQs](/dental-insurance-how-to)\\n\\n![Unum, simplyhealth dental Insurance Accepted at London Dental Centre](https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/de6ce575-0ef3-43ce-9918-5766fe332848/6.png)\\n\\n![Axa, Bupa Insurance Accepted at London Dental Centre](https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/07a3fea6-c8ad-477e-99f0-edd2e23fc1a8/7.png)\\n\\n![Vitality Dental Insurance Accepted at London Dental Centre](https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/2ddff145-5d13-492c-9213-68bf7073b502/8.png)\\n\\n& more\\n\\n[Book Online](https://uk.dentalhub.online/soe/new/London%20Dental%20Centre?pid=UKLZC01)\\n\\n### Testimonials\\n\\nTRANSFORMING SMILES\\n-------------------\\n\\n### with COMPOSITE BONDING\\n\\n![Cosmetic Dentistry Smile Makeover](https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/c18af1db-f956-4b6b-8455-3f2bd3ab4128/IMG_2207.jpg)\\n\\n![Broken front teeth](https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/110e5798-3b6b-4050-b30a-2909d75350f0/IMG_8146.jpg)\\n\\n> ### **“**I’m so grateful for the team at London Dental Centre, they were so friendly and professional. I’ve definitely got my **confidence** back!**”**\\n\\nLawrence, 26, Engineer\\n\\n**Click here to** [**Get Directions**](https://www.google.com/maps/dir//London+Dental+Centre,+109+Lever+St,+London+EC1V+3RQ/@51.5264152,-0.0935398,17z/data=!4m9!4m8!1m0!1m5!1m1!1s0x48761b5843e22463:0xb619ffcfb358349c!2m2!1d-0.0956875!2d51.5271567!3e2?entry=ttu)\\n\\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\nNearest Tube Lines\\n\\nA 5-10 minute walk from\\n\\n[Old Street](/old-street-dentist)\\n Underground\\n\\nNorthern Line/London Overground\\n\\nA 10 minute walk from\\n\\nBarbican & Farringdon Underground\\n\\nElizabeth/Hammersmith & City/Metropolitan Lines\\n\\n![](https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/cb58337f-6832-4299-8f53-a6e3301cd843/Favicon.png)\\n\\nFollow us on Instagram:\\n\\n[](https://www.instagram.com/londondentalcentre/)\\n[](https://www.facebook.com/londondental)\\n\\n[Emergency Appointments](/emergency-dentist)\\n\\n[Hygiene Clean (Scale & Polish)](https://thelondondentalcentre.co.uk/treatment/scaling-and-polishing)\\n\\n[Invisalign Treatment](https://thelondondentalcentre.co.uk/treatment/invisalign)\\n\\n[Composite Bonding](https://thelondondentalcentre.co.uk/cosmetic-bonding)\\n\\n[Veneers](https://thelondondentalcentre.co.uk/treatment/veneers)\\n\\n[Teeth Whitening](https://thelondondentalcentre.co.uk/treatment/teeth-whitening)\\n\\n[Implants](https://thelondondentalcentre.co.uk/treatment/implants)\\n\\n[Treatments](/treatments)\\n\\n[Home](/home)\\n\\xa0 [Website T&C](/terms-and-conditions)\\n\\xa0\\xa0\\xa0[Terms of Sale](/terms-of-sale)\\n\\xa0\\xa0\\xa0[Privacy Policy](/privacy-policy)\\n\\xa0\\xa0\\xa0[Book Appointment](https://uk.dentalhub.online/soe/new/London%20Dental%20Centre?pid=UKLZC01)\\n\\xa0\\xa0\\xa0[Contact Us](/contact-us)\\n\\xa0\\xa0\\xa0[Testimonials](/testimonials)\\n [Prices/Fees](/fees)\\n [Blog](/blog)\\n\\n109 LEVER STREET, OLD STREET, ISLINGTON, LONDON, EC1V 3RQ | [020 3667 7070](tel:02036677070)\\n | [INFO@THELONDONDENTALCENTRE.CO.UK](mailto:INFO@THELONDONDENTALCENTRE.CO.UK?)\\n\\n© LONDON DENTAL CENTRE 2024 | LDMC Ltd 07156856 | [Other](/team-area)\\n | Regulated by [CQC](https://www.cqc.org.uk/location/1-220304412)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_content[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting markdown\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Installing collected packages: markdown\n",
      "Successfully installed markdown-3.7\n"
     ]
    }
   ],
   "source": [
    "# !pip install markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = markdown.markdown(html_content[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>By using this website, you agree to our use of cookies. We use cookies to provide you with a great experience and to help our website run effectively.</p>\\n<p>OK</p>\\n<p><a href=\"/cart\">0</a></p>\\n<p><a href=\"https://thelondondentalcentre.co.uk/more-services/0-ldc-finance\"></a></p>\\n<p>Interest-Free Finance Available</p>\\n<p><a href=\"/\"><img alt=\"London Dental Centre\" src=\"https://thelondondentalcentre.co.uk/images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/8c5f5d2f-957b-403a-9403-9bec057add03/Web+elements+%281920+x+500+px%29.png?format=1500w\" /></a></p>\\n<p><a href=\"/about-us\">About Us</a></p>\\n<p><a href=\"/treatments\">Treatments</a></p>\\n<p><a href=\"/fees\">Fees</a></p>\\n<p><a href=\"tel:02036677070\">020 3667 7070</a></p>\\n<p><a href=\"https://uk.dentalhub.online/soe/new/London%20Dental%20Centre?pid=UKLZC01\">Book Online Now</a></p>\\n<p>Open Menu Close Menu</p>\\n<p><a href=\"/\"><img alt=\"London Dental Centre\" src=\"https://thelondondentalcentre.co.uk/images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/8c5f5d2f-957b-403a-9403-9bec057add03/Web+elements+%281920+x+500+px%29.png?format=1500w\" /></a></p>\\n<p><a href=\"/about-us\">About Us</a></p>\\n<p><a href=\"/treatments\">Treatments</a></p>\\n<p><a href=\"/fees\">Fees</a></p>\\n<p><a href=\"tel:02036677070\">020 3667 7070</a></p>\\n<p><a href=\"https://uk.dentalhub.online/soe/new/London%20Dental%20Centre?pid=UKLZC01\">Book Online Now</a></p>\\n<p>Open Menu Close Menu</p>\\n<p><a href=\"/about-us\">About Us</a></p>\\n<p><a href=\"/treatments\">Treatments</a></p>\\n<p><a href=\"/fees\">Fees</a></p>\\n<p><a href=\"tel:02036677070\">020 3667 7070</a></p>\\n<p><a href=\"https://uk.dentalhub.online/soe/new/London%20Dental%20Centre?pid=UKLZC01\">Book Online Now</a></p>\\n<p><img alt=\"\" src=\"https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/488b16fe-93ed-4555-b77c-a7b19b994539/1.png\" /></p>\\n<h1>Your Trusted Dentists in the City of London</h1>\\n<p>We’re dedicated to personalised care that makes you feel valued, using world-class techniques to enhance your smile and boost your confidence.</p>\\n<p>% buffered00:00</p>\\n<p>Play</p>\\n<p>Video is not available or format is not supported. Try a different browser.</p>\\n<p><a href=\"/contact-us\">Contact Us</a></p>\\n<p><a href=\"https://uk.dentalhub.online/soe/new/London%20Dental%20Centre?pid=UKLZC01\">Book Now</a></p>\\n<p><img alt=\"\" src=\"https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/0d182fef-92bb-40ec-a686-c2677e3b7c94/Web+elements+%281%29.png\" /></p>\\n<h2>Where <strong><em>expertise</em></strong> meets <strong><em>elegance</em></strong>.</h2>\\n<p><img alt=\"\" src=\"https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/cb0835ec-09aa-41f8-bf47-78f3f4ca7a57/Waiting+room+%281%29.jpg\" /></p>\\n<p><img alt=\"\" src=\"https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/4e653fd0-3db6-47bf-8095-6b0d6f9aa306/scanner+%281%29.jpg\" /></p>\\n<p><img alt=\"\" src=\"https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/152a823f-be2b-41a2-8375-0fcdd34a626f/image.jpg\" /></p>\\n<h3><strong>We all need a dentist.</strong> Picking the right one can be a major decision. Here’s why <strong>you should choose us</strong>.</h3>\\n<p>We are committed to providing a smooth and relaxed dental experience.</p>\\n<p>We offer <a href=\"/treatments\">all types of dentistry</a>\\n including general dentistry, cosmetic dentistry, Invisalign orthodontics and implants.</p>\\n<p>Based in Islington, in the City of London, we are easily accessible. We offer flexible appointments, as well as <a href=\"/emergency-dentist\">Emergency Dental Care</a>\\n in the heart of London.</p>\\n<p>Established in 2010, we provide top-quality private dentistry tailored to your needs. Using state-of-the-art technology, our modern and friendly practice in Old Street is here for you.</p>\\n<p>109 Lever Street, Islington, EC1V 3RQ</p>\\n<p><a href=\"/dentist-london\">We are accepting new patients</a></p>\\n<h3><strong>Testimonials</strong></h3>\\n<p><img alt=\"\" src=\"https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/b25f190c-615f-4421-b17b-9abfe94e57bf/Untitled+%281920+x+500+px%29.png\" /></p>\\n<p>“Was looking for a dentist in Old Street and so glad I found this place! They made me feel really calm and relaxed, really happy with the experience”</p>\\n<p><img alt=\"\" src=\"https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/b25f190c-615f-4421-b17b-9abfe94e57bf/Untitled+%281920+x+500+px%29.png\" /></p>\\n<p>Friendly, professional and genuinely cared for my wellbeing. I just moved into the neighborhood and will make this place my local dentist.</p>\\n<p><strong>Zainab</strong></p>\\n<p><img alt=\"\" src=\"https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/e2a91744-66c6-4dfe-9ce6-1fccc604404c/Z+logo.png\" /></p>\\n<p><img alt=\"\" src=\"https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/72c59bd5-a94f-41e4-ba98-f21c6567edc4/jim.png\" /></p>\\n<p><strong>Jim</strong></p>\\n<p><img alt=\"\" src=\"https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/b25f190c-615f-4421-b17b-9abfe94e57bf/Untitled+%281920+x+500+px%29.png\" /></p>\\n<p>It’s been 4 months since I finished my Invisalign treatment at LDC with Dr. Leila and I am still just as happy! I would highly recommend treatment at LDC, the team are all very friendly and accommodating, thank you! :)</p>\\n<p><strong>Zoe</strong></p>\\n<p><img alt=\"\" src=\"https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/f36073ad-3706-4c08-a2e2-30b241588598/zoe.png\" /></p>\\n<p><a href=\"https://www.google.com/search?gs_ssp=eJzj4tZP1zcsyUguKjNMNmC0UjWoMLEwNzNMMrUwMU41MjIxM7YyqEgyM7RMS0tOSzI2tTA2sUz2EsnJz0vJz1NISc0rScxRSAZSRakADBIWRA&amp;q=london+dental+centre&amp;rlz=1C5CHFA_enGB889GB894&amp;oq=lon&amp;gs_lcrp=EgZjaHJvbWUqFQgCEC4YJxivARjHARiABBiKBRiOBTIGCAAQRRg5MgwIARAjGCcYgAQYigUyFQgCEC4YJxivARjHARiABBiKBRiOBTIMCAMQABhDGIAEGIoFMg0IBBAuGIMBGLEDGIAEMgwIBRAAGEMYgAQYigUyBggGEEUYQTIGCAcQRRhB0gEINDcwNWowajeoAgCwAgA&amp;sourceid=chrome&amp;ie=UTF-8#lrd=0x48761b5843e22463:0xb619ffcfb358349c,1,,,,\">100+ 5-Star Reviews</a></p>\\n<p>Cosmetic</p>\\n<h3><a href=\"/cosmetic-bonding\"><strong>Composite Bonding</strong></a></h3>\\n<p>Our skilled dentists can help to re-shape your teeth to achieve the smile that helps you feel confident.</p>\\n<p>Cosmetic</p>\\n<h3><a href=\"/treatment/teeth-whitening\"><strong>Teeth Whitening</strong></a></h3>\\n<p>We help you to achieve a brighter smile with our professional teeth whitening kits.</p>\\n<p><img alt=\"\" src=\"https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/64d96c63-26af-4739-b9b9-b582c14caede/Composite+Bond+%281%29.jpg\" /></p>\\n<p><img alt=\"\" src=\"https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/27f20f47-9548-4cb1-9f04-40d9f5141cdc/smiling+%281%29.jpg\" /></p>\\n<p>Cosmetic</p>\\n<h3><a href=\"/cosmetic\"><strong>Smile Makeover</strong></a></h3>\\n<p>Speak to one of cosmetic dentists about how you can achieve your perfect smile with a range of treatments.</p>\\n<p>Oral Surgery</p>\\n<h3><a href=\"/treatment/implants\"><strong>Dental Implants</strong></a></h3>\\n<p>Talk to one of our master implantologists about dental implants and how they can restore you smile.</p>\\n<p><img alt=\"\" src=\"https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/ec6df589-632a-49fc-af35-ace34a12fb7f/Scanning+Leila+%281%29.jpeg\" /></p>\\n<p><img alt=\"\" src=\"https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/58e716fe-9c8f-4da4-969b-a37e35c4ebe4/dpa+%281%29+%281%29.png\" /></p>\\n<p><a href=\"/treatments\">Explore all treatments</a></p>\\n<p><a href=\"https://uk.dentalhub.online/soe/new/London%20Dental%20Centre?pid=UKLZC01\">Book Online</a></p>\\n<p><img alt=\"Invisalign Clear Aligners\" src=\"https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/a85c15a5-4bef-4346-84f8-2b9165720059/image+%283%29.png\" /></p>\\n<h1><strong>INVISALIGN</strong></h1>\\n<h3>Straighter teeth, clear aligners, no braces</h3>\\n<h4>We offer Invisalign Full and Lite</h4>\\n<h3><strong>Learn more about</strong> <a href=\"/treatment/invisalign\"><strong>Invisalign</strong></a></h3>\\n<p>From £2,600</p>\\n<p><a href=\"/more-services/0-ldc-finance\">0% finance available</a></p>\\n<p><img alt=\"Invisalign Clear Aligners for straight teeth\" src=\"https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/70be4291-c0bd-4e88-8a07-de4e219f8e4f/invis+retainers.png\" /></p>\\n<p><img alt=\"Cosmetic Dentist London\" src=\"https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/282f11f7-aea7-4c16-9d39-b9a6496ff001/invisalign+%281%29.jpg\" /></p>\\n<h2><strong>Cosmetic Dentistry, Transforming</strong> <strong>Smiles</strong></h2>\\n<p>At LDC, we are dedicated to going above and beyond to make you smile. Our highly skilled team has transformed thousands of smiles and continuously seeks new ways to innovate. From teeth bonding, veneers, and straightening to implants and whitening, ur cosmetic dentists are committed to giving you a smile that enhances your life.</p>\\n<h3><strong>Meet Our Dentists</strong></h3>\\n<p><a href=\"/dr-katya\"><img alt=\"\" src=\"https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/370aef6e-180a-4c4b-b561-fe9c8256f2cc/Dr%2BKatya+%281%29.jpg\" /></a></p>\\n<p><a href=\"/dr-leila-nebia\"><img alt=\"\" src=\"https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/46579ff7-4b1b-4956-931b-dd00d47adc7e/Leila+%281%29.jpeg\" /></a></p>\\n<p>Dr Leila, General &amp; Cosmetic Dentist</p>\\n<p>Dr Katya, General &amp; Cosmetic Dentist</p>\\n<p><a href=\"/dr-zola-banh\"><img alt=\"\" src=\"https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/121e030e-9d19-4eed-b0e2-f9015b45ebd0/zola+%281%29.png\" /></a></p>\\n<p><a href=\"/drarthie\"><img alt=\"\" src=\"https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/b95c29e4-49e7-4284-b67f-291c05cf7ec1/Arthie+%281%29.jpg\" /></a></p>\\n<p>Dr Arthie, General &amp; Cosmetic Dentist</p>\\n<p>Dr Zola, General &amp; Cosmetic Dentist</p>\\n<p><a href=\"/dr-ioannis-plastargias\"><img alt=\"periodontist london\" src=\"https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/855ecb7c-2707-46af-91df-fa1846981d47/dr-Ioannis-plastagaris.jpeg\" /></a></p>\\n<p><a href=\"/drmarios\"><img alt=\"\" src=\"https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/fa07fbda-d2c7-4e23-a1b0-3b4a92fb772a/marios.jpg\" /></a></p>\\n<p>Dr Marios, Oral Surgeon</p>\\n<p>Dr Ioannis, Periodontist</p>\\n<h2>Are you making the most of your <strong>dental insurance</strong>?</h2>\\n<p>Most dental insurance can be used with us. Speak to a member of our team about getting the most out of your insurance.</p>\\n<p>Maximise your oral health, minimise the costs.</p>\\n<p><a href=\"/contact-us\">Contact us</a></p>\\n<p><a href=\"/dental-insurance-how-to\">Dental Insurance FAQs</a></p>\\n<p><img alt=\"Unum, simplyhealth dental Insurance Accepted at London Dental Centre\" src=\"https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/de6ce575-0ef3-43ce-9918-5766fe332848/6.png\" /></p>\\n<p><img alt=\"Axa, Bupa Insurance Accepted at London Dental Centre\" src=\"https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/07a3fea6-c8ad-477e-99f0-edd2e23fc1a8/7.png\" /></p>\\n<p><img alt=\"Vitality Dental Insurance Accepted at London Dental Centre\" src=\"https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/2ddff145-5d13-492c-9213-68bf7073b502/8.png\" /></p>\\n<p>&amp; more</p>\\n<p><a href=\"https://uk.dentalhub.online/soe/new/London%20Dental%20Centre?pid=UKLZC01\">Book Online</a></p>\\n<h3>Testimonials</h3>\\n<h2>TRANSFORMING SMILES</h2>\\n<h3>with COMPOSITE BONDING</h3>\\n<p><img alt=\"Cosmetic Dentistry Smile Makeover\" src=\"https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/c18af1db-f956-4b6b-8455-3f2bd3ab4128/IMG_2207.jpg\" /></p>\\n<p><img alt=\"Broken front teeth\" src=\"https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/110e5798-3b6b-4050-b30a-2909d75350f0/IMG_8146.jpg\" /></p>\\n<blockquote>\\n<h3><strong>“</strong>I’m so grateful for the team at London Dental Centre, they were so friendly and professional. I’ve definitely got my <strong>confidence</strong> back!<strong>”</strong></h3>\\n</blockquote>\\n<p>Lawrence, 26, Engineer</p>\\n<p><strong>Click here to</strong> <a href=\"https://www.google.com/maps/dir//London+Dental+Centre,+109+Lever+St,+London+EC1V+3RQ/@51.5264152,-0.0935398,17z/data=!4m9!4m8!1m0!1m5!1m1!1s0x48761b5843e22463:0xb619ffcfb358349c!2m2!1d-0.0956875!2d51.5271567!3e2?entry=ttu\"><strong>Get Directions</strong></a></p>\\n<hr />\\n<p>Nearest Tube Lines</p>\\n<p>A 5-10 minute walk from</p>\\n<p><a href=\"/old-street-dentist\">Old Street</a>\\n Underground</p>\\n<p>Northern Line/London Overground</p>\\n<p>A 10 minute walk from</p>\\n<p>Barbican &amp; Farringdon Underground</p>\\n<p>Elizabeth/Hammersmith &amp; City/Metropolitan Lines</p>\\n<p><img alt=\"\" src=\"https://images.squarespace-cdn.com/content/v1/5d7bafb2d4f5e4735f21b5cc/cb58337f-6832-4299-8f53-a6e3301cd843/Favicon.png\" /></p>\\n<p>Follow us on Instagram:</p>\\n<p><a href=\"https://www.instagram.com/londondentalcentre/\"></a>\\n<a href=\"https://www.facebook.com/londondental\"></a></p>\\n<p><a href=\"/emergency-dentist\">Emergency Appointments</a></p>\\n<p><a href=\"https://thelondondentalcentre.co.uk/treatment/scaling-and-polishing\">Hygiene Clean (Scale &amp; Polish)</a></p>\\n<p><a href=\"https://thelondondentalcentre.co.uk/treatment/invisalign\">Invisalign Treatment</a></p>\\n<p><a href=\"https://thelondondentalcentre.co.uk/cosmetic-bonding\">Composite Bonding</a></p>\\n<p><a href=\"https://thelondondentalcentre.co.uk/treatment/veneers\">Veneers</a></p>\\n<p><a href=\"https://thelondondentalcentre.co.uk/treatment/teeth-whitening\">Teeth Whitening</a></p>\\n<p><a href=\"https://thelondondentalcentre.co.uk/treatment/implants\">Implants</a></p>\\n<p><a href=\"/treatments\">Treatments</a></p>\\n<p><a href=\"/home\">Home</a>\\n\\xa0 <a href=\"/terms-and-conditions\">Website T&amp;C</a>\\n\\xa0\\xa0\\xa0<a href=\"/terms-of-sale\">Terms of Sale</a>\\n\\xa0\\xa0\\xa0<a href=\"/privacy-policy\">Privacy Policy</a>\\n\\xa0\\xa0\\xa0<a href=\"https://uk.dentalhub.online/soe/new/London%20Dental%20Centre?pid=UKLZC01\">Book Appointment</a>\\n\\xa0\\xa0\\xa0<a href=\"/contact-us\">Contact Us</a>\\n\\xa0\\xa0\\xa0<a href=\"/testimonials\">Testimonials</a>\\n <a href=\"/fees\">Prices/Fees</a>\\n <a href=\"/blog\">Blog</a></p>\\n<p>109 LEVER STREET, OLD STREET, ISLINGTON, LONDON, EC1V 3RQ | <a href=\"tel:02036677070\">020 3667 7070</a>\\n | <a href=\"mailto:INFO@THELONDONDENTALCENTRE.CO.UK?\">INFO@THELONDONDENTALCENTRE.CO.UK</a></p>\\n<p>© LONDON DENTAL CENTRE 2024 | LDMC Ltd 07156856 | <a href=\"/team-area\">Other</a>\\n | Regulated by <a href=\"https://www.cqc.org.uk/location/1-220304412\">CQC</a></p>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.12.3 soupsieve-2.6\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "By using this website, you agree to our use of cookies. We use cookies to provide you with a great experience and to help our website run effectively.\n",
      "--------------------------------------------------\n",
      "OK\n",
      "--------------------------------------------------\n",
      "0\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "Interest-Free Finance Available\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "About Us\n",
      "--------------------------------------------------\n",
      "Treatments\n",
      "--------------------------------------------------\n",
      "Fees\n",
      "--------------------------------------------------\n",
      "020 3667 7070\n",
      "--------------------------------------------------\n",
      "Book Online Now\n",
      "--------------------------------------------------\n",
      "Open Menu Close Menu\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "About Us\n",
      "--------------------------------------------------\n",
      "Treatments\n",
      "--------------------------------------------------\n",
      "Fees\n",
      "--------------------------------------------------\n",
      "020 3667 7070\n",
      "--------------------------------------------------\n",
      "Book Online Now\n",
      "--------------------------------------------------\n",
      "Open Menu Close Menu\n",
      "--------------------------------------------------\n",
      "About Us\n",
      "--------------------------------------------------\n",
      "Treatments\n",
      "--------------------------------------------------\n",
      "Fees\n",
      "--------------------------------------------------\n",
      "020 3667 7070\n",
      "--------------------------------------------------\n",
      "Book Online Now\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "Your Trusted Dentists in the City of London\n",
      "--------------------------------------------------\n",
      "We’re dedicated to personalised care that makes you feel valued, using world-class techniques to enhance your smile and boost your confidence.\n",
      "--------------------------------------------------\n",
      "% buffered00:00\n",
      "--------------------------------------------------\n",
      "Play\n",
      "--------------------------------------------------\n",
      "Video is not available or format is not supported. Try a different browser.\n",
      "--------------------------------------------------\n",
      "Contact Us\n",
      "--------------------------------------------------\n",
      "Book Now\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "Whereexpertisemeetselegance.\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "We all need a dentist.Picking the right one can be a major decision. Here’s whyyou should choose us.\n",
      "--------------------------------------------------\n",
      "We are committed to providing a smooth and relaxed dental experience.\n",
      "--------------------------------------------------\n",
      "We offerall types of dentistryincluding general dentistry, cosmetic dentistry, Invisalign orthodontics and implants.\n",
      "--------------------------------------------------\n",
      "Based in Islington, in the City of London, we are easily accessible. We offer flexible appointments, as well asEmergency Dental Carein the heart of London.\n",
      "--------------------------------------------------\n",
      "Established in 2010, we provide top-quality private dentistry tailored to your needs. Using state-of-the-art technology, our modern and friendly practice in Old Street is here for you.\n",
      "--------------------------------------------------\n",
      "109 Lever Street, Islington, EC1V 3RQ\n",
      "--------------------------------------------------\n",
      "We are accepting new patients\n",
      "--------------------------------------------------\n",
      "Testimonials\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "“Was looking for a dentist in Old Street and so glad I found this place! They made me feel really calm and relaxed, really happy with the experience”\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "Friendly, professional and genuinely cared for my wellbeing. I just moved into the neighborhood and will make this place my local dentist.\n",
      "--------------------------------------------------\n",
      "Zainab\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "Jim\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "It’s been 4 months since I finished my Invisalign treatment at LDC with Dr. Leila and I am still just as happy! I would highly recommend treatment at LDC, the team are all very friendly and accommodating, thank you! :)\n",
      "--------------------------------------------------\n",
      "Zoe\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "100+ 5-Star Reviews\n",
      "--------------------------------------------------\n",
      "Cosmetic\n",
      "--------------------------------------------------\n",
      "Composite Bonding\n",
      "--------------------------------------------------\n",
      "Our skilled dentists can help to re-shape your teeth to achieve the smile that helps you feel confident.\n",
      "--------------------------------------------------\n",
      "Cosmetic\n",
      "--------------------------------------------------\n",
      "Teeth Whitening\n",
      "--------------------------------------------------\n",
      "We help you to achieve a brighter smile with our professional teeth whitening kits.\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "Cosmetic\n",
      "--------------------------------------------------\n",
      "Smile Makeover\n",
      "--------------------------------------------------\n",
      "Speak to one of cosmetic dentists about how you can achieve your perfect smile with a range of treatments.\n",
      "--------------------------------------------------\n",
      "Oral Surgery\n",
      "--------------------------------------------------\n",
      "Dental Implants\n",
      "--------------------------------------------------\n",
      "Talk to one of our master implantologists about dental implants and how they can restore you smile.\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "Explore all treatments\n",
      "--------------------------------------------------\n",
      "Book Online\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "INVISALIGN\n",
      "--------------------------------------------------\n",
      "Straighter teeth, clear aligners, no braces\n",
      "--------------------------------------------------\n",
      "Learn more aboutInvisalign\n",
      "--------------------------------------------------\n",
      "From £2,600\n",
      "--------------------------------------------------\n",
      "0% finance available\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "Cosmetic Dentistry, TransformingSmiles\n",
      "--------------------------------------------------\n",
      "At LDC, we are dedicated to going above and beyond to make you smile. Our highly skilled team has transformed thousands of smiles and continuously seeks new ways to innovate. From teeth bonding, veneers, and straightening to implants and whitening, ur cosmetic dentists are committed to giving you a smile that enhances your life.\n",
      "--------------------------------------------------\n",
      "Meet Our Dentists\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "Dr Leila, General & Cosmetic Dentist\n",
      "--------------------------------------------------\n",
      "Dr Katya, General & Cosmetic Dentist\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "Dr Arthie, General & Cosmetic Dentist\n",
      "--------------------------------------------------\n",
      "Dr Zola, General & Cosmetic Dentist\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "Dr Marios, Oral Surgeon\n",
      "--------------------------------------------------\n",
      "Dr Ioannis, Periodontist\n",
      "--------------------------------------------------\n",
      "Are you making the most of yourdental insurance?\n",
      "--------------------------------------------------\n",
      "Most dental insurance can be used with us. Speak to a member of our team about getting the most out of your insurance.\n",
      "--------------------------------------------------\n",
      "Maximise your oral health, minimise the costs.\n",
      "--------------------------------------------------\n",
      "Contact us\n",
      "--------------------------------------------------\n",
      "Dental Insurance FAQs\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "& more\n",
      "--------------------------------------------------\n",
      "Book Online\n",
      "--------------------------------------------------\n",
      "Testimonials\n",
      "--------------------------------------------------\n",
      "TRANSFORMING SMILES\n",
      "--------------------------------------------------\n",
      "with COMPOSITE BONDING\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "“I’m so grateful for the team at London Dental Centre, they were so friendly and professional. I’ve definitely got myconfidenceback!”\n",
      "--------------------------------------------------\n",
      "Lawrence, 26, Engineer\n",
      "--------------------------------------------------\n",
      "Click here toGet Directions\n",
      "--------------------------------------------------\n",
      "Nearest Tube Lines\n",
      "--------------------------------------------------\n",
      "A 5-10 minute walk from\n",
      "--------------------------------------------------\n",
      "Old StreetUnderground\n",
      "--------------------------------------------------\n",
      "Northern Line/London Overground\n",
      "--------------------------------------------------\n",
      "A 10 minute walk from\n",
      "--------------------------------------------------\n",
      "Barbican & Farringdon Underground\n",
      "--------------------------------------------------\n",
      "Elizabeth/Hammersmith & City/Metropolitan Lines\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "Follow us on Instagram:\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "Emergency Appointments\n",
      "--------------------------------------------------\n",
      "Hygiene Clean (Scale & Polish)\n",
      "--------------------------------------------------\n",
      "Invisalign Treatment\n",
      "--------------------------------------------------\n",
      "Composite Bonding\n",
      "--------------------------------------------------\n",
      "Veneers\n",
      "--------------------------------------------------\n",
      "Teeth Whitening\n",
      "--------------------------------------------------\n",
      "Implants\n",
      "--------------------------------------------------\n",
      "Treatments\n",
      "--------------------------------------------------\n",
      "HomeWebsite T&CTerms of SalePrivacy PolicyBook AppointmentContact UsTestimonialsPrices/FeesBlog\n",
      "--------------------------------------------------\n",
      "109 LEVER STREET, OLD STREET, ISLINGTON, LONDON, EC1V 3RQ |020 3667 7070|INFO@THELONDONDENTALCENTRE.CO.UK\n",
      "--------------------------------------------------\n",
      "© LONDON DENTAL CENTRE 2024 | LDMC Ltd 07156856 |Other| Regulated byCQC\n"
     ]
    }
   ],
   "source": [
    "elements = soup.find_all(['p', 'div', 'span','h1','h2','h3'])  # You can add more tags to this list\n",
    "for elem in elements:\n",
    "    print('-'*50)\n",
    "    print(elem.get_text(strip=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'By using this website, you agree to our use of cookies. We use cookies to provide you with a great experience and to help our website run effectively.\\nOK\\n0\\n\\nInterest-Free Finance Available\\n\\nAbout Us\\nTreatments\\nFees\\n020 3667 7070\\nBook Online Now\\nOpen Menu Close Menu\\n\\nAbout Us\\nTreatments\\nFees\\n020 3667 7070\\nBook Online Now\\nOpen Menu Close Menu\\nAbout Us\\nTreatments\\nFees\\n020 3667 7070\\nBook Online Now\\n\\nYour Trusted Dentists in the City of London\\nWe’re dedicated to personalised care that makes you feel valued, using world-class techniques to enhance your smile and boost your confidence.\\n% buffered00:00\\nPlay\\nVideo is not available or format is not supported. Try a different browser.\\nContact Us\\nBook Now\\n\\nWhere expertise meets elegance .\\n\\n\\n\\nWe all need a dentist. Picking the right one can be a major decision. Here’s why you should choose us .\\nWe are committed to providing a smooth and relaxed dental experience.\\nWe offer all types of dentistry including general dentistry, cosmetic dentistry, Invisalign orthodontics and implants.\\nBased in Islington, in the City of London, we are easily accessible. We offer flexible appointments, as well as Emergency Dental Care in the heart of London.\\nEstablished in 2010, we provide top-quality private dentistry tailored to your needs. Using state-of-the-art technology, our modern and friendly practice in Old Street is here for you.\\n109 Lever Street, Islington, EC1V 3RQ\\nWe are accepting new patients\\nTestimonials\\n\\n“Was looking for a dentist in Old Street and so glad I found this place! They made me feel really calm and relaxed, really happy with the experience”\\n\\nFriendly, professional and genuinely cared for my wellbeing. I just moved into the neighborhood and will make this place my local dentist.\\nZainab\\n\\n\\nJim\\n\\nIt’s been 4 months since I finished my Invisalign treatment at LDC with Dr. Leila and I am still just as happy! I would highly recommend treatment at LDC, the team are all very friendly and accommodating, thank you! :)\\nZoe\\n\\n100+ 5-Star Reviews\\nCosmetic\\nComposite Bonding\\nOur skilled dentists can help to re-shape your teeth to achieve the smile that helps you feel confident.\\nCosmetic\\nTeeth Whitening\\nWe help you to achieve a brighter smile with our professional teeth whitening kits.\\n\\n\\nCosmetic\\nSmile Makeover\\nSpeak to one of cosmetic dentists about how you can achieve your perfect smile with a range of treatments.\\nOral Surgery\\nDental Implants\\nTalk to one of our master implantologists about dental implants and how they can restore you smile.\\n\\n\\nExplore all treatments\\nBook Online\\n\\nINVISALIGN\\nStraighter teeth, clear aligners, no braces\\nLearn more about Invisalign\\nFrom £2,600\\n0% finance available\\n\\n\\nCosmetic Dentistry, Transforming Smiles\\nAt LDC, we are dedicated to going above and beyond to make you smile. Our highly skilled team has transformed thousands of smiles and continuously seeks new ways to innovate. From teeth bonding, veneers, and straightening to implants and whitening, ur cosmetic dentists are committed to giving you a smile that enhances your life.\\nMeet Our Dentists\\n\\n\\nDr Leila, General & Cosmetic Dentist\\nDr Katya, General & Cosmetic Dentist\\n\\n\\nDr Arthie, General & Cosmetic Dentist\\nDr Zola, General & Cosmetic Dentist\\n\\n\\nDr Marios, Oral Surgeon\\nDr Ioannis, Periodontist\\nAre you making the most of your dental insurance ?\\nMost dental insurance can be used with us. Speak to a member of our team about getting the most out of your insurance.\\nMaximise your oral health, minimise the costs.\\nContact us\\nDental Insurance FAQs\\n\\n\\n\\n& more\\nBook Online\\nTestimonials\\nTRANSFORMING SMILES\\nwith COMPOSITE BONDING\\n\\n\\n“ I’m so grateful for the team at London Dental Centre, they were so friendly and professional. I’ve definitely got my confidence back! ”\\nLawrence, 26, Engineer\\nClick here to Get Directions\\nNearest Tube Lines\\nA 5-10 minute walk from\\nOld Street Underground\\nNorthern Line/London Overground\\nA 10 minute walk from\\nBarbican & Farringdon Underground\\nElizabeth/Hammersmith & City/Metropolitan Lines\\n\\nFollow us on Instagram:\\n\\nEmergency Appointments\\nHygiene Clean (Scale & Polish)\\nInvisalign Treatment\\nComposite Bonding\\nVeneers\\nTeeth Whitening\\nImplants\\nTreatments\\nHome Website T&C Terms of Sale Privacy Policy Book Appointment Contact Us Testimonials Prices/Fees Blog\\n109 LEVER STREET, OLD STREET, ISLINGTON, LONDON, EC1V 3RQ | 020 3667 7070 | INFO@THELONDONDENTALCENTRE.CO.UK\\n© LONDON DENTAL CENTRE 2024 | LDMC Ltd 07156856 | Other | Regulated by CQC'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# def preprocess_content(content):\n",
    "#     # Remove all menu items and similar patterns\n",
    "#     content = re.sub(r'\\s+\\[.*?\\]\\(.*?\\)', '', content)  # Remove patterns like [text](URL)\n",
    "#     content = re.sub(r'\\[Skip to the content\\]\\(.*?\\)', '', content)  # Specific removal for \"Skip to the content\"\n",
    "#     content = re.sub(r'\\[.*?\\]\\(.*?\\)', '', content)  # Generic removal for [text](URL)\n",
    "#     content = re.sub(r'\\s*Menu\\s*', '', content)  # Remove \"Menu\"\n",
    "#     content = re.sub(r'\\s*Search\\s*', '', content)  # Remove \"Search\"\n",
    "#     content = re.sub(r'Categories\\s*', '', content)  # Remove \"Categories\"\n",
    "\n",
    "#     # Remove all URLs\n",
    "#     content = re.sub(r'http\\S+', '', content)\n",
    "\n",
    "#     # Remove empty lines or lines with only whitespace\n",
    "#     content = '\\n'.join([line for line in content.splitlines() if line.strip()])\n",
    "\n",
    "#     # Optionally, you could limit the text to a specific number of words\n",
    "\n",
    "#     return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"example.txt\", \"w\") as file:\n",
    "    file.write(html_content[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_str(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    out_text = \"\"\n",
    "    elements = soup.find_all(['p', 'div', 'span','h1','h2','h3'])  # You can add more tags to this list\n",
    "    for elem in elements:\n",
    "        \n",
    "        out_text += ('-'*50)\n",
    "        out_text += '\\n'\n",
    "        out_text += elem.get_text()\n",
    "        out_text += '\\n'\n",
    "    return out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def process_text(text):\n",
    "    # Split the text into lines\n",
    "    lines = text.splitlines()\n",
    "    \n",
    "    # Initialize variables\n",
    "    processed_lines = []\n",
    "    ignore_content = False\n",
    "    merge_line = \"\"\n",
    "    \n",
    "    cookies_meta_found = False\n",
    "    cookies_meta_line_position = 0\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        # Remove leading and trailing whitespace from the line\n",
    "        line = line.strip()\n",
    "\n",
    "        # Skip empty lines\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        # Rule 1: Ignore lines starting with '#'\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "        \n",
    "        # Rule 2: Ignore lines with '|'\n",
    "        if '|' in line:\n",
    "            continue\n",
    "        \n",
    "        # Rule 3: Ignore lines with 'copyright', 'Copy Right', or a copyright symbol\n",
    "        if re.search(r'copyright|Copy Right|©', line, re.IGNORECASE):\n",
    "            continue\n",
    "        \n",
    "        # Rule 4: Handle cookies/meta-data-related content\n",
    "        if re.search(r'cookies?|meta[-_ ]data', line, re.IGNORECASE):\n",
    "            if i < 25:\n",
    "                # If found within the first 25 lines, ignore the line\n",
    "                continue\n",
    "            else:\n",
    "                # If found after the 25th line, set the flag to ignore subsequent lines\n",
    "                cookies_meta_found = True\n",
    "                cookies_meta_line_position = i\n",
    "                continue\n",
    "\n",
    "        # If ignore_content flag is set and we are in the next line after the found keyword\n",
    "        if cookies_meta_found and i > cookies_meta_line_position:\n",
    "            continue\n",
    "        \n",
    "        # Rule 5: Treat content between '--------' lines as a single line\n",
    "        if line.startswith('--------'):\n",
    "            if merge_line:\n",
    "                processed_lines.append(merge_line.strip())\n",
    "                merge_line = \"\"\n",
    "            continue\n",
    "        else:\n",
    "            merge_line += \" \" + line if merge_line else line\n",
    "        \n",
    "        # Additional Rule: Ignore lines containing \"Open Menu\", \"Close Menu\", \"Privacy\", or \"Follow Us\"\n",
    "        if re.search(r'Open Menu|Close Menu|Privacy|Follow Us', line, re.IGNORECASE):\n",
    "            continue\n",
    "    \n",
    "    # Add the last accumulated line if any\n",
    "    if merge_line:\n",
    "        processed_lines.append(merge_line.strip())\n",
    "\n",
    "    # Apply further post-processing rules\n",
    "    final_lines = []\n",
    "    for line in processed_lines:\n",
    "        # Rule 6: Remove lines that contain only numbers\n",
    "        if line.isdigit():\n",
    "            continue\n",
    "        \n",
    "        # Rule 7: Remove lines that have only two words or fewer\n",
    "        if len(line.split()) <= 2:\n",
    "            continue\n",
    "        \n",
    "        # Rule 8: Remove lines with all capital letters and less than 4 words\n",
    "        if line.isupper() and len(line.split()) < 4:\n",
    "            continue\n",
    "        \n",
    "        final_lines.append(line)\n",
    "\n",
    "    # Join the final lines into a single string\n",
    "    processed_text = '\\n'.join(final_lines)\n",
    "    processed_text = processed_text.replace(\"\\\\\", \"\")\n",
    "    return processed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(input_string):\n",
    "    # Define the keywords to remove lines containing them\n",
    "    keywords = ['Open Menu', 'Close Menu', 'Privacy', 'Term and Condition', 'T&C', \n",
    "                'Facebook', 'http', 'https', 'Insta', 'Twitter', 'LinkedIn', 'Snapchat', 'TikTok','field']\n",
    "\n",
    "    # Split the input string into lines\n",
    "    lines = input_string.splitlines()\n",
    "\n",
    "    # Initialize a set to track unique lines\n",
    "    unique_lines = set()\n",
    "\n",
    "    # Define a regex pattern to detect lines with only numeric characters or less than 4 alphabetic characters\n",
    "    alpha_numeric_pattern = re.compile(r'^[\\d\\s]*[a-zA-Z]{0,3}[\\d\\s]*$')\n",
    "\n",
    "    # Define a regex pattern to identify numbers followed by words with no space, excluding ordinal numbers\n",
    "    number_word_pattern = re.compile(r'(\\d)(?!st|nd|rd|th)([a-zA-Z])')\n",
    "\n",
    "    # Process each line\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        # Insert newline if a number is followed by a word without space, except for ordinal numbers\n",
    "        line = number_word_pattern.sub(r'\\1\\n\\2', line)\n",
    "\n",
    "        # Check if line contains any of the keywords or matches the numeric pattern\n",
    "        if (any(keyword.lower() in line.lower() for keyword in keywords) or\n",
    "                alpha_numeric_pattern.match(line)):\n",
    "            continue\n",
    "        \n",
    "        # Check for duplicates\n",
    "        if line not in unique_lines:\n",
    "            unique_lines.add(line)\n",
    "            cleaned_lines.append(line)\n",
    "    \n",
    "    # Join the cleaned lines back into a string\n",
    "    return '\\n'.join(cleaned_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproces_content_new(html_page_content):\n",
    "    html = markdown.markdown(html_page_content)\n",
    "    text_o = get_text_str(html)\n",
    "    preprocessed_text = process_text(text_o)\n",
    "    cleaned_text = clean_text(preprocessed_text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_text = preproces_content_new(html_content[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cart is empty\\nWelcome To CyberCity\\nWho We Are ?\\nWelcome to Cyber City, your trusted destination for computer accessories and electronics located in the renowned Rex City Computer Market of Faisalabad. Since opening our doors in 1998, we have been dedicated to serving both retail and wholesale customers with a commitment to quality that has become synonymous with our name. Our reputation for reliability and exceptional customer service continues to drive our success.\\nOver the years, we have continually expanded our product offerings to include everything from cutting-edge gaming equipment to essential networking solutions, storage devices, converters, and a comprehensive array of audio products. We maintain strong partnerships with leading manufacturers to ensure that our customers receive the finest products at competitive prices.\\nProud of our prestigious history, Cyber City remains committed to innovation and excellence. We strive to stay at the forefront of technological advancements, ensuring that our customers have access to the latest and most effective products on the market. Thank you for choosing Cyber City for your technology needs.\\n\\u200e+92 303 0222392\\nCyber City 70 Ground Floor ,1st Gallery Rex City Satyana Road, Faisalabad, Pakistan, 38000\\nSubscribe To Our Email\\nFor Latest News & Updates'"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"example.txt\", \"w\") as file:\n",
    "    file.write(out_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_repetitions(text):\n",
    "    words = text.split()\n",
    "    seen = set()\n",
    "    result = []\n",
    "\n",
    "    for word in words:\n",
    "        if word not in seen:\n",
    "            seen.add(word)\n",
    "            result.append(word)\n",
    "\n",
    "    return ' '.join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_plain_text_from_markdown(markdown_content):\n",
    "    # Remove image tags (Markdown and HTML)\n",
    "    markdown_content = re.sub(r'!\\[.*?\\]\\(.*?\\)', '', markdown_content)\n",
    "    markdown_content = re.sub(r'<img.*?>', '', markdown_content)\n",
    "    \n",
    "    # Remove links (Markdown and plain URLs)\n",
    "    markdown_content = re.sub(r'\\[.*?\\]\\(.*?\\)', '', markdown_content)\n",
    "    markdown_content = re.sub(r'\\(https?://.*?\\)', '', markdown_content)\n",
    "    markdown_content = re.sub(r'https?://\\S+|www\\.\\S+', '', markdown_content)\n",
    "    \n",
    "    # Remove any remaining inline links\n",
    "    markdown_content = re.sub(r'\\[.*?\\]', '', markdown_content)\n",
    "    \n",
    "    # Remove HTML tags (including SVG)\n",
    "    markdown_content = re.sub(r'<.*?>', '', markdown_content)\n",
    "    \n",
    "    # Remove all symbols except ., \"\", '', ?, and ,\n",
    "    markdown_content = re.sub(r\"[^a-zA-Z0-9\\s.,\\\"'?']\", '', markdown_content)\n",
    "    \n",
    "    # Remove any extra whitespace and newlines\n",
    "    markdown_content = re.sub(r'\\s+', ' ', markdown_content).strip()\n",
    "    \n",
    "    return markdown_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_content = extract_plain_text_from_markdown(html_content[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_content = remove_all_repetitions(cleaned_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Solutions textgenerationinference documentation Flash Attention View all docsAWS Trainium InferentiaAccelerateAmazon SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset viewerDatasetsDiffusersDistilabelEvaluateGoogle TPUsGradioHubHub Python LibraryHuggingface.jsInference API serverlessInference Endpoints dedicatedLeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTransformersTransformers.jstimm Search CtrlK main EN Getting started Tutorials Reference Conceptual Guides Join the Hugging Face community and get access to augmented experience Collaborate on models, datasets Spaces Faster examples with accelerated inference Switch between themes Scaling transformer architecture is heavily bottlenecked by selfattention mechanism, which has quadratic time memory complexity. Recent developments in accelerator hardware mainly focus enhancing compute capacities not transferring data hardware. This results attention operation having a bottleneck. an algorithm used reduce this problem scale transformerbased models more efficiently, enabling faster training inference. Standard mechanism uses High Bandwidth Memory HBM store, read write keys, queries values. large memory, but slow processing, meanwhile SRAM smaller operations. In standard implementation, cost of loading writing queries, values from high. It loads GPU onchip SRAM, performs single step writes it back HBM, repeats for every step. Instead, once, fuses operations them back. implemented supported models. You can check out complete list that support , flash prefix. learn about reading paper .'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=8000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.create_documents([cleaned_content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field, root_validator\n",
    "from pydantic import BaseModel, Field, root_validator, ValidationError\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gv/v0q4rb4n3p9_ydb839lgywph0000gq/T/ipykernel_8980/3567739227.py:6: PydanticDeprecatedSince20: Pydantic V1 style `@root_validator` validators are deprecated. You should migrate to Pydantic V2 style `@model_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  @root_validator(pre=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define SentenceParser model\n",
    "class SentenceParser(BaseModel):\n",
    "    originalContent: str = Field(..., description=\"Original Sentence with no correction\")\n",
    "    correctedSentence: str = Field(..., description=\"Corrected Sentence in English UK. Enclosed corrections in <b></b>.\")\n",
    "\n",
    "    @root_validator(pre=True)\n",
    "    def fill_missing_fields(cls, values):\n",
    "        values['correctedSentence'] = values.get('correctedSentence', '')\n",
    "        return values\n",
    "\n",
    "# Define ResponseModel to include sentences\n",
    "class ResponseModel(BaseModel):\n",
    "    sentences: List[SentenceParser]\n",
    "\n",
    "# Function to get the markdown content\n",
    "def get_markdown_content(page_content):\n",
    "    llm = ChatGroq(\n",
    "            model=\"llama-3.1-70b-versatile\",\n",
    "            temperature=0,\n",
    "            max_tokens=None,\n",
    "            timeout=None,\n",
    "            max_retries=3,\n",
    "            # other params...\n",
    "        )\n",
    "    \n",
    "\n",
    "    parser = PydanticOutputParser(pydantic_object=ResponseModel)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are a smart assistant.\n",
    "        You will be given a page-long document.\n",
    "        You will return a list of sentences in the page.\n",
    "        If there is any HTML content or non-textual content, you will remove it.\n",
    "        First, you will clean the text by removing the images and links, and extract plain text.\n",
    "        You will not expand any point or enhance anything. \n",
    "        If sentence contain the social media or web development related things like cookies etc you will remove it.\n",
    "        Just return what is in the original text.\n",
    "        Provide the English UK version of sentences, performing both spelling and grammar checks.\n",
    "        Return the result as a JSON object with a single key \"sentences\" which contains a list of objects.\n",
    "        Each object should have keys \"originalContent\" and \"correctedSentence\".\n",
    "\n",
    "        \\n\\n{query}\\n\n",
    "        \"\"\",\n",
    "        input_variables=[\"query\"]\n",
    "    )\n",
    "\n",
    "    chain = LLMChain(llm=llm, prompt=prompt, output_parser=parser)\n",
    "\n",
    "    result = chain({\"query\": page_content})\n",
    "\n",
    "    if isinstance(result,dict):\n",
    "        result = result['text']\n",
    "        return result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_random = texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Back to Top CS229 Gaussian Discriminant Analysis Generative Learning Algorithms Weve mainly looked at learning algorithms that model Pyx, the conditional distribution of y given x. For instance, logistic regression modeled Pyx as hxgTx where g is sigmoid function. In this section, well talk about a different type algorithm. Lets use binary classification problem motivation behind our discussion. Consider in which we want learn distinguish between malignant y1 and benign y0 tumors. Given training set, an algorithm like , or initially starts with randomly initialized parameters. Over course learning, performs gradient descent straight line hyperplane decision boundary evolves until you obtain separates positivenegative examples case, Then, classify new sample either benign, it checks on side falls in, makes its prediction accordingly. Theres class arent trying maximize likelihood looking both classes searching for separation boundary. Instead, these look one time. First, tumors, can features what tumors like. build separate Finally, tumor, match against tumor model, see whether looks more had seen set. try directly such mapping from input space X labels are called discriminative algorithms. instead Pxy Py. These generative if indicates example 0 1, then Pxy0 models features, while Pxy1 features. The also learns prior Py independent probability y. To illustrate concept using practical when patient walks into hospital, before doctors even them, odds their versus referred prior. Thus, builds each isolation. At test time, evaluates models, identifies matches most closely returns prediction. After modeling priors Pxy, Bayes rule derive posterior x PyxPxyPyPx Here, denominator by PxPxy1Py1Pxy0Py0, function quantities Note learned part process calculating order make prediction, dont actually need calculate value Px since constant, doesnt appear there. When making predictions algorithms, thus ignore computing save computation. However, end goal value, would compute be able normalize numerator. PyxPxyPyPxPxyPy above equation represents underlying framework . Key takeaways Discriminative i.e., output input. other words, hx0,1 directly. class, tumoridentification setting, may case them first discriminant analysis GDA, used continuousvalued say, classification. assume distributed according multivariate normal distribution. briefly properties distributions moving GDA itself. Multivariate Distribution generalization 1dimensional random variable ndimensional simply, nrandom variable. rather than univariate variable, seeks multiple variables. Assume Gaussian, i.e, XRn, parameterized mean vector Rn covariance matrix Rnn, symmetric positive semidefinite. Formally, written as, XN, density PDF PX,12n212exp12XT1X denotes determinant expected ExxPx,dx vectorvalued defined CovX EXET. Covariance generalizes notion variance realvalued setting. EEET. Since CovX. explore some visualize Recall familiar Similarly, multivariable represented same bellshaped curve two parameters control PDF, but ndimensions. e.g., 2dimensional over 2 variables, 2dimensional, size 22. below figure shows zero 21 zerovector I 22 identity matrix. A standard 0.6I. essentially taken multiplied number has shrunk variance, reduced variability 2I. From images, becomes larger, widershorter, smaller, compressedtaller. This because always integrates area under so scales spread vs. height figures show Gaussians corresponding matrices figures, increase offdiagonal entries compressed towards 45 x1x2. Geometrically speaking, implies variables positively correlated. We clearly contours three densities 3D bumps saw above, should perfectly round circles, aspect ratio image probably little bit fatter places being Heres another set along decreasing elements matrix, now again, opposite direction, 135 line. Again, geometrically, endow negative correlation. vary parameters, tend form ellipses. By varying shift center around. Another values I, visualization PDFs follows carry out eigenvectors points principal axes ellipse contours. takeaway As density, change spreadheight respectively. Model have consider task discussion, expressed equations Pxy1, Gaussians. On hand, Bernoulli takes 0,1. by, xy0N0,xy1N1,yBernoulli 0, classes, vectors 1. Put were assuming representing means. You not commonly seen. More section below. fit your data, will define data. R, 0Rn, 1Rn, Rnn. Writing distributions, Pyy11yPxy012n212exp12x0T1x0Pxy112n212exp12x1T1x1 exponential notation similar earlier Plugging Pxy0,Pxy1,Py0 Py1 formula easily ascertain particular example. Suppose xi,yimi1. aforementioned going joint L, L,0,1,mi1Pxi,yi,0,1,mi1Pxiyi0,1,Pyi big difference cost functions compared choose ,0,1,, Px,y,0,1,. linear regression, generalized Pyx. loglikelihood data simply log L. ,0,1,logmi1Pxi,yi,0,1,logmi1Pxiyi0,1,Pyi respect take derivative equal solve expression yields maximum estimate be, mi1yim An intuitive explanation around maximizes follows. example, chance next office denoted bias coin toss fraction heads Likewise, just label y1. way write indicator notation, mi11yi1m 1 argument true, otherwise 0. true statement false equivalent ifstatement programming context. 0,1 is, 0mi11yi0ximi11yi01mi11yi1ximi11yi1 intuition expression, think all say dataset. reasonable examples, 0s average writing intuition. numerator sum feature sums entire summing i1m, uses instances yi0 times xi. effect term whereas effectively zeroing term. up samples yi0. count represent Because every yi0, get extra sum, ends total examples. made 1mmi1xiyixiyiT fits means classes. Making Predictions Using minz min z minimum any possible z. argminz argmin refers plugged expression. minzz52 attained z520 argminzz52 led z52, z5. maxz argmaxz operators exist they deal leads having lets how go So do benign? boils down predicting likely x, formally argmaxyPyx Looking deeper y0,1 rule, argmaxyPyxargmaxyPxyPyPx reasons highlighted Visualization dataset compareandcontrast operate Pictorially, GDAs operation Shown been shape orientation, share point, determine put together, imply blue Py1x0.5. boundary, predict outcome, side, y0. Points upper right closer classifying lower left classified Logistic Regression x1 x2 start whose randomly. Typically, initialize purpose visualization, starting off shown Running single iteration PYX, repositions 20 iterations, converges illustrates green superimposed blue. arrive slightly boundaries. Why Two Separate Means, Single Matrix? If lot problems. Choosing reasonable, work fine. catch, though, youll roughly double isnt anymore. Discussion Comparing interesting relationship. youve ,0,1,. fixed Py1x,0,1,. theorem, Pxy1,0,1,Pxy1,0,1,Py1Px,0,1, where, Pxy1,0,1, measure evaluating once determined optimum probability, plot Py1x so, simple few Now, Mappingprojecting Xaxis, bump PXY0 PXY1. Were Also, note split 5050 across PY1 0.5, known half Next, PY1X X. unlabeled far Xaxis. infer point almost certainly came left, generating Xaxis datapoint very small. pick Its easy discern PXY10.5. right, youd pretty sure class. repeat exercise sweeping datapoints dense grid evaluate measure, Youll notice coming approach midpoint, increases 0.5 surpasses 0.5. Beyond certain tends Plotting connect dots, turns exactly view quantity Py1x,0,1, find form, Py1x,,0,111expTx appropriate ,,0,1. convention redefining xis righthandside n1 dimensional adding coordinate xi01. 1x. While hypothesis function, hood specific choice choosing quite leading Would Prefer One Another? general, yield boundaries trained Which better? discuss superior viceversa. assumes that, xy0 xy1 parameter logistic, governed Py1x11expTx element x0, raised plotting pointbypoint ultimately yielded curve, shared necessarily assumptions 3. converse, implication does Gaussian. 3 2. stronger prove 2, noted property GDA. correct, better regression. Specifically, indeed asymptotically efficient. strong youre baking information algorithm, Informally, limit large sets m, there no strictly terms of, accurately')"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkd_contents = get_markdown_content(text_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gv/v0q4rb4n3p9_ydb839lgywph0000gq/T/ipykernel_8980/985300905.py:1: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  out_sentences = mkd_contents.dict().get('sentences')\n"
     ]
    }
   ],
   "source": [
    "out_sentences = mkd_contents.dict().get('sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'originalContent': 'Back to Top CS229 Gaussian Discriminant Analysis Generative Learning Algorithms Weve mainly looked at learning algorithms that model Pyx, the conditional distribution of y given x.',\n",
       "  'correctedSentence': \"Back to Top CS229 Gaussian Discriminant Analysis Generative Learning Algorithms We've mainly looked at learning algorithms that model Pyx, the conditional distribution of y given x.\"},\n",
       " {'originalContent': 'For instance, logistic regression modeled Pyx as hxgTx where g is sigmoid function.',\n",
       "  'correctedSentence': 'For instance, logistic regression modelled Pyx as hxgTx where g is sigmoid function.'},\n",
       " {'originalContent': 'In this section, well talk about a different type algorithm.',\n",
       "  'correctedSentence': \"In this section, we'll talk about a different type of algorithm.\"},\n",
       " {'originalContent': 'Lets use binary classification problem motivation behind our discussion.',\n",
       "  'correctedSentence': \"Let's use binary classification problem motivation behind our discussion.\"},\n",
       " {'originalContent': 'Consider in which we want learn distinguish between malignant y1 and benign y0 tumors.',\n",
       "  'correctedSentence': 'Consider in which we want to learn to distinguish between malignant y1 and benign y0 tumours.'},\n",
       " {'originalContent': 'Given training set, an algorithm like , or initially starts with randomly initialized parameters.',\n",
       "  'correctedSentence': 'Given a training set, an algorithm like , or initially starts with randomly initialised parameters.'},\n",
       " {'originalContent': 'Over course learning, performs gradient descent straight line hyperplane decision boundary evolves until you obtain separates positivenegative examples case, Then, classify new sample either benign, it checks on side falls in, makes its prediction accordingly.',\n",
       "  'correctedSentence': 'Over the course of learning, it performs gradient descent on a straight line hyperplane decision boundary, which evolves until it separates positive and negative examples. Then, it classifies a new sample as either benign or malignant, depending on which side it falls in, and makes its prediction accordingly.'},\n",
       " {'originalContent': 'Theres class arent trying maximize likelihood looking both classes searching for separation boundary.',\n",
       "  'correctedSentence': \"There's a class that aren't trying to maximise likelihood, looking at both classes and searching for a separation boundary.\"},\n",
       " {'originalContent': 'Instead, these look one time.',\n",
       "  'correctedSentence': 'Instead, these look at one time.'},\n",
       " {'originalContent': 'First, tumors, can features what tumors like.',\n",
       "  'correctedSentence': 'First, tumours can have features that are like other tumours.'},\n",
       " {'originalContent': 'build separate Finally, tumor, match against tumor model, see whether looks more had seen set.',\n",
       "  'correctedSentence': 'Build separate models for each tumour type. Finally, match the tumour against the tumour model, and see whether it looks more like the tumours in the training set.'},\n",
       " {'originalContent': 'try directly such mapping from input space X labels are called discriminative algorithms.',\n",
       "  'correctedSentence': 'Try to directly map from the input space X to the labels, which are called discriminative algorithms.'},\n",
       " {'originalContent': 'instead Pxy Py.',\n",
       "  'correctedSentence': 'Instead, Pxy = Py.'},\n",
       " {'originalContent': 'These generative if indicates example 0 1, then Pxy0 models features, while Pxy1 features.',\n",
       "  'correctedSentence': 'These generative models indicate that, for example, Pxy0 models the features of class 0, while Pxy1 models the features of class 1.'},\n",
       " {'originalContent': 'The also learns prior Py independent probability y.',\n",
       "  'correctedSentence': 'The model also learns the prior probability Py, which is independent of y.'},\n",
       " {'originalContent': 'To illustrate concept using practical when patient walks into hospital, before doctors even them, odds their versus referred prior.',\n",
       "  'correctedSentence': 'To illustrate this concept, consider a practical example: when a patient walks into a hospital, before the doctors even see them, the odds of their having a particular disease versus not having it are referred to as the prior probability.'},\n",
       " {'originalContent': 'Thus, builds each isolation.',\n",
       "  'correctedSentence': 'Thus, the model builds each in isolation.'},\n",
       " {'originalContent': 'At test time, evaluates models, identifies matches most closely returns prediction.',\n",
       "  'correctedSentence': 'At test time, the model evaluates the models, identifies which one matches most closely, and returns the prediction.'},\n",
       " {'originalContent': 'After modeling priors Pxy, Bayes rule derive posterior x PyxPxyPyPx',\n",
       "  'correctedSentence': \"After modelling the priors Pxy, we can derive the posterior probability x using Bayes' rule: Pyx = PxyPyPx\"},\n",
       " {'originalContent': 'Here, denominator by PxPxy1Py1Pxy0Py0, function quantities',\n",
       "  'correctedSentence': 'Here, the denominator is given by Px = Pxy1Py1 + Pxy0Py0, which is a function of the quantities'},\n",
       " {'originalContent': 'Note learned part process calculating order make prediction, dont actually need calculate value Px since constant, doesnt appear there.',\n",
       "  'correctedSentence': \"Note that the learned part of the process is calculating the order to make a prediction, but we don't actually need to calculate the value of Px since it is a constant and doesn't appear in the final prediction.\"},\n",
       " {'originalContent': 'When making predictions algorithms, thus ignore computing save computation.',\n",
       "  'correctedSentence': 'When making predictions, algorithms thus ignore computing Px to save computation.'},\n",
       " {'originalContent': 'However, end goal value, would compute be able normalize numerator.',\n",
       "  'correctedSentence': 'However, the end goal is to compute the value of Px, which would allow us to normalise the numerator.'},\n",
       " {'originalContent': 'PyxPxyPyPxPxyPy above equation represents underlying framework .',\n",
       "  'correctedSentence': 'The equation Pyx = PxyPyPx represents the underlying framework.'},\n",
       " {'originalContent': 'Key takeaways Discriminative i.e., output input.',\n",
       "  'correctedSentence': 'Key takeaways: discriminative models, i.e., output is a function of input.'},\n",
       " {'originalContent': 'other words, hx0,1 directly.',\n",
       "  'correctedSentence': 'In other words, hx is 0 or 1 directly.'},\n",
       " {'originalContent': 'class, tumoridentification setting, may case them first discriminant analysis GDA, used continuousvalued say, classification.',\n",
       "  'correctedSentence': 'In the case of tumour identification, we may use discriminant analysis (GDA) for continuous-valued classification.'},\n",
       " {'originalContent': 'assume distributed according multivariate normal distribution.',\n",
       "  'correctedSentence': 'We assume that the data is distributed according to a multivariate normal distribution.'},\n",
       " {'originalContent': 'briefly properties distributions moving GDA itself.',\n",
       "  'correctedSentence': 'We will briefly discuss the properties of the distributions and then move on to GDA itself.'},\n",
       " {'originalContent': 'Multivariate Distribution generalization 1dimensional random variable ndimensional simply, nrandom variable.',\n",
       "  'correctedSentence': 'A multivariate distribution is a generalisation of a 1-dimensional random variable to an n-dimensional random variable.'},\n",
       " {'originalContent': 'rather than univariate variable, seeks multiple variables.',\n",
       "  'correctedSentence': 'Rather than a univariate variable, it seeks multiple variables.'},\n",
       " {'originalContent': 'Assume Gaussian, i.e, XRn, parameterized mean vector Rn covariance matrix Rnn, symmetric positive semidefinite.',\n",
       "  'correctedSentence': 'We assume that the data is Gaussian, i.e., X ~ Rn, parameterised by a mean vector in Rn and a covariance matrix in Rnn, which is symmetric and positive semidefinite.'},\n",
       " {'originalContent': 'Formally, written as, XN, density PDF PX,12n212exp12XT1X denotes determinant expected ExxPx,dx vectorvalued defined CovX EXET.',\n",
       "  'correctedSentence': 'Formally, this can be written as X ~ N(μ, Σ), where the density PDF is given by PX(x) = (1/√(2π)^n) * exp(-1/2 * (x-μ)^T * Σ^(-1) * (x-μ)), where μ is the mean vector, Σ is the covariance matrix, and E[x] is the expected value of x.'},\n",
       " {'originalContent': 'Covariance generalizes notion variance realvalued setting.',\n",
       "  'correctedSentence': 'Covariance generalises the notion of variance in the real-valued setting.'},\n",
       " {'originalContent': 'EEET.', 'correctedSentence': 'E[E^T] = E[E]E^T.'},\n",
       " {'originalContent': 'Since CovX.',\n",
       "  'correctedSentence': 'Since Cov(X) = E[(X-E[X])(X-E[X])^T],'},\n",
       " {'originalContent': 'explore some visualize Recall familiar',\n",
       "  'correctedSentence': \"Let's explore some visualisations. Recall the familiar\"},\n",
       " {'originalContent': 'Similarly, multivariable represented same bellshaped curve two parameters control PDF, but ndimensions.',\n",
       "  'correctedSentence': 'Similarly, a multivariate distribution can be represented by the same bell-shaped curve, but with two parameters controlling the PDF in n dimensions.'},\n",
       " {'originalContent': 'e.g., 2dimensional over 2 variables, 2dimensional, size 22.',\n",
       "  'correctedSentence': 'For example, a 2-dimensional distribution over 2 variables has a size of 2x2.'},\n",
       " {'originalContent': 'below figure shows zero 21 zerovector I 22 identity matrix.',\n",
       "  'correctedSentence': 'The figure below shows a zero-mean vector, a 2x1 zero vector, and a 2x2 identity matrix.'},\n",
       " {'originalContent': 'A standard 0.6I.',\n",
       "  'correctedSentence': 'A standard deviation of 0.6I.'},\n",
       " {'originalContent': 'essentially taken multiplied number has shrunk variance, reduced variability 2I.',\n",
       "  'correctedSentence': 'Essentially, the number has been multiplied by 0.6, which has shrunk the variance and reduced the variability. 2I.'},\n",
       " {'originalContent': 'From images, becomes larger, widershorter, smaller, compressedtaller.',\n",
       "  'correctedSentence': 'From the images, we can see that as the covariance matrix becomes larger, the distribution becomes wider and shorter, and as it becomes smaller, the distribution becomes compressed and taller.'},\n",
       " {'originalContent': 'This because always integrates area under so scales spread vs. height figures show Gaussians corresponding matrices figures, increase offdiagonal entries compressed towards 45 x1x2.',\n",
       "  'correctedSentence': 'This is because the area under the curve always integrates to 1, so as the spread increases, the height decreases, and vice versa. The figures show Gaussians corresponding to the matrices, and as the off-diagonal entries increase, the distribution becomes compressed towards the 45° line between x1 and x2.'},\n",
       " {'originalContent': 'Geometrically speaking, implies variables positively correlated.',\n",
       "  'correctedSentence': 'Geometrically speaking, this implies that the variables are positively correlated.'},\n",
       " {'originalContent': 'We clearly contours three densities 3D bumps saw above, should perfectly round circles, aspect ratio image probably little bit fatter places being',\n",
       "  'correctedSentence': 'We can clearly see the contours of the three densities, which are 3D bumps. The aspect ratio of the image is probably a little bit fatter in some places, but the contours should be perfectly round circles.'},\n",
       " {'originalContent': 'Heres another set along decreasing elements matrix, now again, opposite direction, 135 line.',\n",
       "  'correctedSentence': \"Here's another set of images, this time with decreasing elements of the matrix, and again, in the opposite direction, along the 135° line.\"},\n",
       " {'originalContent': 'Again, geometrically, endow negative correlation.',\n",
       "  'correctedSentence': 'Again, geometrically, this endows the variables with a negative correlation.'},\n",
       " {'originalContent': 'vary parameters, tend form ellipses.',\n",
       "  'correctedSentence': 'As we vary the parameters, the distribution tends to form ellipses.'},\n",
       " {'originalContent': 'By varying shift center around.',\n",
       "  'correctedSentence': 'By varying the shift of the center around.'},\n",
       " {'originalContent': 'Another values I, visualization PDFs follows carry out eigenvectors points principal axes ellipse contours.',\n",
       "  'correctedSentence': 'Another way to visualise the PDFs is to carry out the eigenvectors and points of the principal axes of the ellipse contours.'},\n",
       " {'originalContent': 'takeaway As density, change spreadheight respectively.',\n",
       "  'correctedSentence': 'The takeaway is that as the density changes, the spread and height change respectively.'},\n",
       " {'originalContent': 'Model have consider task discussion, expressed equations Pxy1, Gaussians.',\n",
       "  'correctedSentence': 'The model we have been considering for this task is expressed in terms of equations for Pxy1, which are Gaussians.'},\n",
       " {'originalContent': 'On hand, Bernoulli takes 0,1.',\n",
       "  'correctedSentence': 'On the other hand, the Bernoulli distribution takes values 0 and 1.'},\n",
       " {'originalContent': 'by, xy0N0,xy1N1,yBernoulli 0, classes, vectors 1.',\n",
       "  'correctedSentence': 'By the way, xy0 ~ N0, xy1 ~ N1, and y ~ Bernoulli(0, 1), where the classes are represented by vectors of length 1.'},\n",
       " {'originalContent': 'Put were assuming representing means.',\n",
       "  'correctedSentence': 'Put simply, we were assuming that the means are represented by'},\n",
       " {'originalContent': 'You not commonly seen.',\n",
       "  'correctedSentence': 'You may not have commonly seen this.'},\n",
       " {'originalContent': 'More section below.',\n",
       "  'correctedSentence': 'More on this in the section below.'},\n",
       " {'originalContent': 'fit your data, will define data.',\n",
       "  'correctedSentence': 'To fit your data, we will define the data.'},\n",
       " {'originalContent': 'R, 0Rn, 1Rn, Rnn.',\n",
       "  'correctedSentence': 'R, 0 in Rn, 1 in Rn, and Rnn.'},\n",
       " {'originalContent': 'Writing distributions, Pyy11yPxy012n212exp12x0T1x0Pxy112n212exp12x1T1x1',\n",
       "  'correctedSentence': 'Writing the distributions, Py(y=1) = Pxy0 ~ N(0, 1), Py(y=1) = Pxy1 ~ N(1, 1)'},\n",
       " {'originalContent': 'exponential notation similar earlier Plugging Pxy0,Pxy1,Py0 Py1 formula easily ascertain particular example.',\n",
       "  'correctedSentence': 'Using exponential notation similar to earlier, plugging in Pxy0, Pxy1, Py0, and Py1 into the formula, we can easily ascertain the particular example.'},\n",
       " {'originalContent': 'Suppose xi,yimi1.',\n",
       "  'correctedSentence': 'Suppose we have xi and yi, for i = 1 to m.'},\n",
       " {'originalContent': 'aforementioned going joint L, L,0,1,mi1Pxi,yi,0,1,mi1Pxiyi0,1,Pyi',\n",
       "  'correctedSentence': 'The aforementioned joint likelihood L is given by L = ∏[P(xi, yi) for i = 1 to m] = ∏[P(xi|yi)P(yi) for i = 1 to m]'},\n",
       " {'originalContent': 'big difference cost functions compared choose ,0,1,, Px,y,0,1,.',\n",
       "  'correctedSentence': 'There is a big difference between the cost functions. We choose to maximise the likelihood of the data, which is given by L = ∏[P(xi, yi) for i = 1 to m].'},\n",
       " {'originalContent': 'linear regression, generalized Pyx.',\n",
       "  'correctedSentence': 'In linear regression, we generalise Pyx.'},\n",
       " {'originalContent': 'loglikelihood data simply log L.',\n",
       "  'correctedSentence': 'The log-likelihood of the data is simply log L.'},\n",
       " {'originalContent': ',0,1,logmi1Pxi,yi,0,1,logmi1Pxiyi0,1,Pyi',\n",
       "  'correctedSentence': 'The log-likelihood is given by log L = ∑[log P(xi, yi) for i = 1 to m] = ∑[log P(xi|yi)P(yi) for i = 1 to m]'},\n",
       " {'originalContent': 'respect take derivative equal solve expression yields maximum estimate be, mi1yim',\n",
       "  'correctedSentence': 'Taking the derivative with respect to the parameters and setting it equal to zero, we can solve for the expression that yields the maximum likelihood estimate, which is given by μ = (1/m) * ∑[yi * xi for i = 1 to m]'},\n",
       " {'originalContent': 'An intuitive explanation around maximizes follows.',\n",
       "  'correctedSentence': 'An intuitive explanation of how this maximises the likelihood follows.'},\n",
       " {'originalContent': 'example, chance next office denoted bias coin toss fraction heads Likewise, just label y1.',\n",
       "  'correctedSentence': 'For example, the chance of the next office being occupied is denoted by the bias of a coin toss, which is the fraction of heads. Likewise, we just label y as 1.'},\n",
       " {'originalContent': 'way write indicator notation, mi11yi1m 1 argument true, otherwise 0.',\n",
       "  'correctedSentence': 'We can write this in indicator notation as 1(yi = 1) = 1 if the argument is true, and 0 otherwise.'},\n",
       " {'originalContent': 'true statement false equivalent ifstatement programming context.',\n",
       "  'correctedSentence': 'This is equivalent to a true statement being false in an if-statement in a programming context.'},\n",
       " {'originalContent': '0,1 is, 0mi11yi0ximi11yi01mi11yi1',\n",
       "  'correctedSentence': '0 or 1 is given by 0 if yi = 0, and 1 if yi = 1.'},\n",
       " {'originalContent': 'intuition expression, think all say dataset.',\n",
       "  'correctedSentence': 'The intuition behind this expression is to think about all the data points in the dataset.'},\n",
       " {'originalContent': 'reasonable examples, 0s average writing intuition.',\n",
       "  'correctedSentence': 'A reasonable example is to take the average of the 0s and 1s in the dataset.'},\n",
       " {'originalContent': 'numerator sum feature sums entire summing i1m, uses instances yi0 times xi.',\n",
       "  'correctedSentence': 'The numerator is the sum of the feature sums over the entire dataset, summing over all instances where yi = 0, and using the instances where yi = 1 times xi.'},\n",
       " {'originalContent': 'effect term whereas effectively zeroing term.',\n",
       "  'correctedSentence': 'This has the effect of zeroing out the term.'},\n",
       " {'originalContent': 'up samples yi0.',\n",
       "  'correctedSentence': 'This is because we are summing over all samples where yi = 0.'},\n",
       " {'originalContent': 'count represent Because every yi0, get extra sum, ends total examples.',\n",
       "  'correctedSentence': 'This is because every time yi = 0, we get an extra sum, which ends up being the total number of examples.'},\n",
       " {'originalContent': 'made 1mmi1xiyixiT fits means classes.',\n",
       "  'correctedSentence': 'This is made up of 1/m times the sum of xi times yi, which fits the means of the classes.'},\n",
       " {'originalContent': 'Making Predictions Using minz min z minimum any possible z.',\n",
       "  'correctedSentence': 'Making predictions using the minimum of z, where z is any possible value.'},\n",
       " {'originalContent': 'argminz argmin refers plugged expression.',\n",
       "  'correctedSentence': 'The argmin of z refers to the value of z that minimises the expression.'},\n",
       " {'originalContent': 'minzz52 attained z520 argminzz52 led z52, z5.',\n",
       "  'correctedSentence': 'The minimum of z is attained when z = 5, and the argmin of z is 5.'},\n",
       " {'originalContent': 'maxz argmaxz operators exist they deal leads having lets how go',\n",
       "  'correctedSentence': \"The max of z and the argmax of z are operators that exist, and they deal with the maximum value of z. Let's see how to go about finding the maximum value of z.\"},\n",
       " {'originalContent': 'So do benign?',\n",
       "  'correctedSentence': 'So, do we predict benign?'},\n",
       " {'originalContent': 'boils down predicting likely x, formally argmaxyPyx',\n",
       "  'correctedSentence': 'This boils down to predicting the most likely value of x, which is given by the argmax of Pyx.'},\n",
       " {'originalContent': 'Looking deeper y0,1 rule, argmaxyPyxargmaxyPxyPyPx',\n",
       "  'correctedSentence': 'Looking deeper, we can see that the rule for predicting y is given by the argmax of Pyx, which is equal to the argmax of PxyPyPx.'},\n",
       " {'originalContent': 'reasons highlighted Visualization dataset compareandcontrast operate',\n",
       "  'correctedSentence': 'The reasons for this are highlighted in the visualisation of the dataset, which shows how the model operates.'},\n",
       " {'originalContent': 'Pictorially, GDAs operation Shown been shape orientation, share point, determine put together, imply blue Py1x0.5.',\n",
       "  'correctedSentence': 'Pictorially, the operation of GDA is shown to be a shape and orientation that share a point, which determines the decision boundary. This implies that the blue line is the decision boundary, and Py1x = 0.5.'},\n",
       " {'originalContent': 'boundary, predict outcome, side, y0.',\n",
       "  'correctedSentence': 'The boundary is used to predict the outcome, and the side of the boundary determines the value of y.'},\n",
       " {'originalContent': 'Points upper right closer classifying lower left classified',\n",
       "  'correctedSentence': 'Points in the upper right are closer to being classified as y = 1, while points in the lower left are classified as y = 0.'},\n",
       " {'originalContent': 'Logistic Regression x1 x2 start whose randomly.',\n",
       "  'correctedSentence': 'Logistic regression starts with x1 and x2, whose values are randomly initialised.'},\n",
       " {'originalContent': 'Typically, initialize purpose visualization, starting off shown',\n",
       "  'correctedSentence': 'Typically, we initialise the values for the purpose of visualisation, starting off with the values shown.'},\n",
       " {'originalContent': 'Running single iteration PYX, repositions 20 iterations, converges illustrates green superimposed blue.',\n",
       "  'correctedSentence': 'Running a single iteration of the algorithm, we reposition the decision boundary, and after 20 iterations, the algorithm converges, illustrating the green line superimposed on the blue line.'},\n",
       " {'originalContent': 'arrive slightly boundaries.',\n",
       "  'correctedSentence': 'We arrive at the decision boundary, which is slightly different from the original boundary.'},\n",
       " {'originalContent': 'Why Two Separate Means, Single Matrix?',\n",
       "  'correctedSentence': 'Why do we have two separate means and a single matrix?'},\n",
       " {'originalContent': 'If lot problems.',\n",
       "  'correctedSentence': 'If we have a lot of problems.'},\n",
       " {'originalContent': 'Choosing reasonable, work fine.',\n",
       "  'correctedSentence': 'Choosing a reasonable value will work fine.'},\n",
       " {'originalContent': 'catch, though, youll roughly double isnt anymore.',\n",
       "  'correctedSentence': \"The catch, though, is that you'll roughly double the number of parameters, and it isn't anymore.\"},\n",
       " {'originalContent': 'Discussion Comparing interesting relationship.',\n",
       "  'correctedSentence': 'The discussion compares the interesting relationship between'},\n",
       " {'originalContent': 'youve ,0,1,.',\n",
       "  'correctedSentence': \"You've seen the relationship between 0 and 1.\"},\n",
       " {'originalContent': 'fixed Py1x,0,1,.',\n",
       "  'correctedSentence': 'The fixed value of Py1x is 0 or 1.'},\n",
       " {'originalContent': 'theorem, Pxy1,0,1,Pxy1,0,1,Py1Px,0,1,',\n",
       "  'correctedSentence': 'By the theorem, Pxy1 = Pxy1,0,1,Py1Px,0,1,'},\n",
       " {'originalContent': 'where, Pxy1,0,1, measure evaluating once determined optimum probability, plot Py1x so, simple few',\n",
       "  'correctedSentence': 'where Pxy1 is the measure of evaluating the optimum probability, and we plot Py1x, which is a simple function of a few variables.'},\n",
       " {'originalContent': 'Now, Mappingprojecting Xaxis, bump PXY0 PXY1.',\n",
       "  'correctedSentence': 'Now, we map the X-axis to the bump of PXY0 and PXY1.'},\n",
       " {'originalContent': 'Were Also, note split 5050 across PY1 0.5, known half',\n",
       "  'correctedSentence': 'We also note that the split is 50-50 across Py1 = 0.5, which is known as the half-way point.'},\n",
       " {'originalContent': 'Next, PY1X X.',\n",
       "  'correctedSentence': 'Next, we have Py1x = X.'},\n",
       " {'originalContent': 'unlabeled far Xaxis.',\n",
       "  'correctedSentence': 'The unlabeled points are far away from the X-axis.'},\n",
       " {'originalContent': 'infer point almost certainly came left, generating Xaxis datapoint very small.',\n",
       "  'correctedSentence': 'We can infer that the point almost certainly came from the left, generating a datapoint very close to the X-axis.'},\n",
       " {'originalContent': 'pick Its easy discern PXY10.5.',\n",
       "  'correctedSentence': 'We can easily discern that PXY1 = 0.5.'},\n",
       " {'originalContent': 'right, youd pretty sure class.',\n",
       "  'correctedSentence': \"On the right, you'd be pretty sure of the class.\"},\n",
       " {'originalContent': 'repeat exercise sweeping datapoints dense grid evaluate measure, Youll notice coming approach midpoint, increases 0.5 surpasses 0.5.',\n",
       "  'correctedSentence': \"If we repeat the exercise, sweeping the datapoints across a dense grid and evaluating the measure, you'll notice that as we approach the midpoint, the value increases to 0.5 and surpasses 0.5.\"},\n",
       " {'originalContent': 'Beyond certain tends Plotting connect dots, turns exactly view quantity Py1x,0,1,',\n",
       "  'correctedSentence': 'Beyond a certain point, plotting the connect-the-dots turns exactly into the view of the quantity Py1x, which is 0 or 1.'},\n",
       " {'originalContent': 'find form, Py1x,,0,111expTx appropriate ,,0,1.',\n",
       "  'correctedSentence': 'We find that the form of Py1x is given by the expression 1 / (1 + exp(-Tx)), which is appropriate for 0 or 1.'},\n",
       " {'originalContent': 'Py1x,,0,1.', 'correctedSentence': 'Py1x is 0 or 1.'},\n",
       " {'originalContent': 'convention redefining xis righthandside n1 dimensional adding coordinate xi01.',\n",
       "  'correctedSentence': 'By convention, we redefine the xis to be on the right-hand side, adding a coordinate xi0 = 1.'},\n",
       " {'originalContent': '1x.', 'correctedSentence': 'This is 1 times x.'},\n",
       " {'originalContent': 'While hypothesis function, hood specific choice choosing quite leading',\n",
       "  'correctedSentence': 'While the hypothesis function is a specific choice, choosing it is quite leading.'},\n",
       " {'originalContent': 'Would Prefer One Another?',\n",
       "  'correctedSentence': 'Would you prefer one or the other?'},\n",
       " {'originalContent': 'general, yield boundaries trained',\n",
       "  'correctedSentence': 'In general, the boundaries yielded by the trained models'},\n",
       " {'originalContent': 'Which better?',\n",
       "  'correctedSentence': 'Which one is better?'},\n",
       " {'originalContent': 'discuss superior viceversa.',\n",
       "  'correctedSentence': 'We discuss which one is superior, and vice versa.'},\n",
       " {'originalContent': 'assumes that, xy0 xy1 parameter logistic, governed Py1x11expTx',\n",
       "  'correctedSentence': 'It assumes that xy0 and xy1 are parameters of the logistic function, governed by Py1x = 1 / (1 + exp(-Tx)).'},\n",
       " {'originalContent': 'element x0, raised plotting pointbypoint ultimately yielded curve, shared necessarily assumptions',\n",
       "  'correctedSentence': 'The element x0 is raised to the power of the plotting point-by-point, ultimately yielding a curve that shares the necessary assumptions.'},\n",
       " {'originalContent': '3. converse, implication does Gaussian.',\n",
       "  'correctedSentence': 'The converse is also true, and the implication is that the Gaussian distribution'},\n",
       " {'originalContent': '3 2. stronger prove 2, noted property GDA.',\n",
       "  'correctedSentence': '3 implies 2, and this is a stronger result. We prove 2, and note that this is a property of GDA.'},\n",
       " {'originalContent': 'correct, better regression.',\n",
       "  'correctedSentence': 'This is correct, and better than regression.'},\n",
       " {'originalContent': 'Specifically, indeed asymptotically efficient.',\n",
       "  'correctedSentence': 'Specifically, it is indeed asymptotically efficient.'},\n",
       " {'originalContent': 'strong youre baking information algorithm, Informally, limit large sets m, there no strictly terms of, accurately',\n",
       "  'correctedSentence': \"This is a strong result, and you're baking information into the algorithm. Informally, in the limit of large sets m, there is no strictly better algorithm in terms of accuracy.\"}]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import ndiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = out_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'originalContent': 'Back to Top CS229 Gaussian Discriminant Analysis Generative Learning Algorithms Weve mainly looked at learning algorithms that model Pyx, the conditional distribution of y given x.',\n",
       "  'correctedSentence': \"Back to Top CS229 Gaussian Discriminant Analysis Generative Learning Algorithms We've mainly looked at learning algorithms that model Pyx, the conditional distribution of y given x.\"},\n",
       " {'originalContent': 'For instance, logistic regression modeled Pyx as hxgTx where g is sigmoid function.',\n",
       "  'correctedSentence': 'For instance, logistic regression modelled Pyx as hxgTx where g is sigmoid function.'},\n",
       " {'originalContent': 'In this section, well talk about a different type algorithm.',\n",
       "  'correctedSentence': \"In this section, we'll talk about a different type of algorithm.\"},\n",
       " {'originalContent': 'Lets use binary classification problem motivation behind our discussion.',\n",
       "  'correctedSentence': \"Let's use binary classification problem motivation behind our discussion.\"},\n",
       " {'originalContent': 'Consider in which we want learn distinguish between malignant y1 and benign y0 tumors.',\n",
       "  'correctedSentence': 'Consider in which we want to learn to distinguish between malignant y1 and benign y0 tumours.'},\n",
       " {'originalContent': 'Given training set, an algorithm like , or initially starts with randomly initialized parameters.',\n",
       "  'correctedSentence': 'Given a training set, an algorithm like , or initially starts with randomly initialised parameters.'},\n",
       " {'originalContent': 'Over course learning, performs gradient descent straight line hyperplane decision boundary evolves until you obtain separates positivenegative examples case, Then, classify new sample either benign, it checks on side falls in, makes its prediction accordingly.',\n",
       "  'correctedSentence': 'Over the course of learning, it performs gradient descent on a straight line hyperplane decision boundary, which evolves until it separates positive and negative examples. Then, it classifies a new sample as either benign or malignant, depending on which side it falls in, and makes its prediction accordingly.'},\n",
       " {'originalContent': 'Theres class arent trying maximize likelihood looking both classes searching for separation boundary.',\n",
       "  'correctedSentence': \"There's a class that aren't trying to maximise likelihood, looking at both classes and searching for a separation boundary.\"},\n",
       " {'originalContent': 'Instead, these look one time.',\n",
       "  'correctedSentence': 'Instead, these look at one time.'},\n",
       " {'originalContent': 'First, tumors, can features what tumors like.',\n",
       "  'correctedSentence': 'First, tumours can have features that are like other tumours.'},\n",
       " {'originalContent': 'build separate Finally, tumor, match against tumor model, see whether looks more had seen set.',\n",
       "  'correctedSentence': 'Build separate models for each tumour type. Finally, match the tumour against the tumour model, and see whether it looks more like the tumours in the training set.'},\n",
       " {'originalContent': 'try directly such mapping from input space X labels are called discriminative algorithms.',\n",
       "  'correctedSentence': 'Try to directly map from the input space X to the labels, which are called discriminative algorithms.'},\n",
       " {'originalContent': 'instead Pxy Py.',\n",
       "  'correctedSentence': 'Instead, Pxy = Py.'},\n",
       " {'originalContent': 'These generative if indicates example 0 1, then Pxy0 models features, while Pxy1 features.',\n",
       "  'correctedSentence': 'These generative models indicate that, for example, Pxy0 models the features of class 0, while Pxy1 models the features of class 1.'},\n",
       " {'originalContent': 'The also learns prior Py independent probability y.',\n",
       "  'correctedSentence': 'The model also learns the prior probability Py, which is independent of y.'},\n",
       " {'originalContent': 'To illustrate concept using practical when patient walks into hospital, before doctors even them, odds their versus referred prior.',\n",
       "  'correctedSentence': 'To illustrate this concept, consider a practical example: when a patient walks into a hospital, before the doctors even see them, the odds of their having a particular disease versus not having it are referred to as the prior probability.'},\n",
       " {'originalContent': 'Thus, builds each isolation.',\n",
       "  'correctedSentence': 'Thus, the model builds each in isolation.'},\n",
       " {'originalContent': 'At test time, evaluates models, identifies matches most closely returns prediction.',\n",
       "  'correctedSentence': 'At test time, the model evaluates the models, identifies which one matches most closely, and returns the prediction.'},\n",
       " {'originalContent': 'After modeling priors Pxy, Bayes rule derive posterior x PyxPxyPyPx',\n",
       "  'correctedSentence': \"After modelling the priors Pxy, we can derive the posterior probability x using Bayes' rule: Pyx = PxyPyPx\"},\n",
       " {'originalContent': 'Here, denominator by PxPxy1Py1Pxy0Py0, function quantities',\n",
       "  'correctedSentence': 'Here, the denominator is given by Px = Pxy1Py1 + Pxy0Py0, which is a function of the quantities'},\n",
       " {'originalContent': 'Note learned part process calculating order make prediction, dont actually need calculate value Px since constant, doesnt appear there.',\n",
       "  'correctedSentence': \"Note that the learned part of the process is calculating the order to make a prediction, but we don't actually need to calculate the value of Px since it is a constant and doesn't appear in the final prediction.\"},\n",
       " {'originalContent': 'When making predictions algorithms, thus ignore computing save computation.',\n",
       "  'correctedSentence': 'When making predictions, algorithms thus ignore computing Px to save computation.'},\n",
       " {'originalContent': 'However, end goal value, would compute be able normalize numerator.',\n",
       "  'correctedSentence': 'However, the end goal is to compute the value of Px, which would allow us to normalise the numerator.'},\n",
       " {'originalContent': 'PyxPxyPyPxPxyPy above equation represents underlying framework .',\n",
       "  'correctedSentence': 'The equation Pyx = PxyPyPx represents the underlying framework.'},\n",
       " {'originalContent': 'Key takeaways Discriminative i.e., output input.',\n",
       "  'correctedSentence': 'Key takeaways: discriminative models, i.e., output is a function of input.'},\n",
       " {'originalContent': 'other words, hx0,1 directly.',\n",
       "  'correctedSentence': 'In other words, hx is 0 or 1 directly.'},\n",
       " {'originalContent': 'class, tumoridentification setting, may case them first discriminant analysis GDA, used continuousvalued say, classification.',\n",
       "  'correctedSentence': 'In the case of tumour identification, we may use discriminant analysis (GDA) for continuous-valued classification.'},\n",
       " {'originalContent': 'assume distributed according multivariate normal distribution.',\n",
       "  'correctedSentence': 'We assume that the data is distributed according to a multivariate normal distribution.'},\n",
       " {'originalContent': 'briefly properties distributions moving GDA itself.',\n",
       "  'correctedSentence': 'We will briefly discuss the properties of the distributions and then move on to GDA itself.'},\n",
       " {'originalContent': 'Multivariate Distribution generalization 1dimensional random variable ndimensional simply, nrandom variable.',\n",
       "  'correctedSentence': 'A multivariate distribution is a generalisation of a 1-dimensional random variable to an n-dimensional random variable.'},\n",
       " {'originalContent': 'rather than univariate variable, seeks multiple variables.',\n",
       "  'correctedSentence': 'Rather than a univariate variable, it seeks multiple variables.'},\n",
       " {'originalContent': 'Assume Gaussian, i.e, XRn, parameterized mean vector Rn covariance matrix Rnn, symmetric positive semidefinite.',\n",
       "  'correctedSentence': 'We assume that the data is Gaussian, i.e., X ~ Rn, parameterised by a mean vector in Rn and a covariance matrix in Rnn, which is symmetric and positive semidefinite.'},\n",
       " {'originalContent': 'Formally, written as, XN, density PDF PX,12n212exp12XT1X denotes determinant expected ExxPx,dx vectorvalued defined CovX EXET.',\n",
       "  'correctedSentence': 'Formally, this can be written as X ~ N(μ, Σ), where the density PDF is given by PX(x) = (1/√(2π)^n) * exp(-1/2 * (x-μ)^T * Σ^(-1) * (x-μ)), where μ is the mean vector, Σ is the covariance matrix, and E[x] is the expected value of x.'},\n",
       " {'originalContent': 'Covariance generalizes notion variance realvalued setting.',\n",
       "  'correctedSentence': 'Covariance generalises the notion of variance in the real-valued setting.'},\n",
       " {'originalContent': 'EEET.', 'correctedSentence': 'E[E^T] = E[E]E^T.'},\n",
       " {'originalContent': 'Since CovX.',\n",
       "  'correctedSentence': 'Since Cov(X) = E[(X-E[X])(X-E[X])^T],'},\n",
       " {'originalContent': 'explore some visualize Recall familiar',\n",
       "  'correctedSentence': \"Let's explore some visualisations. Recall the familiar\"},\n",
       " {'originalContent': 'Similarly, multivariable represented same bellshaped curve two parameters control PDF, but ndimensions.',\n",
       "  'correctedSentence': 'Similarly, a multivariate distribution can be represented by the same bell-shaped curve, but with two parameters controlling the PDF in n dimensions.'},\n",
       " {'originalContent': 'e.g., 2dimensional over 2 variables, 2dimensional, size 22.',\n",
       "  'correctedSentence': 'For example, a 2-dimensional distribution over 2 variables has a size of 2x2.'},\n",
       " {'originalContent': 'below figure shows zero 21 zerovector I 22 identity matrix.',\n",
       "  'correctedSentence': 'The figure below shows a zero-mean vector, a 2x1 zero vector, and a 2x2 identity matrix.'},\n",
       " {'originalContent': 'A standard 0.6I.',\n",
       "  'correctedSentence': 'A standard deviation of 0.6I.'},\n",
       " {'originalContent': 'essentially taken multiplied number has shrunk variance, reduced variability 2I.',\n",
       "  'correctedSentence': 'Essentially, the number has been multiplied by 0.6, which has shrunk the variance and reduced the variability. 2I.'},\n",
       " {'originalContent': 'From images, becomes larger, widershorter, smaller, compressedtaller.',\n",
       "  'correctedSentence': 'From the images, we can see that as the covariance matrix becomes larger, the distribution becomes wider and shorter, and as it becomes smaller, the distribution becomes compressed and taller.'},\n",
       " {'originalContent': 'This because always integrates area under so scales spread vs. height figures show Gaussians corresponding matrices figures, increase offdiagonal entries compressed towards 45 x1x2.',\n",
       "  'correctedSentence': 'This is because the area under the curve always integrates to 1, so as the spread increases, the height decreases, and vice versa. The figures show Gaussians corresponding to the matrices, and as the off-diagonal entries increase, the distribution becomes compressed towards the 45° line between x1 and x2.'},\n",
       " {'originalContent': 'Geometrically speaking, implies variables positively correlated.',\n",
       "  'correctedSentence': 'Geometrically speaking, this implies that the variables are positively correlated.'},\n",
       " {'originalContent': 'We clearly contours three densities 3D bumps saw above, should perfectly round circles, aspect ratio image probably little bit fatter places being',\n",
       "  'correctedSentence': 'We can clearly see the contours of the three densities, which are 3D bumps. The aspect ratio of the image is probably a little bit fatter in some places, but the contours should be perfectly round circles.'},\n",
       " {'originalContent': 'Heres another set along decreasing elements matrix, now again, opposite direction, 135 line.',\n",
       "  'correctedSentence': \"Here's another set of images, this time with decreasing elements of the matrix, and again, in the opposite direction, along the 135° line.\"},\n",
       " {'originalContent': 'Again, geometrically, endow negative correlation.',\n",
       "  'correctedSentence': 'Again, geometrically, this endows the variables with a negative correlation.'},\n",
       " {'originalContent': 'vary parameters, tend form ellipses.',\n",
       "  'correctedSentence': 'As we vary the parameters, the distribution tends to form ellipses.'},\n",
       " {'originalContent': 'By varying shift center around.',\n",
       "  'correctedSentence': 'By varying the shift of the center around.'},\n",
       " {'originalContent': 'Another values I, visualization PDFs follows carry out eigenvectors points principal axes ellipse contours.',\n",
       "  'correctedSentence': 'Another way to visualise the PDFs is to carry out the eigenvectors and points of the principal axes of the ellipse contours.'},\n",
       " {'originalContent': 'takeaway As density, change spreadheight respectively.',\n",
       "  'correctedSentence': 'The takeaway is that as the density changes, the spread and height change respectively.'},\n",
       " {'originalContent': 'Model have consider task discussion, expressed equations Pxy1, Gaussians.',\n",
       "  'correctedSentence': 'The model we have been considering for this task is expressed in terms of equations for Pxy1, which are Gaussians.'},\n",
       " {'originalContent': 'On hand, Bernoulli takes 0,1.',\n",
       "  'correctedSentence': 'On the other hand, the Bernoulli distribution takes values 0 and 1.'},\n",
       " {'originalContent': 'by, xy0N0,xy1N1,yBernoulli 0, classes, vectors 1.',\n",
       "  'correctedSentence': 'By the way, xy0 ~ N0, xy1 ~ N1, and y ~ Bernoulli(0, 1), where the classes are represented by vectors of length 1.'},\n",
       " {'originalContent': 'Put were assuming representing means.',\n",
       "  'correctedSentence': 'Put simply, we were assuming that the means are represented by'},\n",
       " {'originalContent': 'You not commonly seen.',\n",
       "  'correctedSentence': 'You may not have commonly seen this.'},\n",
       " {'originalContent': 'More section below.',\n",
       "  'correctedSentence': 'More on this in the section below.'},\n",
       " {'originalContent': 'fit your data, will define data.',\n",
       "  'correctedSentence': 'To fit your data, we will define the data.'},\n",
       " {'originalContent': 'R, 0Rn, 1Rn, Rnn.',\n",
       "  'correctedSentence': 'R, 0 in Rn, 1 in Rn, and Rnn.'},\n",
       " {'originalContent': 'Writing distributions, Pyy11yPxy012n212exp12x0T1x0Pxy112n212exp12x1T1x1',\n",
       "  'correctedSentence': 'Writing the distributions, Py(y=1) = Pxy0 ~ N(0, 1), Py(y=1) = Pxy1 ~ N(1, 1)'},\n",
       " {'originalContent': 'exponential notation similar earlier Plugging Pxy0,Pxy1,Py0 Py1 formula easily ascertain particular example.',\n",
       "  'correctedSentence': 'Using exponential notation similar to earlier, plugging in Pxy0, Pxy1, Py0, and Py1 into the formula, we can easily ascertain the particular example.'},\n",
       " {'originalContent': 'Suppose xi,yimi1.',\n",
       "  'correctedSentence': 'Suppose we have xi and yi, for i = 1 to m.'},\n",
       " {'originalContent': 'aforementioned going joint L, L,0,1,mi1Pxi,yi,0,1,mi1Pxiyi0,1,Pyi',\n",
       "  'correctedSentence': 'The aforementioned joint likelihood L is given by L = ∏[P(xi, yi) for i = 1 to m] = ∏[P(xi|yi)P(yi) for i = 1 to m]'},\n",
       " {'originalContent': 'big difference cost functions compared choose ,0,1,, Px,y,0,1,.',\n",
       "  'correctedSentence': 'There is a big difference between the cost functions. We choose to maximise the likelihood of the data, which is given by L = ∏[P(xi, yi) for i = 1 to m].'},\n",
       " {'originalContent': 'linear regression, generalized Pyx.',\n",
       "  'correctedSentence': 'In linear regression, we generalise Pyx.'},\n",
       " {'originalContent': 'loglikelihood data simply log L.',\n",
       "  'correctedSentence': 'The log-likelihood of the data is simply log L.'},\n",
       " {'originalContent': ',0,1,logmi1Pxi,yi,0,1,logmi1Pxiyi0,1,Pyi',\n",
       "  'correctedSentence': 'The log-likelihood is given by log L = ∑[log P(xi, yi) for i = 1 to m] = ∑[log P(xi|yi)P(yi) for i = 1 to m]'},\n",
       " {'originalContent': 'respect take derivative equal solve expression yields maximum estimate be, mi1yim',\n",
       "  'correctedSentence': 'Taking the derivative with respect to the parameters and setting it equal to zero, we can solve for the expression that yields the maximum likelihood estimate, which is given by μ = (1/m) * ∑[yi * xi for i = 1 to m]'},\n",
       " {'originalContent': 'An intuitive explanation around maximizes follows.',\n",
       "  'correctedSentence': 'An intuitive explanation of how this maximises the likelihood follows.'},\n",
       " {'originalContent': 'example, chance next office denoted bias coin toss fraction heads Likewise, just label y1.',\n",
       "  'correctedSentence': 'For example, the chance of the next office being occupied is denoted by the bias of a coin toss, which is the fraction of heads. Likewise, we just label y as 1.'},\n",
       " {'originalContent': 'way write indicator notation, mi11yi1m 1 argument true, otherwise 0.',\n",
       "  'correctedSentence': 'We can write this in indicator notation as 1(yi = 1) = 1 if the argument is true, and 0 otherwise.'},\n",
       " {'originalContent': 'true statement false equivalent ifstatement programming context.',\n",
       "  'correctedSentence': 'This is equivalent to a true statement being false in an if-statement in a programming context.'},\n",
       " {'originalContent': '0,1 is, 0mi11yi0ximi11yi01mi11yi1',\n",
       "  'correctedSentence': '0 or 1 is given by 0 if yi = 0, and 1 if yi = 1.'},\n",
       " {'originalContent': 'intuition expression, think all say dataset.',\n",
       "  'correctedSentence': 'The intuition behind this expression is to think about all the data points in the dataset.'},\n",
       " {'originalContent': 'reasonable examples, 0s average writing intuition.',\n",
       "  'correctedSentence': 'A reasonable example is to take the average of the 0s and 1s in the dataset.'},\n",
       " {'originalContent': 'numerator sum feature sums entire summing i1m, uses instances yi0 times xi.',\n",
       "  'correctedSentence': 'The numerator is the sum of the feature sums over the entire dataset, summing over all instances where yi = 0, and using the instances where yi = 1 times xi.'},\n",
       " {'originalContent': 'effect term whereas effectively zeroing term.',\n",
       "  'correctedSentence': 'This has the effect of zeroing out the term.'},\n",
       " {'originalContent': 'up samples yi0.',\n",
       "  'correctedSentence': 'This is because we are summing over all samples where yi = 0.'},\n",
       " {'originalContent': 'count represent Because every yi0, get extra sum, ends total examples.',\n",
       "  'correctedSentence': 'This is because every time yi = 0, we get an extra sum, which ends up being the total number of examples.'},\n",
       " {'originalContent': 'made 1mmi1xiyixiT fits means classes.',\n",
       "  'correctedSentence': 'This is made up of 1/m times the sum of xi times yi, which fits the means of the classes.'},\n",
       " {'originalContent': 'Making Predictions Using minz min z minimum any possible z.',\n",
       "  'correctedSentence': 'Making predictions using the minimum of z, where z is any possible value.'},\n",
       " {'originalContent': 'argminz argmin refers plugged expression.',\n",
       "  'correctedSentence': 'The argmin of z refers to the value of z that minimises the expression.'},\n",
       " {'originalContent': 'minzz52 attained z520 argminzz52 led z52, z5.',\n",
       "  'correctedSentence': 'The minimum of z is attained when z = 5, and the argmin of z is 5.'},\n",
       " {'originalContent': 'maxz argmaxz operators exist they deal leads having lets how go',\n",
       "  'correctedSentence': \"The max of z and the argmax of z are operators that exist, and they deal with the maximum value of z. Let's see how to go about finding the maximum value of z.\"},\n",
       " {'originalContent': 'So do benign?',\n",
       "  'correctedSentence': 'So, do we predict benign?'},\n",
       " {'originalContent': 'boils down predicting likely x, formally argmaxyPyx',\n",
       "  'correctedSentence': 'This boils down to predicting the most likely value of x, which is given by the argmax of Pyx.'},\n",
       " {'originalContent': 'Looking deeper y0,1 rule, argmaxyPyxargmaxyPxyPyPx',\n",
       "  'correctedSentence': 'Looking deeper, we can see that the rule for predicting y is given by the argmax of Pyx, which is equal to the argmax of PxyPyPx.'},\n",
       " {'originalContent': 'reasons highlighted Visualization dataset compareandcontrast operate',\n",
       "  'correctedSentence': 'The reasons for this are highlighted in the visualisation of the dataset, which shows how the model operates.'},\n",
       " {'originalContent': 'Pictorially, GDAs operation Shown been shape orientation, share point, determine put together, imply blue Py1x0.5.',\n",
       "  'correctedSentence': 'Pictorially, the operation of GDA is shown to be a shape and orientation that share a point, which determines the decision boundary. This implies that the blue line is the decision boundary, and Py1x = 0.5.'},\n",
       " {'originalContent': 'boundary, predict outcome, side, y0.',\n",
       "  'correctedSentence': 'The boundary is used to predict the outcome, and the side of the boundary determines the value of y.'},\n",
       " {'originalContent': 'Points upper right closer classifying lower left classified',\n",
       "  'correctedSentence': 'Points in the upper right are closer to being classified as y = 1, while points in the lower left are classified as y = 0.'},\n",
       " {'originalContent': 'Logistic Regression x1 x2 start whose randomly.',\n",
       "  'correctedSentence': 'Logistic regression starts with x1 and x2, whose values are randomly initialised.'},\n",
       " {'originalContent': 'Typically, initialize purpose visualization, starting off shown',\n",
       "  'correctedSentence': 'Typically, we initialise the values for the purpose of visualisation, starting off with the values shown.'},\n",
       " {'originalContent': 'Running single iteration PYX, repositions 20 iterations, converges illustrates green superimposed blue.',\n",
       "  'correctedSentence': 'Running a single iteration of the algorithm, we reposition the decision boundary, and after 20 iterations, the algorithm converges, illustrating the green line superimposed on the blue line.'},\n",
       " {'originalContent': 'arrive slightly boundaries.',\n",
       "  'correctedSentence': 'We arrive at the decision boundary, which is slightly different from the original boundary.'},\n",
       " {'originalContent': 'Why Two Separate Means, Single Matrix?',\n",
       "  'correctedSentence': 'Why do we have two separate means and a single matrix?'},\n",
       " {'originalContent': 'If lot problems.',\n",
       "  'correctedSentence': 'If we have a lot of problems.'},\n",
       " {'originalContent': 'Choosing reasonable, work fine.',\n",
       "  'correctedSentence': 'Choosing a reasonable value will work fine.'},\n",
       " {'originalContent': 'catch, though, youll roughly double isnt anymore.',\n",
       "  'correctedSentence': \"The catch, though, is that you'll roughly double the number of parameters, and it isn't anymore.\"},\n",
       " {'originalContent': 'Discussion Comparing interesting relationship.',\n",
       "  'correctedSentence': 'The discussion compares the interesting relationship between'},\n",
       " {'originalContent': 'youve ,0,1,.',\n",
       "  'correctedSentence': \"You've seen the relationship between 0 and 1.\"},\n",
       " {'originalContent': 'fixed Py1x,0,1,.',\n",
       "  'correctedSentence': 'The fixed value of Py1x is 0 or 1.'},\n",
       " {'originalContent': 'theorem, Pxy1,0,1,Pxy1,0,1,Py1Px,0,1,',\n",
       "  'correctedSentence': 'By the theorem, Pxy1 = Pxy1,0,1,Py1Px,0,1,'},\n",
       " {'originalContent': 'where, Pxy1,0,1, measure evaluating once determined optimum probability, plot Py1x so, simple few',\n",
       "  'correctedSentence': 'where Pxy1 is the measure of evaluating the optimum probability, and we plot Py1x, which is a simple function of a few variables.'},\n",
       " {'originalContent': 'Now, Mappingprojecting Xaxis, bump PXY0 PXY1.',\n",
       "  'correctedSentence': 'Now, we map the X-axis to the bump of PXY0 and PXY1.'},\n",
       " {'originalContent': 'Were Also, note split 5050 across PY1 0.5, known half',\n",
       "  'correctedSentence': 'We also note that the split is 50-50 across Py1 = 0.5, which is known as the half-way point.'},\n",
       " {'originalContent': 'Next, PY1X X.',\n",
       "  'correctedSentence': 'Next, we have Py1x = X.'},\n",
       " {'originalContent': 'unlabeled far Xaxis.',\n",
       "  'correctedSentence': 'The unlabeled points are far away from the X-axis.'},\n",
       " {'originalContent': 'infer point almost certainly came left, generating Xaxis datapoint very small.',\n",
       "  'correctedSentence': 'We can infer that the point almost certainly came from the left, generating a datapoint very close to the X-axis.'},\n",
       " {'originalContent': 'pick Its easy discern PXY10.5.',\n",
       "  'correctedSentence': 'We can easily discern that PXY1 = 0.5.'},\n",
       " {'originalContent': 'right, youd pretty sure class.',\n",
       "  'correctedSentence': \"On the right, you'd be pretty sure of the class.\"},\n",
       " {'originalContent': 'repeat exercise sweeping datapoints dense grid evaluate measure, Youll notice coming approach midpoint, increases 0.5 surpasses 0.5.',\n",
       "  'correctedSentence': \"If we repeat the exercise, sweeping the datapoints across a dense grid and evaluating the measure, you'll notice that as we approach the midpoint, the value increases to 0.5 and surpasses 0.5.\"},\n",
       " {'originalContent': 'Beyond certain tends Plotting connect dots, turns exactly view quantity Py1x,0,1,',\n",
       "  'correctedSentence': 'Beyond a certain point, plotting the connect-the-dots turns exactly into the view of the quantity Py1x, which is 0 or 1.'},\n",
       " {'originalContent': 'find form, Py1x,,0,111expTx appropriate ,,0,1.',\n",
       "  'correctedSentence': 'We find that the form of Py1x is given by the expression 1 / (1 + exp(-Tx)), which is appropriate for 0 or 1.'},\n",
       " {'originalContent': 'Py1x,,0,1.', 'correctedSentence': 'Py1x is 0 or 1.'},\n",
       " {'originalContent': 'convention redefining xis righthandside n1 dimensional adding coordinate xi01.',\n",
       "  'correctedSentence': 'By convention, we redefine the xis to be on the right-hand side, adding a coordinate xi0 = 1.'},\n",
       " {'originalContent': '1x.', 'correctedSentence': 'This is 1 times x.'},\n",
       " {'originalContent': 'While hypothesis function, hood specific choice choosing quite leading',\n",
       "  'correctedSentence': 'While the hypothesis function is a specific choice, choosing it is quite leading.'},\n",
       " {'originalContent': 'Would Prefer One Another?',\n",
       "  'correctedSentence': 'Would you prefer one or the other?'},\n",
       " {'originalContent': 'general, yield boundaries trained',\n",
       "  'correctedSentence': 'In general, the boundaries yielded by the trained models'},\n",
       " {'originalContent': 'Which better?',\n",
       "  'correctedSentence': 'Which one is better?'},\n",
       " {'originalContent': 'discuss superior viceversa.',\n",
       "  'correctedSentence': 'We discuss which one is superior, and vice versa.'},\n",
       " {'originalContent': 'assumes that, xy0 xy1 parameter logistic, governed Py1x11expTx',\n",
       "  'correctedSentence': 'It assumes that xy0 and xy1 are parameters of the logistic function, governed by Py1x = 1 / (1 + exp(-Tx)).'},\n",
       " {'originalContent': 'element x0, raised plotting pointbypoint ultimately yielded curve, shared necessarily assumptions',\n",
       "  'correctedSentence': 'The element x0 is raised to the power of the plotting point-by-point, ultimately yielding a curve that shares the necessary assumptions.'},\n",
       " {'originalContent': '3. converse, implication does Gaussian.',\n",
       "  'correctedSentence': 'The converse is also true, and the implication is that the Gaussian distribution'},\n",
       " {'originalContent': '3 2. stronger prove 2, noted property GDA.',\n",
       "  'correctedSentence': '3 implies 2, and this is a stronger result. We prove 2, and note that this is a property of GDA.'},\n",
       " {'originalContent': 'correct, better regression.',\n",
       "  'correctedSentence': 'This is correct, and better than regression.'},\n",
       " {'originalContent': 'Specifically, indeed asymptotically efficient.',\n",
       "  'correctedSentence': 'Specifically, it is indeed asymptotically efficient.'},\n",
       " {'originalContent': 'strong youre baking information algorithm, Informally, limit large sets m, there no strictly terms of, accurately',\n",
       "  'correctedSentence': \"This is a strong result, and you're baking information into the algorithm. Informally, in the limit of large sets m, there is no strictly better algorithm in terms of accuracy.\"}]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the <b> corrected </b> text\n"
     ]
    }
   ],
   "source": [
    "from difflib import ndiff\n",
    "\n",
    "def highlight_changes(original, corrected):\n",
    "    diff = list(ndiff(original.split(), corrected.split()))\n",
    "    result = []\n",
    "    in_change = False\n",
    "\n",
    "    for d in diff:\n",
    "        if d.startswith('- '):\n",
    "            continue\n",
    "        elif d.startswith('+ '):\n",
    "            if not in_change:\n",
    "                result.append('<b>')\n",
    "                in_change = True\n",
    "            result.append(d[2:])\n",
    "        elif d.startswith('  '):\n",
    "            if in_change:\n",
    "                result.append('</b>')\n",
    "                in_change = False\n",
    "            result.append(d[2:])\n",
    "    \n",
    "    if in_change:\n",
    "        result.append('</b>')\n",
    "\n",
    "    return ' '.join(result)\n",
    "\n",
    "# Test\n",
    "original_text = \"This is the original text\"\n",
    "corrected_text = \"This is the corrected text\"\n",
    "print(highlight_changes(original_text, corrected_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Back to Top CS229 Gaussian Discriminant Analysis Generative Learning Algorithms <b> We've </b> mainly looked at learning algorithms that model Pyx, the conditional distribution of y given x.\",\n",
       " 'For instance, logistic regression <b> modelled </b> Pyx as hxgTx where g is sigmoid function.',\n",
       " \"In this section, <b> we'll </b> talk about a different type <b> of </b> algorithm.\",\n",
       " \"<b> Let's </b> use binary classification problem motivation behind our discussion.\",\n",
       " 'Consider in which we want <b> to </b> learn <b> to </b> distinguish between malignant y1 and benign y0 <b> tumours. </b>',\n",
       " 'Given <b> a </b> training set, an algorithm like , or initially starts with randomly <b> initialised </b> parameters.',\n",
       " 'Over <b> the </b> course <b> of </b> learning, <b> it </b> performs gradient descent <b> on a </b> straight line hyperplane decision <b> boundary, which </b> evolves until <b> it </b> separates <b> positive and negative examples. </b> Then, <b> it classifies a </b> new sample <b> as </b> either <b> benign or malignant, depending on which side </b> it falls in, <b> and </b> makes its prediction accordingly.',\n",
       " \"<b> There's a </b> class <b> that aren't </b> trying <b> to maximise likelihood, </b> looking <b> at </b> both classes <b> and </b> searching for <b> a </b> separation boundary.\",\n",
       " 'Instead, these look <b> at </b> one time.',\n",
       " 'First, <b> tumours </b> can <b> have </b> features <b> that are like other tumours. </b>',\n",
       " '<b> Build </b> separate <b> models for each tumour type. </b> Finally, match <b> the tumour </b> against <b> the tumour </b> model, <b> and </b> see whether <b> it </b> looks more <b> like the tumours in the training </b> set.',\n",
       " '<b> Try to </b> directly <b> map </b> from <b> the </b> input space X <b> to the labels, which </b> are called discriminative algorithms.',\n",
       " '<b> Instead, </b> Pxy <b> = </b> Py.',\n",
       " 'These generative <b> models indicate that, for example, </b> Pxy0 models <b> the features of class 0, </b> while Pxy1 <b> models the features of class 1. </b>',\n",
       " 'The <b> model </b> also learns <b> the </b> prior <b> probability Py, which is </b> independent <b> of </b> y.',\n",
       " 'To illustrate <b> this concept, consider a </b> practical <b> example: </b> when <b> a </b> patient walks into <b> a </b> hospital, before <b> the </b> doctors even <b> see </b> them, <b> the </b> odds <b> of </b> their <b> having a particular disease </b> versus <b> not having it are </b> referred <b> to as the prior probability. </b>',\n",
       " 'Thus, <b> the model </b> builds each <b> in </b> isolation.',\n",
       " 'At test time, <b> the model </b> evaluates <b> the </b> models, identifies <b> which one </b> matches most <b> closely, and </b> returns <b> the </b> prediction.',\n",
       " \"After <b> modelling the </b> priors Pxy, <b> we can </b> derive <b> the </b> posterior <b> probability </b> x <b> using Bayes' rule: Pyx = PxyPyPx </b>\",\n",
       " 'Here, <b> the </b> denominator <b> is given </b> by <b> Px = Pxy1Py1 + Pxy0Py0, which is a </b> function <b> of the </b> quantities',\n",
       " \"Note <b> that the </b> learned part <b> of the </b> process <b> is </b> calculating <b> the </b> order <b> to </b> make <b> a </b> prediction, <b> but we don't </b> actually need <b> to </b> calculate <b> the </b> value <b> of </b> Px since <b> it is a constant and doesn't </b> appear <b> in the final prediction. </b>\",\n",
       " 'When making <b> predictions, algorithms </b> thus ignore computing <b> Px to </b> save computation.',\n",
       " 'However, <b> the </b> end goal <b> is to compute the value of Px, which </b> would <b> allow us to normalise the </b> numerator.',\n",
       " '<b> The </b> equation <b> Pyx = PxyPyPx </b> represents <b> the </b> underlying <b> framework. </b>',\n",
       " 'Key <b> takeaways: discriminative models, </b> i.e., output <b> is a function of </b> input.',\n",
       " '<b> In </b> other words, <b> hx is 0 or 1 </b> directly.',\n",
       " '<b> In the case of tumour identification, we </b> may <b> use </b> discriminant analysis <b> (GDA) for continuous-valued </b> classification.',\n",
       " '<b> We </b> assume <b> that the data is </b> distributed according <b> to a </b> multivariate normal distribution.',\n",
       " '<b> We will </b> briefly <b> discuss the </b> properties <b> of the </b> distributions <b> and then move on to </b> GDA itself.',\n",
       " '<b> A multivariate distribution is a generalisation of a 1-dimensional </b> random variable <b> to an n-dimensional random </b> variable.',\n",
       " '<b> Rather </b> than <b> a </b> univariate variable, <b> it </b> seeks multiple variables.',\n",
       " '<b> We assume that the data is </b> Gaussian, <b> i.e., X ~ Rn, parameterised by a </b> mean vector <b> in </b> Rn <b> and a </b> covariance matrix <b> in </b> Rnn, <b> which is </b> symmetric <b> and </b> positive semidefinite.',\n",
       " 'Formally, <b> this can be </b> written <b> as X ~ N(μ, Σ), where the </b> density PDF <b> is given by PX(x) = (1/√(2π)^n) * exp(-1/2 * (x-μ)^T * Σ^(-1) * (x-μ)), where μ is the mean vector, Σ is the covariance matrix, and E[x] is the </b> expected <b> value of x. </b>',\n",
       " 'Covariance <b> generalises the </b> notion <b> of </b> variance <b> in the real-valued </b> setting.',\n",
       " '<b> E[E^T] = E[E]E^T. </b>',\n",
       " 'Since <b> Cov(X) = E[(X-E[X])(X-E[X])^T], </b>',\n",
       " \"<b> Let's </b> explore some <b> visualisations. </b> Recall <b> the </b> familiar\",\n",
       " 'Similarly, <b> a multivariate distribution can be </b> represented <b> by the </b> same <b> bell-shaped curve, but with </b> two parameters <b> controlling the PDF in n dimensions. </b>',\n",
       " '<b> For example, a 2-dimensional distribution </b> over 2 <b> variables has a </b> size <b> of 2x2. </b>',\n",
       " '<b> The figure </b> below shows <b> a zero-mean vector, a 2x1 </b> zero <b> vector, and a 2x2 </b> identity matrix.',\n",
       " 'A standard <b> deviation of </b> 0.6I.',\n",
       " '<b> Essentially, the </b> number has <b> been multiplied by 0.6, which has </b> shrunk <b> the variance and </b> reduced <b> the variability. </b> 2I.',\n",
       " 'From <b> the </b> images, <b> we can see that as the covariance matrix </b> becomes larger, <b> the distribution becomes wider and shorter, and as it becomes </b> smaller, <b> the distribution becomes compressed and taller. </b>',\n",
       " 'This <b> is </b> because <b> the area under the curve </b> always integrates <b> to 1, </b> so <b> as the </b> spread <b> increases, the </b> height <b> decreases, and vice versa. The </b> figures show Gaussians corresponding <b> to the matrices, and as the off-diagonal </b> entries <b> increase, the distribution becomes </b> compressed towards <b> the 45° line between x1 and x2. </b>',\n",
       " 'Geometrically speaking, <b> this </b> implies <b> that the </b> variables <b> are </b> positively correlated.',\n",
       " 'We <b> can </b> clearly <b> see the </b> contours <b> of the </b> three <b> densities, which are </b> 3D <b> bumps. The </b> aspect ratio <b> of the </b> image <b> is </b> probably <b> a </b> little bit fatter <b> in some places, but the contours should be perfectly round circles. </b>',\n",
       " \"<b> Here's </b> another set <b> of images, this time with </b> decreasing elements <b> of the </b> matrix, <b> and </b> again, <b> in the </b> opposite direction, <b> along the 135° </b> line.\",\n",
       " 'Again, geometrically, <b> this endows the variables with a </b> negative correlation.',\n",
       " '<b> As we </b> vary <b> the </b> parameters, <b> the distribution tends to </b> form ellipses.',\n",
       " 'By varying <b> the </b> shift <b> of the </b> center around.',\n",
       " 'Another <b> way to visualise the </b> PDFs <b> is to </b> carry out <b> the </b> eigenvectors <b> and </b> points <b> of the </b> principal axes <b> of the </b> ellipse contours.',\n",
       " '<b> The </b> takeaway <b> is that as the density changes, the spread and height </b> change respectively.',\n",
       " '<b> The model we </b> have <b> been considering for this </b> task <b> is </b> expressed <b> in terms of </b> equations <b> for </b> Pxy1, <b> which are </b> Gaussians.',\n",
       " 'On <b> the other </b> hand, <b> the </b> Bernoulli <b> distribution </b> takes <b> values 0 and 1. </b>',\n",
       " '<b> By the way, xy0 ~ N0, xy1 ~ N1, and y ~ Bernoulli(0, 1), where the classes are represented by </b> vectors <b> of length </b> 1.',\n",
       " 'Put <b> simply, we </b> were assuming <b> that the means are represented by </b>',\n",
       " 'You <b> may </b> not <b> have </b> commonly <b> seen this. </b>',\n",
       " 'More <b> on this in the </b> section below.',\n",
       " '<b> To </b> fit your data, <b> we </b> will define <b> the </b> data.',\n",
       " 'R, <b> 0 in Rn, 1 in Rn, and </b> Rnn.',\n",
       " 'Writing <b> the </b> distributions, <b> Py(y=1) = Pxy0 ~ N(0, 1), Py(y=1) = Pxy1 ~ N(1, 1) </b>',\n",
       " '<b> Using </b> exponential notation similar <b> to earlier, plugging in Pxy0, Pxy1, Py0, and </b> Py1 <b> into the formula, we can </b> easily ascertain <b> the </b> particular example.',\n",
       " 'Suppose <b> we have xi and yi, for i = 1 to m. </b>',\n",
       " '<b> The </b> aforementioned joint <b> likelihood L is given by L = ∏[P(xi, yi) for i = 1 to m] = ∏[P(xi|yi)P(yi) for i = 1 to m] </b>',\n",
       " '<b> There is a </b> big difference <b> between the </b> cost <b> functions. We </b> choose <b> to maximise the likelihood of the data, which is given by L = ∏[P(xi, yi) for i = 1 to m]. </b>',\n",
       " '<b> In </b> linear regression, <b> we generalise </b> Pyx.',\n",
       " '<b> The log-likelihood of the </b> data <b> is </b> simply log L.',\n",
       " '<b> The log-likelihood is given by log L = ∑[log P(xi, yi) for i = 1 to m] = ∑[log P(xi|yi)P(yi) for i = 1 to m] </b>',\n",
       " '<b> Taking the derivative with </b> respect <b> to the parameters and setting it </b> equal <b> to zero, we can </b> solve <b> for the </b> expression <b> that </b> yields <b> the </b> maximum <b> likelihood estimate, which is given by μ = (1/m) * ∑[yi * xi for i = 1 to m] </b>',\n",
       " 'An intuitive explanation <b> of how this maximises the likelihood </b> follows.',\n",
       " '<b> For </b> example, <b> the </b> chance <b> of the </b> next office <b> being occupied is </b> denoted <b> by the </b> bias <b> of a </b> coin <b> toss, which is the </b> fraction <b> of heads. </b> Likewise, <b> we </b> just label <b> y as 1. </b>',\n",
       " '<b> We can </b> write <b> this in </b> indicator <b> notation as 1(yi = 1) = </b> 1 <b> if the </b> argument <b> is </b> true, <b> and 0 otherwise. </b>',\n",
       " '<b> This is equivalent to a </b> true statement <b> being </b> false <b> in an if-statement in a </b> programming context.',\n",
       " '<b> 0 or 1 is given by 0 if yi = 0, and 1 if yi = 1. </b>',\n",
       " '<b> The </b> intuition <b> behind this expression is to </b> think <b> about </b> all <b> the data points in the </b> dataset.',\n",
       " '<b> A </b> reasonable <b> example is to take the average of the </b> 0s <b> and 1s in the dataset. </b>',\n",
       " '<b> The </b> numerator <b> is the </b> sum <b> of the </b> feature sums <b> over the </b> entire <b> dataset, </b> summing <b> over all </b> instances <b> where yi = 0, and using the instances where yi = 1 </b> times xi.',\n",
       " '<b> This has the </b> effect <b> of </b> zeroing <b> out the </b> term.',\n",
       " '<b> This is because we are summing over all </b> samples <b> where yi = 0. </b>',\n",
       " '<b> This is because </b> every <b> time yi = 0, we </b> get <b> an </b> extra sum, <b> which </b> ends <b> up being the </b> total <b> number of </b> examples.',\n",
       " '<b> This is </b> made <b> up of 1/m times the sum of xi times yi, which </b> fits <b> the </b> means <b> of the </b> classes.',\n",
       " 'Making <b> predictions using the minimum of z, where </b> z <b> is </b> any possible <b> value. </b>',\n",
       " '<b> The </b> argmin <b> of z </b> refers <b> to the value of z that minimises the </b> expression.',\n",
       " '<b> The minimum of z is </b> attained <b> when z = 5, and the argmin of z is 5. </b>',\n",
       " \"<b> The max of z and the argmax of z are </b> operators <b> that exist, and </b> they deal <b> with the maximum value of z. Let's see </b> how <b> to </b> go <b> about finding the maximum value of z. </b>\",\n",
       " '<b> So, </b> do <b> we predict </b> benign?',\n",
       " '<b> This </b> boils down <b> to </b> predicting <b> the most </b> likely <b> value of </b> x, <b> which is given by the argmax of Pyx. </b>',\n",
       " 'Looking <b> deeper, we can see that the rule for predicting y is given by the argmax of Pyx, which is equal to the argmax of PxyPyPx. </b>',\n",
       " '<b> The </b> reasons <b> for this are </b> highlighted <b> in the visualisation of the dataset, which shows how the model operates. </b>',\n",
       " 'Pictorially, <b> the </b> operation <b> of GDA is shown to be a </b> shape <b> and orientation that </b> share <b> a </b> point, <b> which determines the decision boundary. This implies that the </b> blue <b> line is the decision boundary, and Py1x = 0.5. </b>',\n",
       " '<b> The boundary is used to </b> predict <b> the </b> outcome, <b> and the side of the boundary determines the value of y. </b>',\n",
       " 'Points <b> in the </b> upper right <b> are </b> closer <b> to being classified as y = 1, while points in the </b> lower left <b> are </b> classified <b> as y = 0. </b>',\n",
       " 'Logistic <b> regression starts with </b> x1 <b> and x2, </b> whose <b> values are randomly initialised. </b>',\n",
       " 'Typically, <b> we initialise the values for the </b> purpose <b> of visualisation, </b> starting off <b> with the values shown. </b>',\n",
       " 'Running <b> a </b> single iteration <b> of the algorithm, we reposition the decision boundary, and after </b> 20 iterations, <b> the algorithm converges, illustrating the </b> green <b> line </b> superimposed <b> on the blue line. </b>',\n",
       " '<b> We </b> arrive <b> at the decision boundary, which is </b> slightly <b> different from the original boundary. </b>',\n",
       " 'Why <b> do we have two separate means and a single matrix? </b>',\n",
       " 'If <b> we have a </b> lot <b> of </b> problems.',\n",
       " 'Choosing <b> a reasonable value will </b> work fine.',\n",
       " \"<b> The </b> catch, though, <b> is that you'll </b> roughly double <b> the number of parameters, and it isn't </b> anymore.\",\n",
       " '<b> The discussion compares the </b> interesting <b> relationship between </b>',\n",
       " \"<b> You've seen the relationship between 0 and 1. </b>\",\n",
       " '<b> The </b> fixed <b> value of Py1x is 0 or 1. </b>',\n",
       " '<b> By the </b> theorem, <b> Pxy1 = Pxy1,0,1,Py1Px,0,1, </b>',\n",
       " '<b> where Pxy1 is the </b> measure <b> of </b> evaluating <b> the </b> optimum probability, <b> and we </b> plot <b> Py1x, which is a </b> simple <b> function of a </b> few <b> variables. </b>',\n",
       " 'Now, <b> we map the X-axis to the </b> bump <b> of </b> PXY0 <b> and </b> PXY1.',\n",
       " '<b> We also </b> note <b> that the </b> split <b> is 50-50 </b> across <b> Py1 = </b> 0.5, <b> which is </b> known <b> as the half-way point. </b>',\n",
       " 'Next, <b> we have Py1x = </b> X.',\n",
       " '<b> The </b> unlabeled <b> points are </b> far <b> away from the X-axis. </b>',\n",
       " '<b> We can </b> infer <b> that the </b> point almost certainly came <b> from the </b> left, generating <b> a </b> datapoint very <b> close to the X-axis. </b>',\n",
       " '<b> We can easily </b> discern <b> that PXY1 = 0.5. </b>',\n",
       " \"<b> On the </b> right, <b> you'd be </b> pretty sure <b> of the </b> class.\",\n",
       " \"<b> If we </b> repeat <b> the exercise, </b> sweeping <b> the </b> datapoints <b> across a </b> dense grid <b> and evaluating the </b> measure, <b> you'll </b> notice <b> that as we </b> approach <b> the </b> midpoint, <b> the value </b> increases <b> to </b> 0.5 <b> and </b> surpasses 0.5.\",\n",
       " 'Beyond <b> a </b> certain <b> point, plotting the connect-the-dots </b> turns exactly <b> into the </b> view <b> of the </b> quantity <b> Py1x, which is 0 or 1. </b>',\n",
       " '<b> We </b> find <b> that the form of Py1x is given by the expression 1 / (1 + exp(-Tx)), which is </b> appropriate <b> for 0 or 1. </b>',\n",
       " '<b> Py1x is 0 or 1. </b>',\n",
       " '<b> By convention, we redefine the </b> xis <b> to be on the right-hand side, </b> adding <b> a </b> coordinate <b> xi0 = 1. </b>',\n",
       " '<b> This is 1 times x. </b>',\n",
       " 'While <b> the </b> hypothesis <b> function is a </b> specific <b> choice, </b> choosing <b> it is </b> quite <b> leading. </b>',\n",
       " 'Would <b> you prefer one or the other? </b>',\n",
       " '<b> In </b> general, <b> the </b> boundaries <b> yielded by the </b> trained <b> models </b>',\n",
       " 'Which <b> one is </b> better?',\n",
       " '<b> We </b> discuss <b> which one is superior, and vice versa. </b>',\n",
       " '<b> It </b> assumes <b> that </b> xy0 <b> and </b> xy1 <b> are parameters of the logistic function, </b> governed <b> by Py1x = 1 / (1 + exp(-Tx)). </b>',\n",
       " '<b> The </b> element <b> x0 is </b> raised <b> to the power of the </b> plotting <b> point-by-point, </b> ultimately <b> yielding a curve that shares the necessary assumptions. </b>',\n",
       " '<b> The converse is also true, and the </b> implication <b> is that the Gaussian distribution </b>',\n",
       " '3 <b> implies 2, and this is a </b> stronger <b> result. We </b> prove 2, <b> and note that this is a </b> property <b> of </b> GDA.',\n",
       " '<b> This is </b> correct, <b> and </b> better <b> than </b> regression.',\n",
       " 'Specifically, <b> it is </b> indeed asymptotically efficient.',\n",
       " \"<b> This is a </b> strong <b> result, and you're </b> baking information <b> into the algorithm. </b> Informally, <b> in the </b> limit <b> of </b> large sets m, there <b> is </b> no strictly <b> better algorithm in </b> terms <b> of accuracy. </b>\"]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highlighted_sentences = [highlight_changes(pair['originalContent'], pair['correctedSentence']) for pair in data]\n",
    "highlighted_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(out_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>originalContent</th>\n",
       "      <th>correctedSentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Back to Top CS229 Gaussian Discriminant Analys...</td>\n",
       "      <td>Back to Top CS229 Gaussian Discriminant Analys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For instance, logistic regression modeled Pyx ...</td>\n",
       "      <td>For instance, logistic regression modelled Pyx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In this section, well talk about a different t...</td>\n",
       "      <td>In this section, we'll talk about a different ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lets use binary classification problem motivat...</td>\n",
       "      <td>Let's use binary classification problem motiva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Consider in which we want learn distinguish be...</td>\n",
       "      <td>Consider in which we want to learn to distingu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>3. converse, implication does Gaussian.</td>\n",
       "      <td>The converse is also true, and the implication...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>3 2. stronger prove 2, noted property GDA.</td>\n",
       "      <td>3 implies 2, and this is a stronger result. We...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>correct, better regression.</td>\n",
       "      <td>This is correct, and better than regression.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Specifically, indeed asymptotically efficient.</td>\n",
       "      <td>Specifically, it is indeed asymptotically effi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>strong youre baking information algorithm, Inf...</td>\n",
       "      <td>This is a strong result, and you're baking inf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       originalContent  \\\n",
       "0    Back to Top CS229 Gaussian Discriminant Analys...   \n",
       "1    For instance, logistic regression modeled Pyx ...   \n",
       "2    In this section, well talk about a different t...   \n",
       "3    Lets use binary classification problem motivat...   \n",
       "4    Consider in which we want learn distinguish be...   \n",
       "..                                                 ...   \n",
       "125            3. converse, implication does Gaussian.   \n",
       "126         3 2. stronger prove 2, noted property GDA.   \n",
       "127                        correct, better regression.   \n",
       "128     Specifically, indeed asymptotically efficient.   \n",
       "129  strong youre baking information algorithm, Inf...   \n",
       "\n",
       "                                     correctedSentence  \n",
       "0    Back to Top CS229 Gaussian Discriminant Analys...  \n",
       "1    For instance, logistic regression modelled Pyx...  \n",
       "2    In this section, we'll talk about a different ...  \n",
       "3    Let's use binary classification problem motiva...  \n",
       "4    Consider in which we want to learn to distingu...  \n",
       "..                                                 ...  \n",
       "125  The converse is also true, and the implication...  \n",
       "126  3 implies 2, and this is a stronger result. We...  \n",
       "127       This is correct, and better than regression.  \n",
       "128  Specifically, it is indeed asymptotically effi...  \n",
       "129  This is a strong result, and you're baking inf...  \n",
       "\n",
       "[130 rows x 2 columns]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['changes'] = highlighted_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['changes'].str.contains('<b>')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_filter(df, column_name):\n",
    "    # Create a temporary column in lowercase for normalization and matching\n",
    "    df['temp_lower'] = df[column_name].str.lower()\n",
    "    \n",
    "    # Pattern to match social media names, specific extensions, and keywords like 'cookies'\n",
    "    pattern = r'\\b(company|facebook|twitter|instagram|linkedin|youtube|\\.com|\\.ad|\\.gov|\\.uk|\\.php|\\.asp|cookies|cookies|statistical|tracking|ads|script|event|policy|Cloudflare|cookie|cfduid|llc|party)\\b'\n",
    "    \n",
    "    # Filter rows where the temporary lowercase column does not contain any of the specified patterns\n",
    "    filtered_df = df[~df['temp_lower'].str.contains(pattern, flags=re.IGNORECASE, regex=True)]\n",
    "    \n",
    "    # Drop the temporary column\n",
    "    filtered_df.drop('temp_lower', axis=1, inplace=True)\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gv/v0q4rb4n3p9_ydb839lgywph0000gq/T/ipykernel_8980/3883782903.py:9: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_df = df[~df['temp_lower'].str.contains(pattern, flags=re.IGNORECASE, regex=True)]\n"
     ]
    }
   ],
   "source": [
    "filtered_df = normalize_and_filter(df, 'changes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Back to Top CS229 Gaussian Discriminant Analysis Generative Learning Algorithms <b> We've </b> mainly looked at learning algorithms that model Pyx, the conditional distribution of y given x.\""
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.iloc[0]['changes']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Back to Top CS229 Gaussian Discriminant Analysis Generative Learning Algorithms Weve mainly looked at learning algorithms that model Pyx, the conditional distribution of y given x.'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.iloc[0]['originalContent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"originalContent\":{\"0\":\"Back to Top CS229 Gaussian Discriminant Analysis Generative Learning Algorithms Weve mainly looked at learning algorithms that model Pyx, the conditional distribution of y given x.\",\"1\":\"For instance, logistic regression modeled Pyx as hxgTx where g is sigmoid function.\",\"2\":\"In this section, well talk about a different type algorithm.\",\"3\":\"Lets use binary classification problem motivation behind our discussion.\",\"4\":\"Consider in which we want learn distinguish between malignant y1 and benign y0 tumors.\",\"5\":\"Given training set, an algorithm like , or initially starts with randomly initialized parameters.\",\"6\":\"Over course learning, performs gradient descent straight line hyperplane decision boundary evolves until you obtain separates positivenegative examples case, Then, classify new sample either benign, it checks on side falls in, makes its prediction accordingly.\",\"7\":\"Theres class arent trying maximize likelihood looking both classes searching for separation boundary.\",\"8\":\"Instead, these look one time.\",\"9\":\"First, tumors, can features what tumors like.\",\"10\":\"build separate Finally, tumor, match against tumor model, see whether looks more had seen set.\",\"11\":\"try directly such mapping from input space X labels are called discriminative algorithms.\",\"12\":\"instead Pxy Py.\",\"13\":\"These generative if indicates example 0 1, then Pxy0 models features, while Pxy1 features.\",\"14\":\"The also learns prior Py independent probability y.\",\"15\":\"To illustrate concept using practical when patient walks into hospital, before doctors even them, odds their versus referred prior.\",\"16\":\"Thus, builds each isolation.\",\"17\":\"At test time, evaluates models, identifies matches most closely returns prediction.\",\"18\":\"After modeling priors Pxy, Bayes rule derive posterior x PyxPxyPyPx\",\"19\":\"Here, denominator by PxPxy1Py1Pxy0Py0, function quantities\",\"20\":\"Note learned part process calculating order make prediction, dont actually need calculate value Px since constant, doesnt appear there.\",\"21\":\"When making predictions algorithms, thus ignore computing save computation.\",\"22\":\"However, end goal value, would compute be able normalize numerator.\",\"23\":\"PyxPxyPyPxPxyPy above equation represents underlying framework .\",\"24\":\"Key takeaways Discriminative i.e., output input.\",\"25\":\"other words, hx0,1 directly.\",\"26\":\"class, tumoridentification setting, may case them first discriminant analysis GDA, used continuousvalued say, classification.\",\"27\":\"assume distributed according multivariate normal distribution.\",\"28\":\"briefly properties distributions moving GDA itself.\",\"29\":\"Multivariate Distribution generalization 1dimensional random variable ndimensional simply, nrandom variable.\",\"30\":\"rather than univariate variable, seeks multiple variables.\",\"31\":\"Assume Gaussian, i.e, XRn, parameterized mean vector Rn covariance matrix Rnn, symmetric positive semidefinite.\",\"32\":\"Formally, written as, XN, density PDF PX,12n212exp12XT1X denotes determinant expected ExxPx,dx vectorvalued defined CovX EXET.\",\"33\":\"Covariance generalizes notion variance realvalued setting.\",\"34\":\"EEET.\",\"35\":\"Since CovX.\",\"36\":\"explore some visualize Recall familiar\",\"37\":\"Similarly, multivariable represented same bellshaped curve two parameters control PDF, but ndimensions.\",\"38\":\"e.g., 2dimensional over 2 variables, 2dimensional, size 22.\",\"39\":\"below figure shows zero 21 zerovector I 22 identity matrix.\",\"40\":\"A standard 0.6I.\",\"41\":\"essentially taken multiplied number has shrunk variance, reduced variability 2I.\",\"42\":\"From images, becomes larger, widershorter, smaller, compressedtaller.\",\"43\":\"This because always integrates area under so scales spread vs. height figures show Gaussians corresponding matrices figures, increase offdiagonal entries compressed towards 45 x1x2.\",\"44\":\"Geometrically speaking, implies variables positively correlated.\",\"45\":\"We clearly contours three densities 3D bumps saw above, should perfectly round circles, aspect ratio image probably little bit fatter places being\",\"46\":\"Heres another set along decreasing elements matrix, now again, opposite direction, 135 line.\",\"47\":\"Again, geometrically, endow negative correlation.\",\"48\":\"vary parameters, tend form ellipses.\",\"49\":\"By varying shift center around.\",\"50\":\"Another values I, visualization PDFs follows carry out eigenvectors points principal axes ellipse contours.\",\"51\":\"takeaway As density, change spreadheight respectively.\",\"52\":\"Model have consider task discussion, expressed equations Pxy1, Gaussians.\",\"53\":\"On hand, Bernoulli takes 0,1.\",\"54\":\"by, xy0N0,xy1N1,yBernoulli 0, classes, vectors 1.\",\"55\":\"Put were assuming representing means.\",\"56\":\"You not commonly seen.\",\"57\":\"More section below.\",\"58\":\"fit your data, will define data.\",\"59\":\"R, 0Rn, 1Rn, Rnn.\",\"60\":\"Writing distributions, Pyy11yPxy012n212exp12x0T1x0Pxy112n212exp12x1T1x1\",\"61\":\"exponential notation similar earlier Plugging Pxy0,Pxy1,Py0 Py1 formula easily ascertain particular example.\",\"62\":\"Suppose xi,yimi1.\",\"63\":\"aforementioned going joint L, L,0,1,mi1Pxi,yi,0,1,mi1Pxiyi0,1,Pyi\",\"64\":\"big difference cost functions compared choose ,0,1,, Px,y,0,1,.\",\"65\":\"linear regression, generalized Pyx.\",\"66\":\"loglikelihood data simply log L.\",\"67\":\",0,1,logmi1Pxi,yi,0,1,logmi1Pxiyi0,1,Pyi\",\"68\":\"respect take derivative equal solve expression yields maximum estimate be, mi1yim\",\"69\":\"An intuitive explanation around maximizes follows.\",\"70\":\"example, chance next office denoted bias coin toss fraction heads Likewise, just label y1.\",\"71\":\"way write indicator notation, mi11yi1m 1 argument true, otherwise 0.\",\"72\":\"true statement false equivalent ifstatement programming context.\",\"73\":\"0,1 is, 0mi11yi0ximi11yi01mi11yi1\",\"74\":\"intuition expression, think all say dataset.\",\"75\":\"reasonable examples, 0s average writing intuition.\",\"76\":\"numerator sum feature sums entire summing i1m, uses instances yi0 times xi.\",\"77\":\"effect term whereas effectively zeroing term.\",\"78\":\"up samples yi0.\",\"79\":\"count represent Because every yi0, get extra sum, ends total examples.\",\"80\":\"made 1mmi1xiyixiT fits means classes.\",\"81\":\"Making Predictions Using minz min z minimum any possible z.\",\"82\":\"argminz argmin refers plugged expression.\",\"83\":\"minzz52 attained z520 argminzz52 led z52, z5.\",\"84\":\"maxz argmaxz operators exist they deal leads having lets how go\",\"85\":\"So do benign?\",\"86\":\"boils down predicting likely x, formally argmaxyPyx\",\"87\":\"Looking deeper y0,1 rule, argmaxyPyxargmaxyPxyPyPx\",\"88\":\"reasons highlighted Visualization dataset compareandcontrast operate\",\"89\":\"Pictorially, GDAs operation Shown been shape orientation, share point, determine put together, imply blue Py1x0.5.\",\"90\":\"boundary, predict outcome, side, y0.\",\"91\":\"Points upper right closer classifying lower left classified\",\"92\":\"Logistic Regression x1 x2 start whose randomly.\",\"93\":\"Typically, initialize purpose visualization, starting off shown\",\"94\":\"Running single iteration PYX, repositions 20 iterations, converges illustrates green superimposed blue.\",\"95\":\"arrive slightly boundaries.\",\"96\":\"Why Two Separate Means, Single Matrix?\",\"97\":\"If lot problems.\",\"98\":\"Choosing reasonable, work fine.\",\"99\":\"catch, though, youll roughly double isnt anymore.\",\"100\":\"Discussion Comparing interesting relationship.\",\"101\":\"youve ,0,1,.\",\"102\":\"fixed Py1x,0,1,.\",\"103\":\"theorem, Pxy1,0,1,Pxy1,0,1,Py1Px,0,1,\",\"104\":\"where, Pxy1,0,1, measure evaluating once determined optimum probability, plot Py1x so, simple few\",\"105\":\"Now, Mappingprojecting Xaxis, bump PXY0 PXY1.\",\"106\":\"Were Also, note split 5050 across PY1 0.5, known half\",\"107\":\"Next, PY1X X.\",\"108\":\"unlabeled far Xaxis.\",\"109\":\"infer point almost certainly came left, generating Xaxis datapoint very small.\",\"110\":\"pick Its easy discern PXY10.5.\",\"111\":\"right, youd pretty sure class.\",\"112\":\"repeat exercise sweeping datapoints dense grid evaluate measure, Youll notice coming approach midpoint, increases 0.5 surpasses 0.5.\",\"113\":\"Beyond certain tends Plotting connect dots, turns exactly view quantity Py1x,0,1,\",\"114\":\"find form, Py1x,,0,111expTx appropriate ,,0,1.\",\"115\":\"Py1x,,0,1.\",\"116\":\"convention redefining xis righthandside n1 dimensional adding coordinate xi01.\",\"117\":\"1x.\",\"118\":\"While hypothesis function, hood specific choice choosing quite leading\",\"119\":\"Would Prefer One Another?\",\"120\":\"general, yield boundaries trained\",\"121\":\"Which better?\",\"122\":\"discuss superior viceversa.\",\"123\":\"assumes that, xy0 xy1 parameter logistic, governed Py1x11expTx\",\"124\":\"element x0, raised plotting pointbypoint ultimately yielded curve, shared necessarily assumptions\",\"125\":\"3. converse, implication does Gaussian.\",\"126\":\"3 2. stronger prove 2, noted property GDA.\",\"127\":\"correct, better regression.\",\"128\":\"Specifically, indeed asymptotically efficient.\",\"129\":\"strong youre baking information algorithm, Informally, limit large sets m, there no strictly terms of, accurately\"},\"correctedSentence\":{\"0\":\"Back to Top CS229 Gaussian Discriminant Analysis Generative Learning Algorithms We\\'ve mainly looked at learning algorithms that model Pyx, the conditional distribution of y given x.\",\"1\":\"For instance, logistic regression modelled Pyx as hxgTx where g is sigmoid function.\",\"2\":\"In this section, we\\'ll talk about a different type of algorithm.\",\"3\":\"Let\\'s use binary classification problem motivation behind our discussion.\",\"4\":\"Consider in which we want to learn to distinguish between malignant y1 and benign y0 tumours.\",\"5\":\"Given a training set, an algorithm like , or initially starts with randomly initialised parameters.\",\"6\":\"Over the course of learning, it performs gradient descent on a straight line hyperplane decision boundary, which evolves until it separates positive and negative examples. Then, it classifies a new sample as either benign or malignant, depending on which side it falls in, and makes its prediction accordingly.\",\"7\":\"There\\'s a class that aren\\'t trying to maximise likelihood, looking at both classes and searching for a separation boundary.\",\"8\":\"Instead, these look at one time.\",\"9\":\"First, tumours can have features that are like other tumours.\",\"10\":\"Build separate models for each tumour type. Finally, match the tumour against the tumour model, and see whether it looks more like the tumours in the training set.\",\"11\":\"Try to directly map from the input space X to the labels, which are called discriminative algorithms.\",\"12\":\"Instead, Pxy = Py.\",\"13\":\"These generative models indicate that, for example, Pxy0 models the features of class 0, while Pxy1 models the features of class 1.\",\"14\":\"The model also learns the prior probability Py, which is independent of y.\",\"15\":\"To illustrate this concept, consider a practical example: when a patient walks into a hospital, before the doctors even see them, the odds of their having a particular disease versus not having it are referred to as the prior probability.\",\"16\":\"Thus, the model builds each in isolation.\",\"17\":\"At test time, the model evaluates the models, identifies which one matches most closely, and returns the prediction.\",\"18\":\"After modelling the priors Pxy, we can derive the posterior probability x using Bayes\\' rule: Pyx = PxyPyPx\",\"19\":\"Here, the denominator is given by Px = Pxy1Py1 + Pxy0Py0, which is a function of the quantities\",\"20\":\"Note that the learned part of the process is calculating the order to make a prediction, but we don\\'t actually need to calculate the value of Px since it is a constant and doesn\\'t appear in the final prediction.\",\"21\":\"When making predictions, algorithms thus ignore computing Px to save computation.\",\"22\":\"However, the end goal is to compute the value of Px, which would allow us to normalise the numerator.\",\"23\":\"The equation Pyx = PxyPyPx represents the underlying framework.\",\"24\":\"Key takeaways: discriminative models, i.e., output is a function of input.\",\"25\":\"In other words, hx is 0 or 1 directly.\",\"26\":\"In the case of tumour identification, we may use discriminant analysis (GDA) for continuous-valued classification.\",\"27\":\"We assume that the data is distributed according to a multivariate normal distribution.\",\"28\":\"We will briefly discuss the properties of the distributions and then move on to GDA itself.\",\"29\":\"A multivariate distribution is a generalisation of a 1-dimensional random variable to an n-dimensional random variable.\",\"30\":\"Rather than a univariate variable, it seeks multiple variables.\",\"31\":\"We assume that the data is Gaussian, i.e., X ~ Rn, parameterised by a mean vector in Rn and a covariance matrix in Rnn, which is symmetric and positive semidefinite.\",\"32\":\"Formally, this can be written as X ~ N(\\\\u03bc, \\\\u03a3), where the density PDF is given by PX(x) = (1\\\\/\\\\u221a(2\\\\u03c0)^n) * exp(-1\\\\/2 * (x-\\\\u03bc)^T * \\\\u03a3^(-1) * (x-\\\\u03bc)), where \\\\u03bc is the mean vector, \\\\u03a3 is the covariance matrix, and E[x] is the expected value of x.\",\"33\":\"Covariance generalises the notion of variance in the real-valued setting.\",\"34\":\"E[E^T] = E[E]E^T.\",\"35\":\"Since Cov(X) = E[(X-E[X])(X-E[X])^T],\",\"36\":\"Let\\'s explore some visualisations. Recall the familiar\",\"37\":\"Similarly, a multivariate distribution can be represented by the same bell-shaped curve, but with two parameters controlling the PDF in n dimensions.\",\"38\":\"For example, a 2-dimensional distribution over 2 variables has a size of 2x2.\",\"39\":\"The figure below shows a zero-mean vector, a 2x1 zero vector, and a 2x2 identity matrix.\",\"40\":\"A standard deviation of 0.6I.\",\"41\":\"Essentially, the number has been multiplied by 0.6, which has shrunk the variance and reduced the variability. 2I.\",\"42\":\"From the images, we can see that as the covariance matrix becomes larger, the distribution becomes wider and shorter, and as it becomes smaller, the distribution becomes compressed and taller.\",\"43\":\"This is because the area under the curve always integrates to 1, so as the spread increases, the height decreases, and vice versa. The figures show Gaussians corresponding to the matrices, and as the off-diagonal entries increase, the distribution becomes compressed towards the 45\\\\u00b0 line between x1 and x2.\",\"44\":\"Geometrically speaking, this implies that the variables are positively correlated.\",\"45\":\"We can clearly see the contours of the three densities, which are 3D bumps. The aspect ratio of the image is probably a little bit fatter in some places, but the contours should be perfectly round circles.\",\"46\":\"Here\\'s another set of images, this time with decreasing elements of the matrix, and again, in the opposite direction, along the 135\\\\u00b0 line.\",\"47\":\"Again, geometrically, this endows the variables with a negative correlation.\",\"48\":\"As we vary the parameters, the distribution tends to form ellipses.\",\"49\":\"By varying the shift of the center around.\",\"50\":\"Another way to visualise the PDFs is to carry out the eigenvectors and points of the principal axes of the ellipse contours.\",\"51\":\"The takeaway is that as the density changes, the spread and height change respectively.\",\"52\":\"The model we have been considering for this task is expressed in terms of equations for Pxy1, which are Gaussians.\",\"53\":\"On the other hand, the Bernoulli distribution takes values 0 and 1.\",\"54\":\"By the way, xy0 ~ N0, xy1 ~ N1, and y ~ Bernoulli(0, 1), where the classes are represented by vectors of length 1.\",\"55\":\"Put simply, we were assuming that the means are represented by\",\"56\":\"You may not have commonly seen this.\",\"57\":\"More on this in the section below.\",\"58\":\"To fit your data, we will define the data.\",\"59\":\"R, 0 in Rn, 1 in Rn, and Rnn.\",\"60\":\"Writing the distributions, Py(y=1) = Pxy0 ~ N(0, 1), Py(y=1) = Pxy1 ~ N(1, 1)\",\"61\":\"Using exponential notation similar to earlier, plugging in Pxy0, Pxy1, Py0, and Py1 into the formula, we can easily ascertain the particular example.\",\"62\":\"Suppose we have xi and yi, for i = 1 to m.\",\"63\":\"The aforementioned joint likelihood L is given by L = \\\\u220f[P(xi, yi) for i = 1 to m] = \\\\u220f[P(xi|yi)P(yi) for i = 1 to m]\",\"64\":\"There is a big difference between the cost functions. We choose to maximise the likelihood of the data, which is given by L = \\\\u220f[P(xi, yi) for i = 1 to m].\",\"65\":\"In linear regression, we generalise Pyx.\",\"66\":\"The log-likelihood of the data is simply log L.\",\"67\":\"The log-likelihood is given by log L = \\\\u2211[log P(xi, yi) for i = 1 to m] = \\\\u2211[log P(xi|yi)P(yi) for i = 1 to m]\",\"68\":\"Taking the derivative with respect to the parameters and setting it equal to zero, we can solve for the expression that yields the maximum likelihood estimate, which is given by \\\\u03bc = (1\\\\/m) * \\\\u2211[yi * xi for i = 1 to m]\",\"69\":\"An intuitive explanation of how this maximises the likelihood follows.\",\"70\":\"For example, the chance of the next office being occupied is denoted by the bias of a coin toss, which is the fraction of heads. Likewise, we just label y as 1.\",\"71\":\"We can write this in indicator notation as 1(yi = 1) = 1 if the argument is true, and 0 otherwise.\",\"72\":\"This is equivalent to a true statement being false in an if-statement in a programming context.\",\"73\":\"0 or 1 is given by 0 if yi = 0, and 1 if yi = 1.\",\"74\":\"The intuition behind this expression is to think about all the data points in the dataset.\",\"75\":\"A reasonable example is to take the average of the 0s and 1s in the dataset.\",\"76\":\"The numerator is the sum of the feature sums over the entire dataset, summing over all instances where yi = 0, and using the instances where yi = 1 times xi.\",\"77\":\"This has the effect of zeroing out the term.\",\"78\":\"This is because we are summing over all samples where yi = 0.\",\"79\":\"This is because every time yi = 0, we get an extra sum, which ends up being the total number of examples.\",\"80\":\"This is made up of 1\\\\/m times the sum of xi times yi, which fits the means of the classes.\",\"81\":\"Making predictions using the minimum of z, where z is any possible value.\",\"82\":\"The argmin of z refers to the value of z that minimises the expression.\",\"83\":\"The minimum of z is attained when z = 5, and the argmin of z is 5.\",\"84\":\"The max of z and the argmax of z are operators that exist, and they deal with the maximum value of z. Let\\'s see how to go about finding the maximum value of z.\",\"85\":\"So, do we predict benign?\",\"86\":\"This boils down to predicting the most likely value of x, which is given by the argmax of Pyx.\",\"87\":\"Looking deeper, we can see that the rule for predicting y is given by the argmax of Pyx, which is equal to the argmax of PxyPyPx.\",\"88\":\"The reasons for this are highlighted in the visualisation of the dataset, which shows how the model operates.\",\"89\":\"Pictorially, the operation of GDA is shown to be a shape and orientation that share a point, which determines the decision boundary. This implies that the blue line is the decision boundary, and Py1x = 0.5.\",\"90\":\"The boundary is used to predict the outcome, and the side of the boundary determines the value of y.\",\"91\":\"Points in the upper right are closer to being classified as y = 1, while points in the lower left are classified as y = 0.\",\"92\":\"Logistic regression starts with x1 and x2, whose values are randomly initialised.\",\"93\":\"Typically, we initialise the values for the purpose of visualisation, starting off with the values shown.\",\"94\":\"Running a single iteration of the algorithm, we reposition the decision boundary, and after 20 iterations, the algorithm converges, illustrating the green line superimposed on the blue line.\",\"95\":\"We arrive at the decision boundary, which is slightly different from the original boundary.\",\"96\":\"Why do we have two separate means and a single matrix?\",\"97\":\"If we have a lot of problems.\",\"98\":\"Choosing a reasonable value will work fine.\",\"99\":\"The catch, though, is that you\\'ll roughly double the number of parameters, and it isn\\'t anymore.\",\"100\":\"The discussion compares the interesting relationship between\",\"101\":\"You\\'ve seen the relationship between 0 and 1.\",\"102\":\"The fixed value of Py1x is 0 or 1.\",\"103\":\"By the theorem, Pxy1 = Pxy1,0,1,Py1Px,0,1,\",\"104\":\"where Pxy1 is the measure of evaluating the optimum probability, and we plot Py1x, which is a simple function of a few variables.\",\"105\":\"Now, we map the X-axis to the bump of PXY0 and PXY1.\",\"106\":\"We also note that the split is 50-50 across Py1 = 0.5, which is known as the half-way point.\",\"107\":\"Next, we have Py1x = X.\",\"108\":\"The unlabeled points are far away from the X-axis.\",\"109\":\"We can infer that the point almost certainly came from the left, generating a datapoint very close to the X-axis.\",\"110\":\"We can easily discern that PXY1 = 0.5.\",\"111\":\"On the right, you\\'d be pretty sure of the class.\",\"112\":\"If we repeat the exercise, sweeping the datapoints across a dense grid and evaluating the measure, you\\'ll notice that as we approach the midpoint, the value increases to 0.5 and surpasses 0.5.\",\"113\":\"Beyond a certain point, plotting the connect-the-dots turns exactly into the view of the quantity Py1x, which is 0 or 1.\",\"114\":\"We find that the form of Py1x is given by the expression 1 \\\\/ (1 + exp(-Tx)), which is appropriate for 0 or 1.\",\"115\":\"Py1x is 0 or 1.\",\"116\":\"By convention, we redefine the xis to be on the right-hand side, adding a coordinate xi0 = 1.\",\"117\":\"This is 1 times x.\",\"118\":\"While the hypothesis function is a specific choice, choosing it is quite leading.\",\"119\":\"Would you prefer one or the other?\",\"120\":\"In general, the boundaries yielded by the trained models\",\"121\":\"Which one is better?\",\"122\":\"We discuss which one is superior, and vice versa.\",\"123\":\"It assumes that xy0 and xy1 are parameters of the logistic function, governed by Py1x = 1 \\\\/ (1 + exp(-Tx)).\",\"124\":\"The element x0 is raised to the power of the plotting point-by-point, ultimately yielding a curve that shares the necessary assumptions.\",\"125\":\"The converse is also true, and the implication is that the Gaussian distribution\",\"126\":\"3 implies 2, and this is a stronger result. We prove 2, and note that this is a property of GDA.\",\"127\":\"This is correct, and better than regression.\",\"128\":\"Specifically, it is indeed asymptotically efficient.\",\"129\":\"This is a strong result, and you\\'re baking information into the algorithm. Informally, in the limit of large sets m, there is no strictly better algorithm in terms of accuracy.\"},\"changes\":{\"0\":\"Back to Top CS229 Gaussian Discriminant Analysis Generative Learning Algorithms <b> We\\'ve <\\\\/b> mainly looked at learning algorithms that model Pyx, the conditional distribution of y given x.\",\"1\":\"For instance, logistic regression <b> modelled <\\\\/b> Pyx as hxgTx where g is sigmoid function.\",\"2\":\"In this section, <b> we\\'ll <\\\\/b> talk about a different type <b> of <\\\\/b> algorithm.\",\"3\":\"<b> Let\\'s <\\\\/b> use binary classification problem motivation behind our discussion.\",\"4\":\"Consider in which we want <b> to <\\\\/b> learn <b> to <\\\\/b> distinguish between malignant y1 and benign y0 <b> tumours. <\\\\/b>\",\"5\":\"Given <b> a <\\\\/b> training set, an algorithm like , or initially starts with randomly <b> initialised <\\\\/b> parameters.\",\"6\":\"Over <b> the <\\\\/b> course <b> of <\\\\/b> learning, <b> it <\\\\/b> performs gradient descent <b> on a <\\\\/b> straight line hyperplane decision <b> boundary, which <\\\\/b> evolves until <b> it <\\\\/b> separates <b> positive and negative examples. <\\\\/b> Then, <b> it classifies a <\\\\/b> new sample <b> as <\\\\/b> either <b> benign or malignant, depending on which side <\\\\/b> it falls in, <b> and <\\\\/b> makes its prediction accordingly.\",\"7\":\"<b> There\\'s a <\\\\/b> class <b> that aren\\'t <\\\\/b> trying <b> to maximise likelihood, <\\\\/b> looking <b> at <\\\\/b> both classes <b> and <\\\\/b> searching for <b> a <\\\\/b> separation boundary.\",\"8\":\"Instead, these look <b> at <\\\\/b> one time.\",\"9\":\"First, <b> tumours <\\\\/b> can <b> have <\\\\/b> features <b> that are like other tumours. <\\\\/b>\",\"10\":\"<b> Build <\\\\/b> separate <b> models for each tumour type. <\\\\/b> Finally, match <b> the tumour <\\\\/b> against <b> the tumour <\\\\/b> model, <b> and <\\\\/b> see whether <b> it <\\\\/b> looks more <b> like the tumours in the training <\\\\/b> set.\",\"11\":\"<b> Try to <\\\\/b> directly <b> map <\\\\/b> from <b> the <\\\\/b> input space X <b> to the labels, which <\\\\/b> are called discriminative algorithms.\",\"12\":\"<b> Instead, <\\\\/b> Pxy <b> = <\\\\/b> Py.\",\"13\":\"These generative <b> models indicate that, for example, <\\\\/b> Pxy0 models <b> the features of class 0, <\\\\/b> while Pxy1 <b> models the features of class 1. <\\\\/b>\",\"14\":\"The <b> model <\\\\/b> also learns <b> the <\\\\/b> prior <b> probability Py, which is <\\\\/b> independent <b> of <\\\\/b> y.\",\"15\":\"To illustrate <b> this concept, consider a <\\\\/b> practical <b> example: <\\\\/b> when <b> a <\\\\/b> patient walks into <b> a <\\\\/b> hospital, before <b> the <\\\\/b> doctors even <b> see <\\\\/b> them, <b> the <\\\\/b> odds <b> of <\\\\/b> their <b> having a particular disease <\\\\/b> versus <b> not having it are <\\\\/b> referred <b> to as the prior probability. <\\\\/b>\",\"16\":\"Thus, <b> the model <\\\\/b> builds each <b> in <\\\\/b> isolation.\",\"17\":\"At test time, <b> the model <\\\\/b> evaluates <b> the <\\\\/b> models, identifies <b> which one <\\\\/b> matches most <b> closely, and <\\\\/b> returns <b> the <\\\\/b> prediction.\",\"18\":\"After <b> modelling the <\\\\/b> priors Pxy, <b> we can <\\\\/b> derive <b> the <\\\\/b> posterior <b> probability <\\\\/b> x <b> using Bayes\\' rule: Pyx = PxyPyPx <\\\\/b>\",\"19\":\"Here, <b> the <\\\\/b> denominator <b> is given <\\\\/b> by <b> Px = Pxy1Py1 + Pxy0Py0, which is a <\\\\/b> function <b> of the <\\\\/b> quantities\",\"20\":\"Note <b> that the <\\\\/b> learned part <b> of the <\\\\/b> process <b> is <\\\\/b> calculating <b> the <\\\\/b> order <b> to <\\\\/b> make <b> a <\\\\/b> prediction, <b> but we don\\'t <\\\\/b> actually need <b> to <\\\\/b> calculate <b> the <\\\\/b> value <b> of <\\\\/b> Px since <b> it is a constant and doesn\\'t <\\\\/b> appear <b> in the final prediction. <\\\\/b>\",\"21\":\"When making <b> predictions, algorithms <\\\\/b> thus ignore computing <b> Px to <\\\\/b> save computation.\",\"22\":\"However, <b> the <\\\\/b> end goal <b> is to compute the value of Px, which <\\\\/b> would <b> allow us to normalise the <\\\\/b> numerator.\",\"23\":\"<b> The <\\\\/b> equation <b> Pyx = PxyPyPx <\\\\/b> represents <b> the <\\\\/b> underlying <b> framework. <\\\\/b>\",\"24\":\"Key <b> takeaways: discriminative models, <\\\\/b> i.e., output <b> is a function of <\\\\/b> input.\",\"25\":\"<b> In <\\\\/b> other words, <b> hx is 0 or 1 <\\\\/b> directly.\",\"26\":\"<b> In the case of tumour identification, we <\\\\/b> may <b> use <\\\\/b> discriminant analysis <b> (GDA) for continuous-valued <\\\\/b> classification.\",\"27\":\"<b> We <\\\\/b> assume <b> that the data is <\\\\/b> distributed according <b> to a <\\\\/b> multivariate normal distribution.\",\"28\":\"<b> We will <\\\\/b> briefly <b> discuss the <\\\\/b> properties <b> of the <\\\\/b> distributions <b> and then move on to <\\\\/b> GDA itself.\",\"29\":\"<b> A multivariate distribution is a generalisation of a 1-dimensional <\\\\/b> random variable <b> to an n-dimensional random <\\\\/b> variable.\",\"30\":\"<b> Rather <\\\\/b> than <b> a <\\\\/b> univariate variable, <b> it <\\\\/b> seeks multiple variables.\",\"31\":\"<b> We assume that the data is <\\\\/b> Gaussian, <b> i.e., X ~ Rn, parameterised by a <\\\\/b> mean vector <b> in <\\\\/b> Rn <b> and a <\\\\/b> covariance matrix <b> in <\\\\/b> Rnn, <b> which is <\\\\/b> symmetric <b> and <\\\\/b> positive semidefinite.\",\"32\":\"Formally, <b> this can be <\\\\/b> written <b> as X ~ N(\\\\u03bc, \\\\u03a3), where the <\\\\/b> density PDF <b> is given by PX(x) = (1\\\\/\\\\u221a(2\\\\u03c0)^n) * exp(-1\\\\/2 * (x-\\\\u03bc)^T * \\\\u03a3^(-1) * (x-\\\\u03bc)), where \\\\u03bc is the mean vector, \\\\u03a3 is the covariance matrix, and E[x] is the <\\\\/b> expected <b> value of x. <\\\\/b>\",\"33\":\"Covariance <b> generalises the <\\\\/b> notion <b> of <\\\\/b> variance <b> in the real-valued <\\\\/b> setting.\",\"34\":\"<b> E[E^T] = E[E]E^T. <\\\\/b>\",\"35\":\"Since <b> Cov(X) = E[(X-E[X])(X-E[X])^T], <\\\\/b>\",\"36\":\"<b> Let\\'s <\\\\/b> explore some <b> visualisations. <\\\\/b> Recall <b> the <\\\\/b> familiar\",\"37\":\"Similarly, <b> a multivariate distribution can be <\\\\/b> represented <b> by the <\\\\/b> same <b> bell-shaped curve, but with <\\\\/b> two parameters <b> controlling the PDF in n dimensions. <\\\\/b>\",\"38\":\"<b> For example, a 2-dimensional distribution <\\\\/b> over 2 <b> variables has a <\\\\/b> size <b> of 2x2. <\\\\/b>\",\"39\":\"<b> The figure <\\\\/b> below shows <b> a zero-mean vector, a 2x1 <\\\\/b> zero <b> vector, and a 2x2 <\\\\/b> identity matrix.\",\"40\":\"A standard <b> deviation of <\\\\/b> 0.6I.\",\"41\":\"<b> Essentially, the <\\\\/b> number has <b> been multiplied by 0.6, which has <\\\\/b> shrunk <b> the variance and <\\\\/b> reduced <b> the variability. <\\\\/b> 2I.\",\"42\":\"From <b> the <\\\\/b> images, <b> we can see that as the covariance matrix <\\\\/b> becomes larger, <b> the distribution becomes wider and shorter, and as it becomes <\\\\/b> smaller, <b> the distribution becomes compressed and taller. <\\\\/b>\",\"43\":\"This <b> is <\\\\/b> because <b> the area under the curve <\\\\/b> always integrates <b> to 1, <\\\\/b> so <b> as the <\\\\/b> spread <b> increases, the <\\\\/b> height <b> decreases, and vice versa. The <\\\\/b> figures show Gaussians corresponding <b> to the matrices, and as the off-diagonal <\\\\/b> entries <b> increase, the distribution becomes <\\\\/b> compressed towards <b> the 45\\\\u00b0 line between x1 and x2. <\\\\/b>\",\"44\":\"Geometrically speaking, <b> this <\\\\/b> implies <b> that the <\\\\/b> variables <b> are <\\\\/b> positively correlated.\",\"45\":\"We <b> can <\\\\/b> clearly <b> see the <\\\\/b> contours <b> of the <\\\\/b> three <b> densities, which are <\\\\/b> 3D <b> bumps. The <\\\\/b> aspect ratio <b> of the <\\\\/b> image <b> is <\\\\/b> probably <b> a <\\\\/b> little bit fatter <b> in some places, but the contours should be perfectly round circles. <\\\\/b>\",\"46\":\"<b> Here\\'s <\\\\/b> another set <b> of images, this time with <\\\\/b> decreasing elements <b> of the <\\\\/b> matrix, <b> and <\\\\/b> again, <b> in the <\\\\/b> opposite direction, <b> along the 135\\\\u00b0 <\\\\/b> line.\",\"47\":\"Again, geometrically, <b> this endows the variables with a <\\\\/b> negative correlation.\",\"48\":\"<b> As we <\\\\/b> vary <b> the <\\\\/b> parameters, <b> the distribution tends to <\\\\/b> form ellipses.\",\"49\":\"By varying <b> the <\\\\/b> shift <b> of the <\\\\/b> center around.\",\"50\":\"Another <b> way to visualise the <\\\\/b> PDFs <b> is to <\\\\/b> carry out <b> the <\\\\/b> eigenvectors <b> and <\\\\/b> points <b> of the <\\\\/b> principal axes <b> of the <\\\\/b> ellipse contours.\",\"51\":\"<b> The <\\\\/b> takeaway <b> is that as the density changes, the spread and height <\\\\/b> change respectively.\",\"52\":\"<b> The model we <\\\\/b> have <b> been considering for this <\\\\/b> task <b> is <\\\\/b> expressed <b> in terms of <\\\\/b> equations <b> for <\\\\/b> Pxy1, <b> which are <\\\\/b> Gaussians.\",\"53\":\"On <b> the other <\\\\/b> hand, <b> the <\\\\/b> Bernoulli <b> distribution <\\\\/b> takes <b> values 0 and 1. <\\\\/b>\",\"54\":\"<b> By the way, xy0 ~ N0, xy1 ~ N1, and y ~ Bernoulli(0, 1), where the classes are represented by <\\\\/b> vectors <b> of length <\\\\/b> 1.\",\"55\":\"Put <b> simply, we <\\\\/b> were assuming <b> that the means are represented by <\\\\/b>\",\"56\":\"You <b> may <\\\\/b> not <b> have <\\\\/b> commonly <b> seen this. <\\\\/b>\",\"57\":\"More <b> on this in the <\\\\/b> section below.\",\"58\":\"<b> To <\\\\/b> fit your data, <b> we <\\\\/b> will define <b> the <\\\\/b> data.\",\"59\":\"R, <b> 0 in Rn, 1 in Rn, and <\\\\/b> Rnn.\",\"60\":\"Writing <b> the <\\\\/b> distributions, <b> Py(y=1) = Pxy0 ~ N(0, 1), Py(y=1) = Pxy1 ~ N(1, 1) <\\\\/b>\",\"61\":\"<b> Using <\\\\/b> exponential notation similar <b> to earlier, plugging in Pxy0, Pxy1, Py0, and <\\\\/b> Py1 <b> into the formula, we can <\\\\/b> easily ascertain <b> the <\\\\/b> particular example.\",\"62\":\"Suppose <b> we have xi and yi, for i = 1 to m. <\\\\/b>\",\"63\":\"<b> The <\\\\/b> aforementioned joint <b> likelihood L is given by L = \\\\u220f[P(xi, yi) for i = 1 to m] = \\\\u220f[P(xi|yi)P(yi) for i = 1 to m] <\\\\/b>\",\"64\":\"<b> There is a <\\\\/b> big difference <b> between the <\\\\/b> cost <b> functions. We <\\\\/b> choose <b> to maximise the likelihood of the data, which is given by L = \\\\u220f[P(xi, yi) for i = 1 to m]. <\\\\/b>\",\"65\":\"<b> In <\\\\/b> linear regression, <b> we generalise <\\\\/b> Pyx.\",\"66\":\"<b> The log-likelihood of the <\\\\/b> data <b> is <\\\\/b> simply log L.\",\"67\":\"<b> The log-likelihood is given by log L = \\\\u2211[log P(xi, yi) for i = 1 to m] = \\\\u2211[log P(xi|yi)P(yi) for i = 1 to m] <\\\\/b>\",\"68\":\"<b> Taking the derivative with <\\\\/b> respect <b> to the parameters and setting it <\\\\/b> equal <b> to zero, we can <\\\\/b> solve <b> for the <\\\\/b> expression <b> that <\\\\/b> yields <b> the <\\\\/b> maximum <b> likelihood estimate, which is given by \\\\u03bc = (1\\\\/m) * \\\\u2211[yi * xi for i = 1 to m] <\\\\/b>\",\"69\":\"An intuitive explanation <b> of how this maximises the likelihood <\\\\/b> follows.\",\"70\":\"<b> For <\\\\/b> example, <b> the <\\\\/b> chance <b> of the <\\\\/b> next office <b> being occupied is <\\\\/b> denoted <b> by the <\\\\/b> bias <b> of a <\\\\/b> coin <b> toss, which is the <\\\\/b> fraction <b> of heads. <\\\\/b> Likewise, <b> we <\\\\/b> just label <b> y as 1. <\\\\/b>\",\"71\":\"<b> We can <\\\\/b> write <b> this in <\\\\/b> indicator <b> notation as 1(yi = 1) = <\\\\/b> 1 <b> if the <\\\\/b> argument <b> is <\\\\/b> true, <b> and 0 otherwise. <\\\\/b>\",\"72\":\"<b> This is equivalent to a <\\\\/b> true statement <b> being <\\\\/b> false <b> in an if-statement in a <\\\\/b> programming context.\",\"73\":\"<b> 0 or 1 is given by 0 if yi = 0, and 1 if yi = 1. <\\\\/b>\",\"74\":\"<b> The <\\\\/b> intuition <b> behind this expression is to <\\\\/b> think <b> about <\\\\/b> all <b> the data points in the <\\\\/b> dataset.\",\"75\":\"<b> A <\\\\/b> reasonable <b> example is to take the average of the <\\\\/b> 0s <b> and 1s in the dataset. <\\\\/b>\",\"76\":\"<b> The <\\\\/b> numerator <b> is the <\\\\/b> sum <b> of the <\\\\/b> feature sums <b> over the <\\\\/b> entire <b> dataset, <\\\\/b> summing <b> over all <\\\\/b> instances <b> where yi = 0, and using the instances where yi = 1 <\\\\/b> times xi.\",\"77\":\"<b> This has the <\\\\/b> effect <b> of <\\\\/b> zeroing <b> out the <\\\\/b> term.\",\"78\":\"<b> This is because we are summing over all <\\\\/b> samples <b> where yi = 0. <\\\\/b>\",\"79\":\"<b> This is because <\\\\/b> every <b> time yi = 0, we <\\\\/b> get <b> an <\\\\/b> extra sum, <b> which <\\\\/b> ends <b> up being the <\\\\/b> total <b> number of <\\\\/b> examples.\",\"80\":\"<b> This is <\\\\/b> made <b> up of 1\\\\/m times the sum of xi times yi, which <\\\\/b> fits <b> the <\\\\/b> means <b> of the <\\\\/b> classes.\",\"81\":\"Making <b> predictions using the minimum of z, where <\\\\/b> z <b> is <\\\\/b> any possible <b> value. <\\\\/b>\",\"82\":\"<b> The <\\\\/b> argmin <b> of z <\\\\/b> refers <b> to the value of z that minimises the <\\\\/b> expression.\",\"83\":\"<b> The minimum of z is <\\\\/b> attained <b> when z = 5, and the argmin of z is 5. <\\\\/b>\",\"84\":\"<b> The max of z and the argmax of z are <\\\\/b> operators <b> that exist, and <\\\\/b> they deal <b> with the maximum value of z. Let\\'s see <\\\\/b> how <b> to <\\\\/b> go <b> about finding the maximum value of z. <\\\\/b>\",\"85\":\"<b> So, <\\\\/b> do <b> we predict <\\\\/b> benign?\",\"86\":\"<b> This <\\\\/b> boils down <b> to <\\\\/b> predicting <b> the most <\\\\/b> likely <b> value of <\\\\/b> x, <b> which is given by the argmax of Pyx. <\\\\/b>\",\"87\":\"Looking <b> deeper, we can see that the rule for predicting y is given by the argmax of Pyx, which is equal to the argmax of PxyPyPx. <\\\\/b>\",\"88\":\"<b> The <\\\\/b> reasons <b> for this are <\\\\/b> highlighted <b> in the visualisation of the dataset, which shows how the model operates. <\\\\/b>\",\"89\":\"Pictorially, <b> the <\\\\/b> operation <b> of GDA is shown to be a <\\\\/b> shape <b> and orientation that <\\\\/b> share <b> a <\\\\/b> point, <b> which determines the decision boundary. This implies that the <\\\\/b> blue <b> line is the decision boundary, and Py1x = 0.5. <\\\\/b>\",\"90\":\"<b> The boundary is used to <\\\\/b> predict <b> the <\\\\/b> outcome, <b> and the side of the boundary determines the value of y. <\\\\/b>\",\"91\":\"Points <b> in the <\\\\/b> upper right <b> are <\\\\/b> closer <b> to being classified as y = 1, while points in the <\\\\/b> lower left <b> are <\\\\/b> classified <b> as y = 0. <\\\\/b>\",\"92\":\"Logistic <b> regression starts with <\\\\/b> x1 <b> and x2, <\\\\/b> whose <b> values are randomly initialised. <\\\\/b>\",\"93\":\"Typically, <b> we initialise the values for the <\\\\/b> purpose <b> of visualisation, <\\\\/b> starting off <b> with the values shown. <\\\\/b>\",\"94\":\"Running <b> a <\\\\/b> single iteration <b> of the algorithm, we reposition the decision boundary, and after <\\\\/b> 20 iterations, <b> the algorithm converges, illustrating the <\\\\/b> green <b> line <\\\\/b> superimposed <b> on the blue line. <\\\\/b>\",\"95\":\"<b> We <\\\\/b> arrive <b> at the decision boundary, which is <\\\\/b> slightly <b> different from the original boundary. <\\\\/b>\",\"96\":\"Why <b> do we have two separate means and a single matrix? <\\\\/b>\",\"97\":\"If <b> we have a <\\\\/b> lot <b> of <\\\\/b> problems.\",\"98\":\"Choosing <b> a reasonable value will <\\\\/b> work fine.\",\"99\":\"<b> The <\\\\/b> catch, though, <b> is that you\\'ll <\\\\/b> roughly double <b> the number of parameters, and it isn\\'t <\\\\/b> anymore.\",\"100\":\"<b> The discussion compares the <\\\\/b> interesting <b> relationship between <\\\\/b>\",\"101\":\"<b> You\\'ve seen the relationship between 0 and 1. <\\\\/b>\",\"102\":\"<b> The <\\\\/b> fixed <b> value of Py1x is 0 or 1. <\\\\/b>\",\"103\":\"<b> By the <\\\\/b> theorem, <b> Pxy1 = Pxy1,0,1,Py1Px,0,1, <\\\\/b>\",\"104\":\"<b> where Pxy1 is the <\\\\/b> measure <b> of <\\\\/b> evaluating <b> the <\\\\/b> optimum probability, <b> and we <\\\\/b> plot <b> Py1x, which is a <\\\\/b> simple <b> function of a <\\\\/b> few <b> variables. <\\\\/b>\",\"105\":\"Now, <b> we map the X-axis to the <\\\\/b> bump <b> of <\\\\/b> PXY0 <b> and <\\\\/b> PXY1.\",\"106\":\"<b> We also <\\\\/b> note <b> that the <\\\\/b> split <b> is 50-50 <\\\\/b> across <b> Py1 = <\\\\/b> 0.5, <b> which is <\\\\/b> known <b> as the half-way point. <\\\\/b>\",\"107\":\"Next, <b> we have Py1x = <\\\\/b> X.\",\"108\":\"<b> The <\\\\/b> unlabeled <b> points are <\\\\/b> far <b> away from the X-axis. <\\\\/b>\",\"109\":\"<b> We can <\\\\/b> infer <b> that the <\\\\/b> point almost certainly came <b> from the <\\\\/b> left, generating <b> a <\\\\/b> datapoint very <b> close to the X-axis. <\\\\/b>\",\"110\":\"<b> We can easily <\\\\/b> discern <b> that PXY1 = 0.5. <\\\\/b>\",\"111\":\"<b> On the <\\\\/b> right, <b> you\\'d be <\\\\/b> pretty sure <b> of the <\\\\/b> class.\",\"112\":\"<b> If we <\\\\/b> repeat <b> the exercise, <\\\\/b> sweeping <b> the <\\\\/b> datapoints <b> across a <\\\\/b> dense grid <b> and evaluating the <\\\\/b> measure, <b> you\\'ll <\\\\/b> notice <b> that as we <\\\\/b> approach <b> the <\\\\/b> midpoint, <b> the value <\\\\/b> increases <b> to <\\\\/b> 0.5 <b> and <\\\\/b> surpasses 0.5.\",\"113\":\"Beyond <b> a <\\\\/b> certain <b> point, plotting the connect-the-dots <\\\\/b> turns exactly <b> into the <\\\\/b> view <b> of the <\\\\/b> quantity <b> Py1x, which is 0 or 1. <\\\\/b>\",\"114\":\"<b> We <\\\\/b> find <b> that the form of Py1x is given by the expression 1 \\\\/ (1 + exp(-Tx)), which is <\\\\/b> appropriate <b> for 0 or 1. <\\\\/b>\",\"115\":\"<b> Py1x is 0 or 1. <\\\\/b>\",\"116\":\"<b> By convention, we redefine the <\\\\/b> xis <b> to be on the right-hand side, <\\\\/b> adding <b> a <\\\\/b> coordinate <b> xi0 = 1. <\\\\/b>\",\"117\":\"<b> This is 1 times x. <\\\\/b>\",\"118\":\"While <b> the <\\\\/b> hypothesis <b> function is a <\\\\/b> specific <b> choice, <\\\\/b> choosing <b> it is <\\\\/b> quite <b> leading. <\\\\/b>\",\"119\":\"Would <b> you prefer one or the other? <\\\\/b>\",\"120\":\"<b> In <\\\\/b> general, <b> the <\\\\/b> boundaries <b> yielded by the <\\\\/b> trained <b> models <\\\\/b>\",\"121\":\"Which <b> one is <\\\\/b> better?\",\"122\":\"<b> We <\\\\/b> discuss <b> which one is superior, and vice versa. <\\\\/b>\",\"123\":\"<b> It <\\\\/b> assumes <b> that <\\\\/b> xy0 <b> and <\\\\/b> xy1 <b> are parameters of the logistic function, <\\\\/b> governed <b> by Py1x = 1 \\\\/ (1 + exp(-Tx)). <\\\\/b>\",\"124\":\"<b> The <\\\\/b> element <b> x0 is <\\\\/b> raised <b> to the power of the <\\\\/b> plotting <b> point-by-point, <\\\\/b> ultimately <b> yielding a curve that shares the necessary assumptions. <\\\\/b>\",\"125\":\"<b> The converse is also true, and the <\\\\/b> implication <b> is that the Gaussian distribution <\\\\/b>\",\"126\":\"3 <b> implies 2, and this is a <\\\\/b> stronger <b> result. We <\\\\/b> prove 2, <b> and note that this is a <\\\\/b> property <b> of <\\\\/b> GDA.\",\"127\":\"<b> This is <\\\\/b> correct, <b> and <\\\\/b> better <b> than <\\\\/b> regression.\",\"128\":\"Specifically, <b> it is <\\\\/b> indeed asymptotically efficient.\",\"129\":\"<b> This is a <\\\\/b> strong <b> result, and you\\'re <\\\\/b> baking information <b> into the algorithm. <\\\\/b> Informally, <b> in the <\\\\/b> limit <b> of <\\\\/b> large sets m, there <b> is <\\\\/b> no strictly <b> better algorithm in <\\\\/b> terms <b> of accuracy. <\\\\/b>\"},\"temp_lower\":{\"0\":\"back to top cs229 gaussian discriminant analysis generative learning algorithms <b> we\\'ve <\\\\/b> mainly looked at learning algorithms that model pyx, the conditional distribution of y given x.\",\"1\":\"for instance, logistic regression <b> modelled <\\\\/b> pyx as hxgtx where g is sigmoid function.\",\"2\":\"in this section, <b> we\\'ll <\\\\/b> talk about a different type <b> of <\\\\/b> algorithm.\",\"3\":\"<b> let\\'s <\\\\/b> use binary classification problem motivation behind our discussion.\",\"4\":\"consider in which we want <b> to <\\\\/b> learn <b> to <\\\\/b> distinguish between malignant y1 and benign y0 <b> tumours. <\\\\/b>\",\"5\":\"given <b> a <\\\\/b> training set, an algorithm like , or initially starts with randomly <b> initialised <\\\\/b> parameters.\",\"6\":\"over <b> the <\\\\/b> course <b> of <\\\\/b> learning, <b> it <\\\\/b> performs gradient descent <b> on a <\\\\/b> straight line hyperplane decision <b> boundary, which <\\\\/b> evolves until <b> it <\\\\/b> separates <b> positive and negative examples. <\\\\/b> then, <b> it classifies a <\\\\/b> new sample <b> as <\\\\/b> either <b> benign or malignant, depending on which side <\\\\/b> it falls in, <b> and <\\\\/b> makes its prediction accordingly.\",\"7\":\"<b> there\\'s a <\\\\/b> class <b> that aren\\'t <\\\\/b> trying <b> to maximise likelihood, <\\\\/b> looking <b> at <\\\\/b> both classes <b> and <\\\\/b> searching for <b> a <\\\\/b> separation boundary.\",\"8\":\"instead, these look <b> at <\\\\/b> one time.\",\"9\":\"first, <b> tumours <\\\\/b> can <b> have <\\\\/b> features <b> that are like other tumours. <\\\\/b>\",\"10\":\"<b> build <\\\\/b> separate <b> models for each tumour type. <\\\\/b> finally, match <b> the tumour <\\\\/b> against <b> the tumour <\\\\/b> model, <b> and <\\\\/b> see whether <b> it <\\\\/b> looks more <b> like the tumours in the training <\\\\/b> set.\",\"11\":\"<b> try to <\\\\/b> directly <b> map <\\\\/b> from <b> the <\\\\/b> input space x <b> to the labels, which <\\\\/b> are called discriminative algorithms.\",\"12\":\"<b> instead, <\\\\/b> pxy <b> = <\\\\/b> py.\",\"13\":\"these generative <b> models indicate that, for example, <\\\\/b> pxy0 models <b> the features of class 0, <\\\\/b> while pxy1 <b> models the features of class 1. <\\\\/b>\",\"14\":\"the <b> model <\\\\/b> also learns <b> the <\\\\/b> prior <b> probability py, which is <\\\\/b> independent <b> of <\\\\/b> y.\",\"15\":\"to illustrate <b> this concept, consider a <\\\\/b> practical <b> example: <\\\\/b> when <b> a <\\\\/b> patient walks into <b> a <\\\\/b> hospital, before <b> the <\\\\/b> doctors even <b> see <\\\\/b> them, <b> the <\\\\/b> odds <b> of <\\\\/b> their <b> having a particular disease <\\\\/b> versus <b> not having it are <\\\\/b> referred <b> to as the prior probability. <\\\\/b>\",\"16\":\"thus, <b> the model <\\\\/b> builds each <b> in <\\\\/b> isolation.\",\"17\":\"at test time, <b> the model <\\\\/b> evaluates <b> the <\\\\/b> models, identifies <b> which one <\\\\/b> matches most <b> closely, and <\\\\/b> returns <b> the <\\\\/b> prediction.\",\"18\":\"after <b> modelling the <\\\\/b> priors pxy, <b> we can <\\\\/b> derive <b> the <\\\\/b> posterior <b> probability <\\\\/b> x <b> using bayes\\' rule: pyx = pxypypx <\\\\/b>\",\"19\":\"here, <b> the <\\\\/b> denominator <b> is given <\\\\/b> by <b> px = pxy1py1 + pxy0py0, which is a <\\\\/b> function <b> of the <\\\\/b> quantities\",\"20\":\"note <b> that the <\\\\/b> learned part <b> of the <\\\\/b> process <b> is <\\\\/b> calculating <b> the <\\\\/b> order <b> to <\\\\/b> make <b> a <\\\\/b> prediction, <b> but we don\\'t <\\\\/b> actually need <b> to <\\\\/b> calculate <b> the <\\\\/b> value <b> of <\\\\/b> px since <b> it is a constant and doesn\\'t <\\\\/b> appear <b> in the final prediction. <\\\\/b>\",\"21\":\"when making <b> predictions, algorithms <\\\\/b> thus ignore computing <b> px to <\\\\/b> save computation.\",\"22\":\"however, <b> the <\\\\/b> end goal <b> is to compute the value of px, which <\\\\/b> would <b> allow us to normalise the <\\\\/b> numerator.\",\"23\":\"<b> the <\\\\/b> equation <b> pyx = pxypypx <\\\\/b> represents <b> the <\\\\/b> underlying <b> framework. <\\\\/b>\",\"24\":\"key <b> takeaways: discriminative models, <\\\\/b> i.e., output <b> is a function of <\\\\/b> input.\",\"25\":\"<b> in <\\\\/b> other words, <b> hx is 0 or 1 <\\\\/b> directly.\",\"26\":\"<b> in the case of tumour identification, we <\\\\/b> may <b> use <\\\\/b> discriminant analysis <b> (gda) for continuous-valued <\\\\/b> classification.\",\"27\":\"<b> we <\\\\/b> assume <b> that the data is <\\\\/b> distributed according <b> to a <\\\\/b> multivariate normal distribution.\",\"28\":\"<b> we will <\\\\/b> briefly <b> discuss the <\\\\/b> properties <b> of the <\\\\/b> distributions <b> and then move on to <\\\\/b> gda itself.\",\"29\":\"<b> a multivariate distribution is a generalisation of a 1-dimensional <\\\\/b> random variable <b> to an n-dimensional random <\\\\/b> variable.\",\"30\":\"<b> rather <\\\\/b> than <b> a <\\\\/b> univariate variable, <b> it <\\\\/b> seeks multiple variables.\",\"31\":\"<b> we assume that the data is <\\\\/b> gaussian, <b> i.e., x ~ rn, parameterised by a <\\\\/b> mean vector <b> in <\\\\/b> rn <b> and a <\\\\/b> covariance matrix <b> in <\\\\/b> rnn, <b> which is <\\\\/b> symmetric <b> and <\\\\/b> positive semidefinite.\",\"32\":\"formally, <b> this can be <\\\\/b> written <b> as x ~ n(\\\\u03bc, \\\\u03c3), where the <\\\\/b> density pdf <b> is given by px(x) = (1\\\\/\\\\u221a(2\\\\u03c0)^n) * exp(-1\\\\/2 * (x-\\\\u03bc)^t * \\\\u03c3^(-1) * (x-\\\\u03bc)), where \\\\u03bc is the mean vector, \\\\u03c3 is the covariance matrix, and e[x] is the <\\\\/b> expected <b> value of x. <\\\\/b>\",\"33\":\"covariance <b> generalises the <\\\\/b> notion <b> of <\\\\/b> variance <b> in the real-valued <\\\\/b> setting.\",\"34\":\"<b> e[e^t] = e[e]e^t. <\\\\/b>\",\"35\":\"since <b> cov(x) = e[(x-e[x])(x-e[x])^t], <\\\\/b>\",\"36\":\"<b> let\\'s <\\\\/b> explore some <b> visualisations. <\\\\/b> recall <b> the <\\\\/b> familiar\",\"37\":\"similarly, <b> a multivariate distribution can be <\\\\/b> represented <b> by the <\\\\/b> same <b> bell-shaped curve, but with <\\\\/b> two parameters <b> controlling the pdf in n dimensions. <\\\\/b>\",\"38\":\"<b> for example, a 2-dimensional distribution <\\\\/b> over 2 <b> variables has a <\\\\/b> size <b> of 2x2. <\\\\/b>\",\"39\":\"<b> the figure <\\\\/b> below shows <b> a zero-mean vector, a 2x1 <\\\\/b> zero <b> vector, and a 2x2 <\\\\/b> identity matrix.\",\"40\":\"a standard <b> deviation of <\\\\/b> 0.6i.\",\"41\":\"<b> essentially, the <\\\\/b> number has <b> been multiplied by 0.6, which has <\\\\/b> shrunk <b> the variance and <\\\\/b> reduced <b> the variability. <\\\\/b> 2i.\",\"42\":\"from <b> the <\\\\/b> images, <b> we can see that as the covariance matrix <\\\\/b> becomes larger, <b> the distribution becomes wider and shorter, and as it becomes <\\\\/b> smaller, <b> the distribution becomes compressed and taller. <\\\\/b>\",\"43\":\"this <b> is <\\\\/b> because <b> the area under the curve <\\\\/b> always integrates <b> to 1, <\\\\/b> so <b> as the <\\\\/b> spread <b> increases, the <\\\\/b> height <b> decreases, and vice versa. the <\\\\/b> figures show gaussians corresponding <b> to the matrices, and as the off-diagonal <\\\\/b> entries <b> increase, the distribution becomes <\\\\/b> compressed towards <b> the 45\\\\u00b0 line between x1 and x2. <\\\\/b>\",\"44\":\"geometrically speaking, <b> this <\\\\/b> implies <b> that the <\\\\/b> variables <b> are <\\\\/b> positively correlated.\",\"45\":\"we <b> can <\\\\/b> clearly <b> see the <\\\\/b> contours <b> of the <\\\\/b> three <b> densities, which are <\\\\/b> 3d <b> bumps. the <\\\\/b> aspect ratio <b> of the <\\\\/b> image <b> is <\\\\/b> probably <b> a <\\\\/b> little bit fatter <b> in some places, but the contours should be perfectly round circles. <\\\\/b>\",\"46\":\"<b> here\\'s <\\\\/b> another set <b> of images, this time with <\\\\/b> decreasing elements <b> of the <\\\\/b> matrix, <b> and <\\\\/b> again, <b> in the <\\\\/b> opposite direction, <b> along the 135\\\\u00b0 <\\\\/b> line.\",\"47\":\"again, geometrically, <b> this endows the variables with a <\\\\/b> negative correlation.\",\"48\":\"<b> as we <\\\\/b> vary <b> the <\\\\/b> parameters, <b> the distribution tends to <\\\\/b> form ellipses.\",\"49\":\"by varying <b> the <\\\\/b> shift <b> of the <\\\\/b> center around.\",\"50\":\"another <b> way to visualise the <\\\\/b> pdfs <b> is to <\\\\/b> carry out <b> the <\\\\/b> eigenvectors <b> and <\\\\/b> points <b> of the <\\\\/b> principal axes <b> of the <\\\\/b> ellipse contours.\",\"51\":\"<b> the <\\\\/b> takeaway <b> is that as the density changes, the spread and height <\\\\/b> change respectively.\",\"52\":\"<b> the model we <\\\\/b> have <b> been considering for this <\\\\/b> task <b> is <\\\\/b> expressed <b> in terms of <\\\\/b> equations <b> for <\\\\/b> pxy1, <b> which are <\\\\/b> gaussians.\",\"53\":\"on <b> the other <\\\\/b> hand, <b> the <\\\\/b> bernoulli <b> distribution <\\\\/b> takes <b> values 0 and 1. <\\\\/b>\",\"54\":\"<b> by the way, xy0 ~ n0, xy1 ~ n1, and y ~ bernoulli(0, 1), where the classes are represented by <\\\\/b> vectors <b> of length <\\\\/b> 1.\",\"55\":\"put <b> simply, we <\\\\/b> were assuming <b> that the means are represented by <\\\\/b>\",\"56\":\"you <b> may <\\\\/b> not <b> have <\\\\/b> commonly <b> seen this. <\\\\/b>\",\"57\":\"more <b> on this in the <\\\\/b> section below.\",\"58\":\"<b> to <\\\\/b> fit your data, <b> we <\\\\/b> will define <b> the <\\\\/b> data.\",\"59\":\"r, <b> 0 in rn, 1 in rn, and <\\\\/b> rnn.\",\"60\":\"writing <b> the <\\\\/b> distributions, <b> py(y=1) = pxy0 ~ n(0, 1), py(y=1) = pxy1 ~ n(1, 1) <\\\\/b>\",\"61\":\"<b> using <\\\\/b> exponential notation similar <b> to earlier, plugging in pxy0, pxy1, py0, and <\\\\/b> py1 <b> into the formula, we can <\\\\/b> easily ascertain <b> the <\\\\/b> particular example.\",\"62\":\"suppose <b> we have xi and yi, for i = 1 to m. <\\\\/b>\",\"63\":\"<b> the <\\\\/b> aforementioned joint <b> likelihood l is given by l = \\\\u220f[p(xi, yi) for i = 1 to m] = \\\\u220f[p(xi|yi)p(yi) for i = 1 to m] <\\\\/b>\",\"64\":\"<b> there is a <\\\\/b> big difference <b> between the <\\\\/b> cost <b> functions. we <\\\\/b> choose <b> to maximise the likelihood of the data, which is given by l = \\\\u220f[p(xi, yi) for i = 1 to m]. <\\\\/b>\",\"65\":\"<b> in <\\\\/b> linear regression, <b> we generalise <\\\\/b> pyx.\",\"66\":\"<b> the log-likelihood of the <\\\\/b> data <b> is <\\\\/b> simply log l.\",\"67\":\"<b> the log-likelihood is given by log l = \\\\u2211[log p(xi, yi) for i = 1 to m] = \\\\u2211[log p(xi|yi)p(yi) for i = 1 to m] <\\\\/b>\",\"68\":\"<b> taking the derivative with <\\\\/b> respect <b> to the parameters and setting it <\\\\/b> equal <b> to zero, we can <\\\\/b> solve <b> for the <\\\\/b> expression <b> that <\\\\/b> yields <b> the <\\\\/b> maximum <b> likelihood estimate, which is given by \\\\u03bc = (1\\\\/m) * \\\\u2211[yi * xi for i = 1 to m] <\\\\/b>\",\"69\":\"an intuitive explanation <b> of how this maximises the likelihood <\\\\/b> follows.\",\"70\":\"<b> for <\\\\/b> example, <b> the <\\\\/b> chance <b> of the <\\\\/b> next office <b> being occupied is <\\\\/b> denoted <b> by the <\\\\/b> bias <b> of a <\\\\/b> coin <b> toss, which is the <\\\\/b> fraction <b> of heads. <\\\\/b> likewise, <b> we <\\\\/b> just label <b> y as 1. <\\\\/b>\",\"71\":\"<b> we can <\\\\/b> write <b> this in <\\\\/b> indicator <b> notation as 1(yi = 1) = <\\\\/b> 1 <b> if the <\\\\/b> argument <b> is <\\\\/b> true, <b> and 0 otherwise. <\\\\/b>\",\"72\":\"<b> this is equivalent to a <\\\\/b> true statement <b> being <\\\\/b> false <b> in an if-statement in a <\\\\/b> programming context.\",\"73\":\"<b> 0 or 1 is given by 0 if yi = 0, and 1 if yi = 1. <\\\\/b>\",\"74\":\"<b> the <\\\\/b> intuition <b> behind this expression is to <\\\\/b> think <b> about <\\\\/b> all <b> the data points in the <\\\\/b> dataset.\",\"75\":\"<b> a <\\\\/b> reasonable <b> example is to take the average of the <\\\\/b> 0s <b> and 1s in the dataset. <\\\\/b>\",\"76\":\"<b> the <\\\\/b> numerator <b> is the <\\\\/b> sum <b> of the <\\\\/b> feature sums <b> over the <\\\\/b> entire <b> dataset, <\\\\/b> summing <b> over all <\\\\/b> instances <b> where yi = 0, and using the instances where yi = 1 <\\\\/b> times xi.\",\"77\":\"<b> this has the <\\\\/b> effect <b> of <\\\\/b> zeroing <b> out the <\\\\/b> term.\",\"78\":\"<b> this is because we are summing over all <\\\\/b> samples <b> where yi = 0. <\\\\/b>\",\"79\":\"<b> this is because <\\\\/b> every <b> time yi = 0, we <\\\\/b> get <b> an <\\\\/b> extra sum, <b> which <\\\\/b> ends <b> up being the <\\\\/b> total <b> number of <\\\\/b> examples.\",\"80\":\"<b> this is <\\\\/b> made <b> up of 1\\\\/m times the sum of xi times yi, which <\\\\/b> fits <b> the <\\\\/b> means <b> of the <\\\\/b> classes.\",\"81\":\"making <b> predictions using the minimum of z, where <\\\\/b> z <b> is <\\\\/b> any possible <b> value. <\\\\/b>\",\"82\":\"<b> the <\\\\/b> argmin <b> of z <\\\\/b> refers <b> to the value of z that minimises the <\\\\/b> expression.\",\"83\":\"<b> the minimum of z is <\\\\/b> attained <b> when z = 5, and the argmin of z is 5. <\\\\/b>\",\"84\":\"<b> the max of z and the argmax of z are <\\\\/b> operators <b> that exist, and <\\\\/b> they deal <b> with the maximum value of z. let\\'s see <\\\\/b> how <b> to <\\\\/b> go <b> about finding the maximum value of z. <\\\\/b>\",\"85\":\"<b> so, <\\\\/b> do <b> we predict <\\\\/b> benign?\",\"86\":\"<b> this <\\\\/b> boils down <b> to <\\\\/b> predicting <b> the most <\\\\/b> likely <b> value of <\\\\/b> x, <b> which is given by the argmax of pyx. <\\\\/b>\",\"87\":\"looking <b> deeper, we can see that the rule for predicting y is given by the argmax of pyx, which is equal to the argmax of pxypypx. <\\\\/b>\",\"88\":\"<b> the <\\\\/b> reasons <b> for this are <\\\\/b> highlighted <b> in the visualisation of the dataset, which shows how the model operates. <\\\\/b>\",\"89\":\"pictorially, <b> the <\\\\/b> operation <b> of gda is shown to be a <\\\\/b> shape <b> and orientation that <\\\\/b> share <b> a <\\\\/b> point, <b> which determines the decision boundary. this implies that the <\\\\/b> blue <b> line is the decision boundary, and py1x = 0.5. <\\\\/b>\",\"90\":\"<b> the boundary is used to <\\\\/b> predict <b> the <\\\\/b> outcome, <b> and the side of the boundary determines the value of y. <\\\\/b>\",\"91\":\"points <b> in the <\\\\/b> upper right <b> are <\\\\/b> closer <b> to being classified as y = 1, while points in the <\\\\/b> lower left <b> are <\\\\/b> classified <b> as y = 0. <\\\\/b>\",\"92\":\"logistic <b> regression starts with <\\\\/b> x1 <b> and x2, <\\\\/b> whose <b> values are randomly initialised. <\\\\/b>\",\"93\":\"typically, <b> we initialise the values for the <\\\\/b> purpose <b> of visualisation, <\\\\/b> starting off <b> with the values shown. <\\\\/b>\",\"94\":\"running <b> a <\\\\/b> single iteration <b> of the algorithm, we reposition the decision boundary, and after <\\\\/b> 20 iterations, <b> the algorithm converges, illustrating the <\\\\/b> green <b> line <\\\\/b> superimposed <b> on the blue line. <\\\\/b>\",\"95\":\"<b> we <\\\\/b> arrive <b> at the decision boundary, which is <\\\\/b> slightly <b> different from the original boundary. <\\\\/b>\",\"96\":\"why <b> do we have two separate means and a single matrix? <\\\\/b>\",\"97\":\"if <b> we have a <\\\\/b> lot <b> of <\\\\/b> problems.\",\"98\":\"choosing <b> a reasonable value will <\\\\/b> work fine.\",\"99\":\"<b> the <\\\\/b> catch, though, <b> is that you\\'ll <\\\\/b> roughly double <b> the number of parameters, and it isn\\'t <\\\\/b> anymore.\",\"100\":\"<b> the discussion compares the <\\\\/b> interesting <b> relationship between <\\\\/b>\",\"101\":\"<b> you\\'ve seen the relationship between 0 and 1. <\\\\/b>\",\"102\":\"<b> the <\\\\/b> fixed <b> value of py1x is 0 or 1. <\\\\/b>\",\"103\":\"<b> by the <\\\\/b> theorem, <b> pxy1 = pxy1,0,1,py1px,0,1, <\\\\/b>\",\"104\":\"<b> where pxy1 is the <\\\\/b> measure <b> of <\\\\/b> evaluating <b> the <\\\\/b> optimum probability, <b> and we <\\\\/b> plot <b> py1x, which is a <\\\\/b> simple <b> function of a <\\\\/b> few <b> variables. <\\\\/b>\",\"105\":\"now, <b> we map the x-axis to the <\\\\/b> bump <b> of <\\\\/b> pxy0 <b> and <\\\\/b> pxy1.\",\"106\":\"<b> we also <\\\\/b> note <b> that the <\\\\/b> split <b> is 50-50 <\\\\/b> across <b> py1 = <\\\\/b> 0.5, <b> which is <\\\\/b> known <b> as the half-way point. <\\\\/b>\",\"107\":\"next, <b> we have py1x = <\\\\/b> x.\",\"108\":\"<b> the <\\\\/b> unlabeled <b> points are <\\\\/b> far <b> away from the x-axis. <\\\\/b>\",\"109\":\"<b> we can <\\\\/b> infer <b> that the <\\\\/b> point almost certainly came <b> from the <\\\\/b> left, generating <b> a <\\\\/b> datapoint very <b> close to the x-axis. <\\\\/b>\",\"110\":\"<b> we can easily <\\\\/b> discern <b> that pxy1 = 0.5. <\\\\/b>\",\"111\":\"<b> on the <\\\\/b> right, <b> you\\'d be <\\\\/b> pretty sure <b> of the <\\\\/b> class.\",\"112\":\"<b> if we <\\\\/b> repeat <b> the exercise, <\\\\/b> sweeping <b> the <\\\\/b> datapoints <b> across a <\\\\/b> dense grid <b> and evaluating the <\\\\/b> measure, <b> you\\'ll <\\\\/b> notice <b> that as we <\\\\/b> approach <b> the <\\\\/b> midpoint, <b> the value <\\\\/b> increases <b> to <\\\\/b> 0.5 <b> and <\\\\/b> surpasses 0.5.\",\"113\":\"beyond <b> a <\\\\/b> certain <b> point, plotting the connect-the-dots <\\\\/b> turns exactly <b> into the <\\\\/b> view <b> of the <\\\\/b> quantity <b> py1x, which is 0 or 1. <\\\\/b>\",\"114\":\"<b> we <\\\\/b> find <b> that the form of py1x is given by the expression 1 \\\\/ (1 + exp(-tx)), which is <\\\\/b> appropriate <b> for 0 or 1. <\\\\/b>\",\"115\":\"<b> py1x is 0 or 1. <\\\\/b>\",\"116\":\"<b> by convention, we redefine the <\\\\/b> xis <b> to be on the right-hand side, <\\\\/b> adding <b> a <\\\\/b> coordinate <b> xi0 = 1. <\\\\/b>\",\"117\":\"<b> this is 1 times x. <\\\\/b>\",\"118\":\"while <b> the <\\\\/b> hypothesis <b> function is a <\\\\/b> specific <b> choice, <\\\\/b> choosing <b> it is <\\\\/b> quite <b> leading. <\\\\/b>\",\"119\":\"would <b> you prefer one or the other? <\\\\/b>\",\"120\":\"<b> in <\\\\/b> general, <b> the <\\\\/b> boundaries <b> yielded by the <\\\\/b> trained <b> models <\\\\/b>\",\"121\":\"which <b> one is <\\\\/b> better?\",\"122\":\"<b> we <\\\\/b> discuss <b> which one is superior, and vice versa. <\\\\/b>\",\"123\":\"<b> it <\\\\/b> assumes <b> that <\\\\/b> xy0 <b> and <\\\\/b> xy1 <b> are parameters of the logistic function, <\\\\/b> governed <b> by py1x = 1 \\\\/ (1 + exp(-tx)). <\\\\/b>\",\"124\":\"<b> the <\\\\/b> element <b> x0 is <\\\\/b> raised <b> to the power of the <\\\\/b> plotting <b> point-by-point, <\\\\/b> ultimately <b> yielding a curve that shares the necessary assumptions. <\\\\/b>\",\"125\":\"<b> the converse is also true, and the <\\\\/b> implication <b> is that the gaussian distribution <\\\\/b>\",\"126\":\"3 <b> implies 2, and this is a <\\\\/b> stronger <b> result. we <\\\\/b> prove 2, <b> and note that this is a <\\\\/b> property <b> of <\\\\/b> gda.\",\"127\":\"<b> this is <\\\\/b> correct, <b> and <\\\\/b> better <b> than <\\\\/b> regression.\",\"128\":\"specifically, <b> it is <\\\\/b> indeed asymptotically efficient.\",\"129\":\"<b> this is a <\\\\/b> strong <b> result, and you\\'re <\\\\/b> baking information <b> into the algorithm. <\\\\/b> informally, <b> in the <\\\\/b> limit <b> of <\\\\/b> large sets m, there <b> is <\\\\/b> no strictly <b> better algorithm in <\\\\/b> terms <b> of accuracy. <\\\\/b>\"}}'"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['changes'].str.contains('<b>')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv('a_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(filtered_df['originalContent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "caution_word_list = [\"Best\", \"Specialist\", \"Specialised\", \"Finest\", \"Most experienced\",\n",
    "                                \"Superior\", \"Principle\", \"Expert\", \"Amazing\", \"Speciality\",\n",
    "                                \"leader\", \"leaders\", \"service\", \"implantologist\"\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discuss <b>superior</b> viceversa.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Function to add <b></b> around caution words\n",
    "def highlight_caution_words(sentence):\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(word) for word in caution_word_list) + r')\\b', re.IGNORECASE)\n",
    "    highlighted_sentence = pattern.sub(r'<b>\\1</b>', sentence)\n",
    "    if highlighted_sentence != sentence:\n",
    "        return highlighted_sentence\n",
    "    return None\n",
    "\n",
    "# Apply the function to all sentences and filter out None results\n",
    "highlighted_sentences = [highlight_caution_words(sentence) for sentence in sentences if highlight_caution_words(sentence)]\n",
    "\n",
    "# Print the results\n",
    "for sentence in highlighted_sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
