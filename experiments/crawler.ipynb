{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import FireCrawlLoader\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "import threading\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator, root_validator\n",
    "# Load HTML\n",
    "from langchain_community.document_loaders import FireCrawlLoader\n",
    "from langchain_groq import ChatGroq\n",
    "from typing import List\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import os\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import List\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = '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'\n",
    "open_ai_key = 'sk-None-YYN2MievH2m1islKXEIkT3BlbkFJ9vmzUu8bHOOXgK9498jA'\n",
    "os.environ[\"OPENAI_API_KEY\"] = open_ai_key\n",
    "llm=ChatOpenAI(temperature=0.0, model_name=\"gpt-4o\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_markdown_content(page_content):\n",
    "    llm = ChatGroq(\n",
    "            model=\"llama-3.1-70b-versatile\",\n",
    "            temperature=0,\n",
    "            max_tokens=None,\n",
    "            timeout=None,\n",
    "            max_retries=3,\n",
    "            # other params...\n",
    "        )\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "            template=\" You will be given a page which contain link and other html stuff. You need to clean it and return only text. No need for images link or anything like that.\\n \\n{query}\\n\",\n",
    "            input_variables=[\"query\"],\n",
    "        )\n",
    "    chain = prompt | llm  \n",
    "\n",
    "    result = chain.invoke({\"query\": page_content})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url,mode='scrape',ignore_pages=[],max_pages=4):\n",
    "    if mode == 'scrape':\n",
    "        loader = FireCrawlLoader(\n",
    "        api_key=\"fc-d22f7f0d1f7c46949c24b237959797f6\", url=url, mode=mode\n",
    "            )\n",
    "    else:\n",
    "        crawl_params = {\n",
    "                'crawlerOptions': {\n",
    "                    'excludes': [\n",
    "                        'blog/*', 'login/*', 'account/*', 'user/*', 'profile/*',\n",
    "                        'admin/*', 'dashboard/*', 'search/*', 'filter/*',\n",
    "                        'checkout/*', 'payment/*', 'cart/*', 'css/*', 'js/*',\n",
    "                        'images/*', 'assets/*', 'temp/*', 'under-construction/*',\n",
    "                        'api/*', '404', '500', 'downloads/*', 'files/*', 'pdfs/*',\n",
    "                        'archive/*', 'old/*', 'version/*', 'forum/*', 'comments/*',\n",
    "                        'reviews/*', 'external/*', 'outbound/*', 'product/*', 'shop/*',\n",
    "                        'category/*', 'promo/*', 'deals/*', 'offers/*', 'help/*',\n",
    "                        'support/*', 'news/*', 'press/*', 'events/*',\n",
    "                        'calendar/*', 'subscribe/*', 'signup/*', 'test/*', 'staging/*',\n",
    "                        'wp-admin/*', 'backend/*', 'admin-panel/*', 'management/*'\n",
    "                    ],\n",
    "                    'includes': [], # leave empty for all pages\n",
    "                    'limit': max_pages,\n",
    "                }\n",
    "            }\n",
    "\n",
    "        loader = FireCrawlLoader(\n",
    "        api_key=\"fc-d22f7f0d1f7c46949c24b237959797f6\", url=url, mode=mode,\n",
    "        params=crawl_params\n",
    "            ) \n",
    "    docs = loader.load()\n",
    "    # print(docs)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content = get_html(\"https://aman.ai/cs229/gda/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_repetitions(text):\n",
    "    words = text.split()\n",
    "    seen = set()\n",
    "    result = []\n",
    "\n",
    "    for word in words:\n",
    "        if word not in seen:\n",
    "            seen.add(word)\n",
    "            result.append(word)\n",
    "\n",
    "    return ' '.join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_plain_text_from_markdown(markdown_content):\n",
    "    # Remove image tags (Markdown and HTML)\n",
    "    markdown_content = re.sub(r'!\\[.*?\\]\\(.*?\\)', '', markdown_content)\n",
    "    markdown_content = re.sub(r'<img.*?>', '', markdown_content)\n",
    "    \n",
    "    # Remove links (Markdown and plain URLs)\n",
    "    markdown_content = re.sub(r'\\[.*?\\]\\(.*?\\)', '', markdown_content)\n",
    "    markdown_content = re.sub(r'\\(https?://.*?\\)', '', markdown_content)\n",
    "    markdown_content = re.sub(r'https?://\\S+|www\\.\\S+', '', markdown_content)\n",
    "    \n",
    "    # Remove any remaining inline links\n",
    "    markdown_content = re.sub(r'\\[.*?\\]', '', markdown_content)\n",
    "    \n",
    "    # Remove HTML tags (including SVG)\n",
    "    markdown_content = re.sub(r'<.*?>', '', markdown_content)\n",
    "    \n",
    "    # Remove all symbols except ., \"\", '', ?, and ,\n",
    "    markdown_content = re.sub(r\"[^a-zA-Z0-9\\s.,\\\"'?']\", '', markdown_content)\n",
    "    \n",
    "    # Remove any extra whitespace and newlines\n",
    "    markdown_content = re.sub(r'\\s+', ' ', markdown_content).strip()\n",
    "    \n",
    "    return markdown_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_content = extract_plain_text_from_markdown(html_content[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_content = remove_all_repetitions(cleaned_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Back to Top CS229 Gaussian Discriminant Analysis Generative Learning Algorithms Weve mainly looked at learning algorithms that model Pyx, the conditional distribution of y given x. For instance, logistic regression modeled Pyx as hxgTx where g is sigmoid function. In this section, well talk about a different type algorithm. Lets use binary classification problem motivation behind our discussion. Consider in which we want learn distinguish between malignant y1 and benign y0 tumors. Given training set, an algorithm like , or initially starts with randomly initialized parameters. Over course learning, performs gradient descent straight line hyperplane decision boundary evolves until you obtain separates positivenegative examples case, Then, classify new sample either benign, it checks on side falls in, makes its prediction accordingly. Theres class arent trying maximize likelihood looking both classes searching for separation boundary. Instead, these look one time. First, tumors, can features what tumors like. build separate Finally, tumor, match against tumor model, see whether looks more had seen set. try directly such mapping from input space X labels are called discriminative algorithms. instead Pxy Py. These generative if indicates example 0 1, then Pxy0 models features, while Pxy1 features. The also learns prior Py independent probability y. To illustrate concept using practical when patient walks into hospital, before doctors even them, odds their versus referred prior. Thus, builds each isolation. At test time, evaluates models, identifies matches most closely returns prediction. After modeling priors Pxy, Bayes rule derive posterior x PyxPxyPyPx Here, denominator by PxPxy1Py1Pxy0Py0, function quantities Note learned part process calculating order make prediction, dont actually need calculate value Px since constant, doesnt appear there. When making predictions algorithms, thus ignore computing save computation. However, end goal value, would compute be able normalize numerator. PyxPxyPyPxPxyPy above equation represents underlying framework . Key takeaways Discriminative i.e., output input. other words, hx0,1 directly. class, tumoridentification setting, may case them first discriminant analysis GDA, used continuousvalued say, classification. assume distributed according multivariate normal distribution. briefly properties distributions moving GDA itself. Multivariate Distribution generalization 1dimensional random variable ndimensional simply, nrandom variable. rather than univariate variable, seeks multiple variables. Assume Gaussian, i.e, XRn, parameterized mean vector Rn covariance matrix Rnn, symmetric positive semidefinite. Formally, written as, XN, density PDF PX,12n212exp12XT1X denotes determinant expected ExxPx,dx vectorvalued defined CovX EXET. Covariance generalizes notion variance realvalued setting. EEET. Since CovX. explore some visualize Recall familiar Similarly, multivariable represented same bellshaped curve two parameters control PDF, but ndimensions. e.g., 2dimensional over 2 variables, 2dimensional, size 22. below figure shows zero 21 zerovector I 22 identity matrix. A standard 0.6I. essentially taken multiplied number has shrunk variance, reduced variability 2I. From images, becomes larger, widershorter, smaller, compressedtaller. This because always integrates area under so scales spread vs. height figures show Gaussians corresponding matrices figures, increase offdiagonal entries compressed towards 45 x1x2. Geometrically speaking, implies variables positively correlated. We clearly contours three densities 3D bumps saw above, should perfectly round circles, aspect ratio image probably little bit fatter places being Heres another set along decreasing elements matrix, now again, opposite direction, 135 line. Again, geometrically, endow negative correlation. vary parameters, tend form ellipses. By varying shift center around. Another values I, visualization PDFs follows carry out eigenvectors points principal axes ellipse contours. takeaway As density, change spreadheight respectively. Model have consider task discussion, expressed equations Pxy1, Gaussians. On hand, Bernoulli takes 0,1. by, xy0N0,xy1N1,yBernoulli 0, classes, vectors 1. Put were assuming representing means. You not commonly seen. More section below. fit your data, will define data. R, 0Rn, 1Rn, Rnn. Writing distributions, Pyy11yPxy012n212exp12x0T1x0Pxy112n212exp12x1T1x1 exponential notation similar earlier Plugging Pxy0,Pxy1,Py0 Py1 formula easily ascertain particular example. Suppose xi,yimi1. aforementioned going joint L, L,0,1,mi1Pxi,yi,0,1,mi1Pxiyi0,1,Pyi big difference cost functions compared choose ,0,1,, Px,y,0,1,. linear regression, generalized Pyx. loglikelihood data simply log L. ,0,1,logmi1Pxi,yi,0,1,logmi1Pxiyi0,1,Pyi respect take derivative equal solve expression yields maximum estimate be, mi1yim An intuitive explanation around maximizes follows. example, chance next office denoted bias coin toss fraction heads Likewise, just label y1. way write indicator notation, mi11yi1m 1 argument true, otherwise 0. true statement false equivalent ifstatement programming context. 0,1 is, 0mi11yi0ximi11yi01mi11yi1ximi11yi1 intuition expression, think all say dataset. reasonable examples, 0s average writing intuition. numerator sum feature sums entire summing i1m, uses instances yi0 times xi. effect term whereas effectively zeroing term. up samples yi0. count represent Because every yi0, get extra sum, ends total examples. made 1mmi1xiyixiyiT fits means classes. Making Predictions Using minz min z minimum any possible z. argminz argmin refers plugged expression. minzz52 attained z520 argminzz52 led z52, z5. maxz argmaxz operators exist they deal leads having lets how go So do benign? boils down predicting likely x, formally argmaxyPyx Looking deeper y0,1 rule, argmaxyPyxargmaxyPxyPyPx reasons highlighted Visualization dataset compareandcontrast operate Pictorially, GDAs operation Shown been shape orientation, share point, determine put together, imply blue Py1x0.5. boundary, predict outcome, side, y0. Points upper right closer classifying lower left classified Logistic Regression x1 x2 start whose randomly. Typically, initialize purpose visualization, starting off shown Running single iteration PYX, repositions 20 iterations, converges illustrates green superimposed blue. arrive slightly boundaries. Why Two Separate Means, Single Matrix? If lot problems. Choosing reasonable, work fine. catch, though, youll roughly double isnt anymore. Discussion Comparing interesting relationship. youve ,0,1,. fixed Py1x,0,1,. theorem, Pxy1,0,1,Pxy1,0,1,Py1Px,0,1, where, Pxy1,0,1, measure evaluating once determined optimum probability, plot Py1x so, simple few Now, Mappingprojecting Xaxis, bump PXY0 PXY1. Were Also, note split 5050 across PY1 0.5, known half Next, PY1X X. unlabeled far Xaxis. infer point almost certainly came left, generating Xaxis datapoint very small. pick Its easy discern PXY10.5. right, youd pretty sure class. repeat exercise sweeping datapoints dense grid evaluate measure, Youll notice coming approach midpoint, increases 0.5 surpasses 0.5. Beyond certain tends Plotting connect dots, turns exactly view quantity Py1x,0,1, find form, Py1x,,0,111expTx appropriate ,,0,1. convention redefining xis righthandside n1 dimensional adding coordinate xi01. 1x. While hypothesis function, hood specific choice choosing quite leading Would Prefer One Another? general, yield boundaries trained Which better? discuss superior viceversa. assumes that, xy0 xy1 parameter logistic, governed Py1x11expTx element x0, raised plotting pointbypoint ultimately yielded curve, shared necessarily assumptions 3. converse, implication does Gaussian. 3 2. stronger prove 2, noted property GDA. correct, better regression. Specifically, indeed asymptotically efficient. strong youre baking information algorithm, Informally, limit large sets m, there no strictly terms of, accurately particular, generally, small sizes, generally expect better. flipside turn incorrect, xy might inherently lead doing poorly. cases, choice. still bunch quadratic eventually nonlinear separated circular following assumptions, xy0Poisson0xy1Poisson1yBernoulli Poisson ones above. why Further, many taking apply only natural family know distribution, serve case. But nonGaussian results less predictable, well. contrast robust sensitive incorrect significantly weaker assumptions. Compared classification, simpler computationally efficient implement cases. sets, requires correct least approximately correct. dataset, allow telling flip deviations accidentally not. nature GaussianPoissonsome member family, fine scenario Poisson, datasets, reason, often practice. Practical Advice Right Algorithm How use? sources knowledge pass todays era trend whatever wants problems available, could overcome motivated computation performance. reason cases ton time computational resources run iteratively, option fact much stronger, affect performance negatively agree those causing hit accuracy. question weak general philosophical point. principle machine skill days getting millions typically There lots applications hundred designing matters more. ImageNet, million images. dozens teams great results. Owing relatively minimal. highskilled low skilled teams, gap smaller giant datasets. working limited intuitions fuel experienced drive differences system really especially bake bigger gains team to. Appendix Do Figure Out Your Data Gaussian? Most universe matter degree. Particularly, errornoise distributions. several noise too correlated, statistics. offers insight fully vaguely depends amount Other Some related considerations Naive considered effective popular Further Reading Here optional links further reading Michael I. Jordan 2nd ed. McCullagh Nelder served major inspiration discussion topic. Citation found useful, please cite articleChadha2020DistilledGaussianDiscriminantAnalysis, title Analysis, author Chadha, Aman, journal Distilled Notes Stanford Machine Learning, year 2020, url'"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=8000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.create_documents([cleaned_content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field, root_validator\n",
    "from pydantic import BaseModel, Field, root_validator, ValidationError\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gv/v0q4rb4n3p9_ydb839lgywph0000gq/T/ipykernel_8980/3567739227.py:6: PydanticDeprecatedSince20: Pydantic V1 style `@root_validator` validators are deprecated. You should migrate to Pydantic V2 style `@model_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  @root_validator(pre=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define SentenceParser model\n",
    "class SentenceParser(BaseModel):\n",
    "    originalContent: str = Field(..., description=\"Original Sentence with no correction\")\n",
    "    correctedSentence: str = Field(..., description=\"Corrected Sentence in English UK. Enclosed corrections in <b></b>.\")\n",
    "\n",
    "    @root_validator(pre=True)\n",
    "    def fill_missing_fields(cls, values):\n",
    "        values['correctedSentence'] = values.get('correctedSentence', '')\n",
    "        return values\n",
    "\n",
    "# Define ResponseModel to include sentences\n",
    "class ResponseModel(BaseModel):\n",
    "    sentences: List[SentenceParser]\n",
    "\n",
    "# Function to get the markdown content\n",
    "def get_markdown_content(page_content):\n",
    "    llm = ChatGroq(\n",
    "            model=\"llama-3.1-70b-versatile\",\n",
    "            temperature=0,\n",
    "            max_tokens=None,\n",
    "            timeout=None,\n",
    "            max_retries=3,\n",
    "            # other params...\n",
    "        )\n",
    "    \n",
    "\n",
    "    parser = PydanticOutputParser(pydantic_object=ResponseModel)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are a smart assistant.\n",
    "        You will be given a page-long document.\n",
    "        You will return a list of sentences in the page.\n",
    "        If there is any HTML content or non-textual content, you will remove it.\n",
    "        First, you will clean the text by removing the images and links, and extract plain text.\n",
    "        You will not expand any point or enhance anything. \n",
    "        If sentence contain the social media or web development related things like cookies etc you will remove it.\n",
    "        Just return what is in the original text.\n",
    "        Provide the English UK version of sentences, performing both spelling and grammar checks.\n",
    "        Return the result as a JSON object with a single key \"sentences\" which contains a list of objects.\n",
    "        Each object should have keys \"originalContent\" and \"correctedSentence\".\n",
    "\n",
    "        \\n\\n{query}\\n\n",
    "        \"\"\",\n",
    "        input_variables=[\"query\"]\n",
    "    )\n",
    "\n",
    "    chain = LLMChain(llm=llm, prompt=prompt, output_parser=parser)\n",
    "\n",
    "    result = chain({\"query\": page_content})\n",
    "\n",
    "    if isinstance(result,dict):\n",
    "        result = result['text']\n",
    "        return result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_random = texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Back to Top CS229 Gaussian Discriminant Analysis Generative Learning Algorithms Weve mainly looked at learning algorithms that model Pyx, the conditional distribution of y given x. For instance, logistic regression modeled Pyx as hxgTx where g is sigmoid function. In this section, well talk about a different type algorithm. Lets use binary classification problem motivation behind our discussion. Consider in which we want learn distinguish between malignant y1 and benign y0 tumors. Given training set, an algorithm like , or initially starts with randomly initialized parameters. Over course learning, performs gradient descent straight line hyperplane decision boundary evolves until you obtain separates positivenegative examples case, Then, classify new sample either benign, it checks on side falls in, makes its prediction accordingly. Theres class arent trying maximize likelihood looking both classes searching for separation boundary. Instead, these look one time. First, tumors, can features what tumors like. build separate Finally, tumor, match against tumor model, see whether looks more had seen set. try directly such mapping from input space X labels are called discriminative algorithms. instead Pxy Py. These generative if indicates example 0 1, then Pxy0 models features, while Pxy1 features. The also learns prior Py independent probability y. To illustrate concept using practical when patient walks into hospital, before doctors even them, odds their versus referred prior. Thus, builds each isolation. At test time, evaluates models, identifies matches most closely returns prediction. After modeling priors Pxy, Bayes rule derive posterior x PyxPxyPyPx Here, denominator by PxPxy1Py1Pxy0Py0, function quantities Note learned part process calculating order make prediction, dont actually need calculate value Px since constant, doesnt appear there. When making predictions algorithms, thus ignore computing save computation. However, end goal value, would compute be able normalize numerator. PyxPxyPyPxPxyPy above equation represents underlying framework . Key takeaways Discriminative i.e., output input. other words, hx0,1 directly. class, tumoridentification setting, may case them first discriminant analysis GDA, used continuousvalued say, classification. assume distributed according multivariate normal distribution. briefly properties distributions moving GDA itself. Multivariate Distribution generalization 1dimensional random variable ndimensional simply, nrandom variable. rather than univariate variable, seeks multiple variables. Assume Gaussian, i.e, XRn, parameterized mean vector Rn covariance matrix Rnn, symmetric positive semidefinite. Formally, written as, XN, density PDF PX,12n212exp12XT1X denotes determinant expected ExxPx,dx vectorvalued defined CovX EXET. Covariance generalizes notion variance realvalued setting. EEET. Since CovX. explore some visualize Recall familiar Similarly, multivariable represented same bellshaped curve two parameters control PDF, but ndimensions. e.g., 2dimensional over 2 variables, 2dimensional, size 22. below figure shows zero 21 zerovector I 22 identity matrix. A standard 0.6I. essentially taken multiplied number has shrunk variance, reduced variability 2I. From images, becomes larger, widershorter, smaller, compressedtaller. This because always integrates area under so scales spread vs. height figures show Gaussians corresponding matrices figures, increase offdiagonal entries compressed towards 45 x1x2. Geometrically speaking, implies variables positively correlated. We clearly contours three densities 3D bumps saw above, should perfectly round circles, aspect ratio image probably little bit fatter places being Heres another set along decreasing elements matrix, now again, opposite direction, 135 line. Again, geometrically, endow negative correlation. vary parameters, tend form ellipses. By varying shift center around. Another values I, visualization PDFs follows carry out eigenvectors points principal axes ellipse contours. takeaway As density, change spreadheight respectively. Model have consider task discussion, expressed equations Pxy1, Gaussians. On hand, Bernoulli takes 0,1. by, xy0N0,xy1N1,yBernoulli 0, classes, vectors 1. Put were assuming representing means. You not commonly seen. More section below. fit your data, will define data. R, 0Rn, 1Rn, Rnn. Writing distributions, Pyy11yPxy012n212exp12x0T1x0Pxy112n212exp12x1T1x1 exponential notation similar earlier Plugging Pxy0,Pxy1,Py0 Py1 formula easily ascertain particular example. Suppose xi,yimi1. aforementioned going joint L, L,0,1,mi1Pxi,yi,0,1,mi1Pxiyi0,1,Pyi big difference cost functions compared choose ,0,1,, Px,y,0,1,. linear regression, generalized Pyx. loglikelihood data simply log L. ,0,1,logmi1Pxi,yi,0,1,logmi1Pxiyi0,1,Pyi respect take derivative equal solve expression yields maximum estimate be, mi1yim An intuitive explanation around maximizes follows. example, chance next office denoted bias coin toss fraction heads Likewise, just label y1. way write indicator notation, mi11yi1m 1 argument true, otherwise 0. true statement false equivalent ifstatement programming context. 0,1 is, 0mi11yi0ximi11yi01mi11yi1ximi11yi1 intuition expression, think all say dataset. reasonable examples, 0s average writing intuition. numerator sum feature sums entire summing i1m, uses instances yi0 times xi. effect term whereas effectively zeroing term. up samples yi0. count represent Because every yi0, get extra sum, ends total examples. made 1mmi1xiyixiyiT fits means classes. Making Predictions Using minz min z minimum any possible z. argminz argmin refers plugged expression. minzz52 attained z520 argminzz52 led z52, z5. maxz argmaxz operators exist they deal leads having lets how go So do benign? boils down predicting likely x, formally argmaxyPyx Looking deeper y0,1 rule, argmaxyPyxargmaxyPxyPyPx reasons highlighted Visualization dataset compareandcontrast operate Pictorially, GDAs operation Shown been shape orientation, share point, determine put together, imply blue Py1x0.5. boundary, predict outcome, side, y0. Points upper right closer classifying lower left classified Logistic Regression x1 x2 start whose randomly. Typically, initialize purpose visualization, starting off shown Running single iteration PYX, repositions 20 iterations, converges illustrates green superimposed blue. arrive slightly boundaries. Why Two Separate Means, Single Matrix? If lot problems. Choosing reasonable, work fine. catch, though, youll roughly double isnt anymore. Discussion Comparing interesting relationship. youve ,0,1,. fixed Py1x,0,1,. theorem, Pxy1,0,1,Pxy1,0,1,Py1Px,0,1, where, Pxy1,0,1, measure evaluating once determined optimum probability, plot Py1x so, simple few Now, Mappingprojecting Xaxis, bump PXY0 PXY1. Were Also, note split 5050 across PY1 0.5, known half Next, PY1X X. unlabeled far Xaxis. infer point almost certainly came left, generating Xaxis datapoint very small. pick Its easy discern PXY10.5. right, youd pretty sure class. repeat exercise sweeping datapoints dense grid evaluate measure, Youll notice coming approach midpoint, increases 0.5 surpasses 0.5. Beyond certain tends Plotting connect dots, turns exactly view quantity Py1x,0,1, find form, Py1x,,0,111expTx appropriate ,,0,1. convention redefining xis righthandside n1 dimensional adding coordinate xi01. 1x. While hypothesis function, hood specific choice choosing quite leading Would Prefer One Another? general, yield boundaries trained Which better? discuss superior viceversa. assumes that, xy0 xy1 parameter logistic, governed Py1x11expTx element x0, raised plotting pointbypoint ultimately yielded curve, shared necessarily assumptions 3. converse, implication does Gaussian. 3 2. stronger prove 2, noted property GDA. correct, better regression. Specifically, indeed asymptotically efficient. strong youre baking information algorithm, Informally, limit large sets m, there no strictly terms of, accurately')"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkd_contents = get_markdown_content(text_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gv/v0q4rb4n3p9_ydb839lgywph0000gq/T/ipykernel_8980/985300905.py:1: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  out_sentences = mkd_contents.dict().get('sentences')\n"
     ]
    }
   ],
   "source": [
    "out_sentences = mkd_contents.dict().get('sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'originalContent': 'Back to Top CS229 Gaussian Discriminant Analysis Generative Learning Algorithms Weve mainly looked at learning algorithms that model Pyx, the conditional distribution of y given x.',\n",
       "  'correctedSentence': \"Back to Top CS229 Gaussian Discriminant Analysis Generative Learning Algorithms We've mainly looked at learning algorithms that model Pyx, the conditional distribution of y given x.\"},\n",
       " {'originalContent': 'For instance, logistic regression modeled Pyx as hxgTx where g is sigmoid function.',\n",
       "  'correctedSentence': 'For instance, logistic regression modelled Pyx as hxgTx where g is sigmoid function.'},\n",
       " {'originalContent': 'In this section, well talk about a different type algorithm.',\n",
       "  'correctedSentence': \"In this section, we'll talk about a different type of algorithm.\"},\n",
       " {'originalContent': 'Lets use binary classification problem motivation behind our discussion.',\n",
       "  'correctedSentence': \"Let's use binary classification problem motivation behind our discussion.\"},\n",
       " {'originalContent': 'Consider in which we want learn distinguish between malignant y1 and benign y0 tumors.',\n",
       "  'correctedSentence': 'Consider in which we want to learn to distinguish between malignant y1 and benign y0 tumours.'},\n",
       " {'originalContent': 'Given training set, an algorithm like , or initially starts with randomly initialized parameters.',\n",
       "  'correctedSentence': 'Given a training set, an algorithm like , or initially starts with randomly initialised parameters.'},\n",
       " {'originalContent': 'Over course learning, performs gradient descent straight line hyperplane decision boundary evolves until you obtain separates positivenegative examples case, Then, classify new sample either benign, it checks on side falls in, makes its prediction accordingly.',\n",
       "  'correctedSentence': 'Over the course of learning, it performs gradient descent on a straight line hyperplane decision boundary, which evolves until it separates positive and negative examples. Then, it classifies a new sample as either benign or malignant, depending on which side it falls in, and makes its prediction accordingly.'},\n",
       " {'originalContent': 'Theres class arent trying maximize likelihood looking both classes searching for separation boundary.',\n",
       "  'correctedSentence': \"There's a class that aren't trying to maximise likelihood, looking at both classes and searching for a separation boundary.\"},\n",
       " {'originalContent': 'Instead, these look one time.',\n",
       "  'correctedSentence': 'Instead, these look at one time.'},\n",
       " {'originalContent': 'First, tumors, can features what tumors like.',\n",
       "  'correctedSentence': 'First, tumours can have features that are like other tumours.'},\n",
       " {'originalContent': 'build separate Finally, tumor, match against tumor model, see whether looks more had seen set.',\n",
       "  'correctedSentence': 'Build separate models for each tumour type. Finally, match the tumour against the tumour model, and see whether it looks more like the tumours in the training set.'},\n",
       " {'originalContent': 'try directly such mapping from input space X labels are called discriminative algorithms.',\n",
       "  'correctedSentence': 'Try to directly map from the input space X to the labels, which are called discriminative algorithms.'},\n",
       " {'originalContent': 'instead Pxy Py.',\n",
       "  'correctedSentence': 'Instead, Pxy = Py.'},\n",
       " {'originalContent': 'These generative if indicates example 0 1, then Pxy0 models features, while Pxy1 features.',\n",
       "  'correctedSentence': 'These generative models indicate that, for example, Pxy0 models the features of class 0, while Pxy1 models the features of class 1.'},\n",
       " {'originalContent': 'The also learns prior Py independent probability y.',\n",
       "  'correctedSentence': 'The model also learns the prior probability Py, which is independent of y.'},\n",
       " {'originalContent': 'To illustrate concept using practical when patient walks into hospital, before doctors even them, odds their versus referred prior.',\n",
       "  'correctedSentence': 'To illustrate this concept, consider a practical example: when a patient walks into a hospital, before the doctors even see them, the odds of their having a particular disease versus not having it are referred to as the prior probability.'},\n",
       " {'originalContent': 'Thus, builds each isolation.',\n",
       "  'correctedSentence': 'Thus, the model builds each in isolation.'},\n",
       " {'originalContent': 'At test time, evaluates models, identifies matches most closely returns prediction.',\n",
       "  'correctedSentence': 'At test time, the model evaluates the models, identifies which one matches most closely, and returns the prediction.'},\n",
       " {'originalContent': 'After modeling priors Pxy, Bayes rule derive posterior x PyxPxyPyPx',\n",
       "  'correctedSentence': \"After modelling the priors Pxy, we can derive the posterior probability x using Bayes' rule: Pyx = PxyPyPx\"},\n",
       " {'originalContent': 'Here, denominator by PxPxy1Py1Pxy0Py0, function quantities',\n",
       "  'correctedSentence': 'Here, the denominator is given by Px = Pxy1Py1 + Pxy0Py0, which is a function of the quantities'},\n",
       " {'originalContent': 'Note learned part process calculating order make prediction, dont actually need calculate value Px since constant, doesnt appear there.',\n",
       "  'correctedSentence': \"Note that the learned part of the process is calculating the order to make a prediction, but we don't actually need to calculate the value of Px since it is a constant and doesn't appear in the final prediction.\"},\n",
       " {'originalContent': 'When making predictions algorithms, thus ignore computing save computation.',\n",
       "  'correctedSentence': 'When making predictions, algorithms thus ignore computing Px to save computation.'},\n",
       " {'originalContent': 'However, end goal value, would compute be able normalize numerator.',\n",
       "  'correctedSentence': 'However, the end goal is to compute the value of Px, which would allow us to normalise the numerator.'},\n",
       " {'originalContent': 'PyxPxyPyPxPxyPy above equation represents underlying framework .',\n",
       "  'correctedSentence': 'The equation Pyx = PxyPyPx represents the underlying framework.'},\n",
       " {'originalContent': 'Key takeaways Discriminative i.e., output input.',\n",
       "  'correctedSentence': 'Key takeaways: discriminative models, i.e., output is a function of input.'},\n",
       " {'originalContent': 'other words, hx0,1 directly.',\n",
       "  'correctedSentence': 'In other words, hx is 0 or 1 directly.'},\n",
       " {'originalContent': 'class, tumoridentification setting, may case them first discriminant analysis GDA, used continuousvalued say, classification.',\n",
       "  'correctedSentence': 'In the case of tumour identification, we may use discriminant analysis (GDA) for continuous-valued classification.'},\n",
       " {'originalContent': 'assume distributed according multivariate normal distribution.',\n",
       "  'correctedSentence': 'We assume that the data is distributed according to a multivariate normal distribution.'},\n",
       " {'originalContent': 'briefly properties distributions moving GDA itself.',\n",
       "  'correctedSentence': 'We will briefly discuss the properties of the distributions and then move on to GDA itself.'},\n",
       " {'originalContent': 'Multivariate Distribution generalization 1dimensional random variable ndimensional simply, nrandom variable.',\n",
       "  'correctedSentence': 'A multivariate distribution is a generalisation of a 1-dimensional random variable to an n-dimensional random variable.'},\n",
       " {'originalContent': 'rather than univariate variable, seeks multiple variables.',\n",
       "  'correctedSentence': 'Rather than a univariate variable, it seeks multiple variables.'},\n",
       " {'originalContent': 'Assume Gaussian, i.e, XRn, parameterized mean vector Rn covariance matrix Rnn, symmetric positive semidefinite.',\n",
       "  'correctedSentence': 'We assume that the data is Gaussian, i.e., X ~ Rn, parameterised by a mean vector in Rn and a covariance matrix in Rnn, which is symmetric and positive semidefinite.'},\n",
       " {'originalContent': 'Formally, written as, XN, density PDF PX,12n212exp12XT1X denotes determinant expected ExxPx,dx vectorvalued defined CovX EXET.',\n",
       "  'correctedSentence': 'Formally, this can be written as X ~ N(μ, Σ), where the density PDF is given by PX(x) = (1/√(2π)^n) * exp(-1/2 * (x-μ)^T * Σ^(-1) * (x-μ)), where μ is the mean vector, Σ is the covariance matrix, and E[x] is the expected value of x.'},\n",
       " {'originalContent': 'Covariance generalizes notion variance realvalued setting.',\n",
       "  'correctedSentence': 'Covariance generalises the notion of variance in the real-valued setting.'},\n",
       " {'originalContent': 'EEET.', 'correctedSentence': 'E[E^T] = E[E]E^T.'},\n",
       " {'originalContent': 'Since CovX.',\n",
       "  'correctedSentence': 'Since Cov(X) = E[(X-E[X])(X-E[X])^T],'},\n",
       " {'originalContent': 'explore some visualize Recall familiar',\n",
       "  'correctedSentence': \"Let's explore some visualisations. Recall the familiar\"},\n",
       " {'originalContent': 'Similarly, multivariable represented same bellshaped curve two parameters control PDF, but ndimensions.',\n",
       "  'correctedSentence': 'Similarly, a multivariate distribution can be represented by the same bell-shaped curve, but with two parameters controlling the PDF in n dimensions.'},\n",
       " {'originalContent': 'e.g., 2dimensional over 2 variables, 2dimensional, size 22.',\n",
       "  'correctedSentence': 'For example, a 2-dimensional distribution over 2 variables has a size of 2x2.'},\n",
       " {'originalContent': 'below figure shows zero 21 zerovector I 22 identity matrix.',\n",
       "  'correctedSentence': 'The figure below shows a zero-mean vector, a 2x1 zero vector, and a 2x2 identity matrix.'},\n",
       " {'originalContent': 'A standard 0.6I.',\n",
       "  'correctedSentence': 'A standard deviation of 0.6I.'},\n",
       " {'originalContent': 'essentially taken multiplied number has shrunk variance, reduced variability 2I.',\n",
       "  'correctedSentence': 'Essentially, the number has been multiplied by 0.6, which has shrunk the variance and reduced the variability. 2I.'},\n",
       " {'originalContent': 'From images, becomes larger, widershorter, smaller, compressedtaller.',\n",
       "  'correctedSentence': 'From the images, we can see that as the covariance matrix becomes larger, the distribution becomes wider and shorter, and as it becomes smaller, the distribution becomes compressed and taller.'},\n",
       " {'originalContent': 'This because always integrates area under so scales spread vs. height figures show Gaussians corresponding matrices figures, increase offdiagonal entries compressed towards 45 x1x2.',\n",
       "  'correctedSentence': 'This is because the area under the curve always integrates to 1, so as the spread increases, the height decreases, and vice versa. The figures show Gaussians corresponding to the matrices, and as the off-diagonal entries increase, the distribution becomes compressed towards the 45° line between x1 and x2.'},\n",
       " {'originalContent': 'Geometrically speaking, implies variables positively correlated.',\n",
       "  'correctedSentence': 'Geometrically speaking, this implies that the variables are positively correlated.'},\n",
       " {'originalContent': 'We clearly contours three densities 3D bumps saw above, should perfectly round circles, aspect ratio image probably little bit fatter places being',\n",
       "  'correctedSentence': 'We can clearly see the contours of the three densities, which are 3D bumps. The aspect ratio of the image is probably a little bit fatter in some places, but the contours should be perfectly round circles.'},\n",
       " {'originalContent': 'Heres another set along decreasing elements matrix, now again, opposite direction, 135 line.',\n",
       "  'correctedSentence': \"Here's another set of images, this time with decreasing elements of the matrix, and again, in the opposite direction, along the 135° line.\"},\n",
       " {'originalContent': 'Again, geometrically, endow negative correlation.',\n",
       "  'correctedSentence': 'Again, geometrically, this endows the variables with a negative correlation.'},\n",
       " {'originalContent': 'vary parameters, tend form ellipses.',\n",
       "  'correctedSentence': 'As we vary the parameters, the distribution tends to form ellipses.'},\n",
       " {'originalContent': 'By varying shift center around.',\n",
       "  'correctedSentence': 'By varying the shift of the center around.'},\n",
       " {'originalContent': 'Another values I, visualization PDFs follows carry out eigenvectors points principal axes ellipse contours.',\n",
       "  'correctedSentence': 'Another way to visualise the PDFs is to carry out the eigenvectors and points of the principal axes of the ellipse contours.'},\n",
       " {'originalContent': 'takeaway As density, change spreadheight respectively.',\n",
       "  'correctedSentence': 'The takeaway is that as the density changes, the spread and height change respectively.'},\n",
       " {'originalContent': 'Model have consider task discussion, expressed equations Pxy1, Gaussians.',\n",
       "  'correctedSentence': 'The model we have been considering for this task is expressed in terms of equations for Pxy1, which are Gaussians.'},\n",
       " {'originalContent': 'On hand, Bernoulli takes 0,1.',\n",
       "  'correctedSentence': 'On the other hand, the Bernoulli distribution takes values 0 and 1.'},\n",
       " {'originalContent': 'by, xy0N0,xy1N1,yBernoulli 0, classes, vectors 1.',\n",
       "  'correctedSentence': 'By the way, xy0 ~ N0, xy1 ~ N1, and y ~ Bernoulli(0, 1), where the classes are represented by vectors of length 1.'},\n",
       " {'originalContent': 'Put were assuming representing means.',\n",
       "  'correctedSentence': 'Put simply, we were assuming that the means are represented by'},\n",
       " {'originalContent': 'You not commonly seen.',\n",
       "  'correctedSentence': 'You may not have commonly seen this.'},\n",
       " {'originalContent': 'More section below.',\n",
       "  'correctedSentence': 'More on this in the section below.'},\n",
       " {'originalContent': 'fit your data, will define data.',\n",
       "  'correctedSentence': 'To fit your data, we will define the data.'},\n",
       " {'originalContent': 'R, 0Rn, 1Rn, Rnn.',\n",
       "  'correctedSentence': 'R, 0 in Rn, 1 in Rn, and Rnn.'},\n",
       " {'originalContent': 'Writing distributions, Pyy11yPxy012n212exp12x0T1x0Pxy112n212exp12x1T1x1',\n",
       "  'correctedSentence': 'Writing the distributions, Py(y=1) = Pxy0 ~ N(0, 1), Py(y=1) = Pxy1 ~ N(1, 1)'},\n",
       " {'originalContent': 'exponential notation similar earlier Plugging Pxy0,Pxy1,Py0 Py1 formula easily ascertain particular example.',\n",
       "  'correctedSentence': 'Using exponential notation similar to earlier, plugging in Pxy0, Pxy1, Py0, and Py1 into the formula, we can easily ascertain the particular example.'},\n",
       " {'originalContent': 'Suppose xi,yimi1.',\n",
       "  'correctedSentence': 'Suppose we have xi and yi, for i = 1 to m.'},\n",
       " {'originalContent': 'aforementioned going joint L, L,0,1,mi1Pxi,yi,0,1,mi1Pxiyi0,1,Pyi',\n",
       "  'correctedSentence': 'The aforementioned joint likelihood L is given by L = ∏[P(xi, yi) for i = 1 to m] = ∏[P(xi|yi)P(yi) for i = 1 to m]'},\n",
       " {'originalContent': 'big difference cost functions compared choose ,0,1,, Px,y,0,1,.',\n",
       "  'correctedSentence': 'There is a big difference between the cost functions. We choose to maximise the likelihood of the data, which is given by L = ∏[P(xi, yi) for i = 1 to m].'},\n",
       " {'originalContent': 'linear regression, generalized Pyx.',\n",
       "  'correctedSentence': 'In linear regression, we generalise Pyx.'},\n",
       " {'originalContent': 'loglikelihood data simply log L.',\n",
       "  'correctedSentence': 'The log-likelihood of the data is simply log L.'},\n",
       " {'originalContent': ',0,1,logmi1Pxi,yi,0,1,logmi1Pxiyi0,1,Pyi',\n",
       "  'correctedSentence': 'The log-likelihood is given by log L = ∑[log P(xi, yi) for i = 1 to m] = ∑[log P(xi|yi)P(yi) for i = 1 to m]'},\n",
       " {'originalContent': 'respect take derivative equal solve expression yields maximum estimate be, mi1yim',\n",
       "  'correctedSentence': 'Taking the derivative with respect to the parameters and setting it equal to zero, we can solve for the expression that yields the maximum likelihood estimate, which is given by μ = (1/m) * ∑[yi * xi for i = 1 to m]'},\n",
       " {'originalContent': 'An intuitive explanation around maximizes follows.',\n",
       "  'correctedSentence': 'An intuitive explanation of how this maximises the likelihood follows.'},\n",
       " {'originalContent': 'example, chance next office denoted bias coin toss fraction heads Likewise, just label y1.',\n",
       "  'correctedSentence': 'For example, the chance of the next office being occupied is denoted by the bias of a coin toss, which is the fraction of heads. Likewise, we just label y as 1.'},\n",
       " {'originalContent': 'way write indicator notation, mi11yi1m 1 argument true, otherwise 0.',\n",
       "  'correctedSentence': 'We can write this in indicator notation as 1(yi = 1) = 1 if the argument is true, and 0 otherwise.'},\n",
       " {'originalContent': 'true statement false equivalent ifstatement programming context.',\n",
       "  'correctedSentence': 'This is equivalent to a true statement being false in an if-statement in a programming context.'},\n",
       " {'originalContent': '0,1 is, 0mi11yi0ximi11yi01mi11yi1',\n",
       "  'correctedSentence': '0 or 1 is given by 0 if yi = 0, and 1 if yi = 1.'},\n",
       " {'originalContent': 'intuition expression, think all say dataset.',\n",
       "  'correctedSentence': 'The intuition behind this expression is to think about all the data points in the dataset.'},\n",
       " {'originalContent': 'reasonable examples, 0s average writing intuition.',\n",
       "  'correctedSentence': 'A reasonable example is to take the average of the 0s and 1s in the dataset.'},\n",
       " {'originalContent': 'numerator sum feature sums entire summing i1m, uses instances yi0 times xi.',\n",
       "  'correctedSentence': 'The numerator is the sum of the feature sums over the entire dataset, summing over all instances where yi = 0, and using the instances where yi = 1 times xi.'},\n",
       " {'originalContent': 'effect term whereas effectively zeroing term.',\n",
       "  'correctedSentence': 'This has the effect of zeroing out the term.'},\n",
       " {'originalContent': 'up samples yi0.',\n",
       "  'correctedSentence': 'This is because we are summing over all samples where yi = 0.'},\n",
       " {'originalContent': 'count represent Because every yi0, get extra sum, ends total examples.',\n",
       "  'correctedSentence': 'This is because every time yi = 0, we get an extra sum, which ends up being the total number of examples.'},\n",
       " {'originalContent': 'made 1mmi1xiyixiT fits means classes.',\n",
       "  'correctedSentence': 'This is made up of 1/m times the sum of xi times yi, which fits the means of the classes.'},\n",
       " {'originalContent': 'Making Predictions Using minz min z minimum any possible z.',\n",
       "  'correctedSentence': 'Making predictions using the minimum of z, where z is any possible value.'},\n",
       " {'originalContent': 'argminz argmin refers plugged expression.',\n",
       "  'correctedSentence': 'The argmin of z refers to the value of z that minimises the expression.'},\n",
       " {'originalContent': 'minzz52 attained z520 argminzz52 led z52, z5.',\n",
       "  'correctedSentence': 'The minimum of z is attained when z = 5, and the argmin of z is 5.'},\n",
       " {'originalContent': 'maxz argmaxz operators exist they deal leads having lets how go',\n",
       "  'correctedSentence': \"The max of z and the argmax of z are operators that exist, and they deal with the maximum value of z. Let's see how to go about finding the maximum value of z.\"},\n",
       " {'originalContent': 'So do benign?',\n",
       "  'correctedSentence': 'So, do we predict benign?'},\n",
       " {'originalContent': 'boils down predicting likely x, formally argmaxyPyx',\n",
       "  'correctedSentence': 'This boils down to predicting the most likely value of x, which is given by the argmax of Pyx.'},\n",
       " {'originalContent': 'Looking deeper y0,1 rule, argmaxyPyxargmaxyPxyPyPx',\n",
       "  'correctedSentence': 'Looking deeper, we can see that the rule for predicting y is given by the argmax of Pyx, which is equal to the argmax of PxyPyPx.'},\n",
       " {'originalContent': 'reasons highlighted Visualization dataset compareandcontrast operate',\n",
       "  'correctedSentence': 'The reasons for this are highlighted in the visualisation of the dataset, which shows how the model operates.'},\n",
       " {'originalContent': 'Pictorially, GDAs operation Shown been shape orientation, share point, determine put together, imply blue Py1x0.5.',\n",
       "  'correctedSentence': 'Pictorially, the operation of GDA is shown to be a shape and orientation that share a point, which determines the decision boundary. This implies that the blue line is the decision boundary, and Py1x = 0.5.'},\n",
       " {'originalContent': 'boundary, predict outcome, side, y0.',\n",
       "  'correctedSentence': 'The boundary is used to predict the outcome, and the side of the boundary determines the value of y.'},\n",
       " {'originalContent': 'Points upper right closer classifying lower left classified',\n",
       "  'correctedSentence': 'Points in the upper right are closer to being classified as y = 1, while points in the lower left are classified as y = 0.'},\n",
       " {'originalContent': 'Logistic Regression x1 x2 start whose randomly.',\n",
       "  'correctedSentence': 'Logistic regression starts with x1 and x2, whose values are randomly initialised.'},\n",
       " {'originalContent': 'Typically, initialize purpose visualization, starting off shown',\n",
       "  'correctedSentence': 'Typically, we initialise the values for the purpose of visualisation, starting off with the values shown.'},\n",
       " {'originalContent': 'Running single iteration PYX, repositions 20 iterations, converges illustrates green superimposed blue.',\n",
       "  'correctedSentence': 'Running a single iteration of the algorithm, we reposition the decision boundary, and after 20 iterations, the algorithm converges, illustrating the green line superimposed on the blue line.'},\n",
       " {'originalContent': 'arrive slightly boundaries.',\n",
       "  'correctedSentence': 'We arrive at the decision boundary, which is slightly different from the original boundary.'},\n",
       " {'originalContent': 'Why Two Separate Means, Single Matrix?',\n",
       "  'correctedSentence': 'Why do we have two separate means and a single matrix?'},\n",
       " {'originalContent': 'If lot problems.',\n",
       "  'correctedSentence': 'If we have a lot of problems.'},\n",
       " {'originalContent': 'Choosing reasonable, work fine.',\n",
       "  'correctedSentence': 'Choosing a reasonable value will work fine.'},\n",
       " {'originalContent': 'catch, though, youll roughly double isnt anymore.',\n",
       "  'correctedSentence': \"The catch, though, is that you'll roughly double the number of parameters, and it isn't anymore.\"},\n",
       " {'originalContent': 'Discussion Comparing interesting relationship.',\n",
       "  'correctedSentence': 'The discussion compares the interesting relationship between'},\n",
       " {'originalContent': 'youve ,0,1,.',\n",
       "  'correctedSentence': \"You've seen the relationship between 0 and 1.\"},\n",
       " {'originalContent': 'fixed Py1x,0,1,.',\n",
       "  'correctedSentence': 'The fixed value of Py1x is 0 or 1.'},\n",
       " {'originalContent': 'theorem, Pxy1,0,1,Pxy1,0,1,Py1Px,0,1,',\n",
       "  'correctedSentence': 'By the theorem, Pxy1 = Pxy1,0,1,Py1Px,0,1,'},\n",
       " {'originalContent': 'where, Pxy1,0,1, measure evaluating once determined optimum probability, plot Py1x so, simple few',\n",
       "  'correctedSentence': 'where Pxy1 is the measure of evaluating the optimum probability, and we plot Py1x, which is a simple function of a few variables.'},\n",
       " {'originalContent': 'Now, Mappingprojecting Xaxis, bump PXY0 PXY1.',\n",
       "  'correctedSentence': 'Now, we map the X-axis to the bump of PXY0 and PXY1.'},\n",
       " {'originalContent': 'Were Also, note split 5050 across PY1 0.5, known half',\n",
       "  'correctedSentence': 'We also note that the split is 50-50 across Py1 = 0.5, which is known as the half-way point.'},\n",
       " {'originalContent': 'Next, PY1X X.',\n",
       "  'correctedSentence': 'Next, we have Py1x = X.'},\n",
       " {'originalContent': 'unlabeled far Xaxis.',\n",
       "  'correctedSentence': 'The unlabeled points are far away from the X-axis.'},\n",
       " {'originalContent': 'infer point almost certainly came left, generating Xaxis datapoint very small.',\n",
       "  'correctedSentence': 'We can infer that the point almost certainly came from the left, generating a datapoint very close to the X-axis.'},\n",
       " {'originalContent': 'pick Its easy discern PXY10.5.',\n",
       "  'correctedSentence': 'We can easily discern that PXY1 = 0.5.'},\n",
       " {'originalContent': 'right, youd pretty sure class.',\n",
       "  'correctedSentence': \"On the right, you'd be pretty sure of the class.\"},\n",
       " {'originalContent': 'repeat exercise sweeping datapoints dense grid evaluate measure, Youll notice coming approach midpoint, increases 0.5 surpasses 0.5.',\n",
       "  'correctedSentence': \"If we repeat the exercise, sweeping the datapoints across a dense grid and evaluating the measure, you'll notice that as we approach the midpoint, the value increases to 0.5 and surpasses 0.5.\"},\n",
       " {'originalContent': 'Beyond certain tends Plotting connect dots, turns exactly view quantity Py1x,0,1,',\n",
       "  'correctedSentence': 'Beyond a certain point, plotting the connect-the-dots turns exactly into the view of the quantity Py1x, which is 0 or 1.'},\n",
       " {'originalContent': 'find form, Py1x,,0,111expTx appropriate ,,0,1.',\n",
       "  'correctedSentence': 'We find that the form of Py1x is given by the expression 1 / (1 + exp(-Tx)), which is appropriate for 0 or 1.'},\n",
       " {'originalContent': 'Py1x,,0,1.', 'correctedSentence': 'Py1x is 0 or 1.'},\n",
       " {'originalContent': 'convention redefining xis righthandside n1 dimensional adding coordinate xi01.',\n",
       "  'correctedSentence': 'By convention, we redefine the xis to be on the right-hand side, adding a coordinate xi0 = 1.'},\n",
       " {'originalContent': '1x.', 'correctedSentence': 'This is 1 times x.'},\n",
       " {'originalContent': 'While hypothesis function, hood specific choice choosing quite leading',\n",
       "  'correctedSentence': 'While the hypothesis function is a specific choice, choosing it is quite leading.'},\n",
       " {'originalContent': 'Would Prefer One Another?',\n",
       "  'correctedSentence': 'Would you prefer one or the other?'},\n",
       " {'originalContent': 'general, yield boundaries trained',\n",
       "  'correctedSentence': 'In general, the boundaries yielded by the trained models'},\n",
       " {'originalContent': 'Which better?',\n",
       "  'correctedSentence': 'Which one is better?'},\n",
       " {'originalContent': 'discuss superior viceversa.',\n",
       "  'correctedSentence': 'We discuss which one is superior, and vice versa.'},\n",
       " {'originalContent': 'assumes that, xy0 xy1 parameter logistic, governed Py1x11expTx',\n",
       "  'correctedSentence': 'It assumes that xy0 and xy1 are parameters of the logistic function, governed by Py1x = 1 / (1 + exp(-Tx)).'},\n",
       " {'originalContent': 'element x0, raised plotting pointbypoint ultimately yielded curve, shared necessarily assumptions',\n",
       "  'correctedSentence': 'The element x0 is raised to the power of the plotting point-by-point, ultimately yielding a curve that shares the necessary assumptions.'},\n",
       " {'originalContent': '3. converse, implication does Gaussian.',\n",
       "  'correctedSentence': 'The converse is also true, and the implication is that the Gaussian distribution'},\n",
       " {'originalContent': '3 2. stronger prove 2, noted property GDA.',\n",
       "  'correctedSentence': '3 implies 2, and this is a stronger result. We prove 2, and note that this is a property of GDA.'},\n",
       " {'originalContent': 'correct, better regression.',\n",
       "  'correctedSentence': 'This is correct, and better than regression.'},\n",
       " {'originalContent': 'Specifically, indeed asymptotically efficient.',\n",
       "  'correctedSentence': 'Specifically, it is indeed asymptotically efficient.'},\n",
       " {'originalContent': 'strong youre baking information algorithm, Informally, limit large sets m, there no strictly terms of, accurately',\n",
       "  'correctedSentence': \"This is a strong result, and you're baking information into the algorithm. Informally, in the limit of large sets m, there is no strictly better algorithm in terms of accuracy.\"}]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import ndiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = out_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'originalContent': 'Back to Top CS229 Gaussian Discriminant Analysis Generative Learning Algorithms Weve mainly looked at learning algorithms that model Pyx, the conditional distribution of y given x.',\n",
       "  'correctedSentence': \"Back to Top CS229 Gaussian Discriminant Analysis Generative Learning Algorithms We've mainly looked at learning algorithms that model Pyx, the conditional distribution of y given x.\"},\n",
       " {'originalContent': 'For instance, logistic regression modeled Pyx as hxgTx where g is sigmoid function.',\n",
       "  'correctedSentence': 'For instance, logistic regression modelled Pyx as hxgTx where g is sigmoid function.'},\n",
       " {'originalContent': 'In this section, well talk about a different type algorithm.',\n",
       "  'correctedSentence': \"In this section, we'll talk about a different type of algorithm.\"},\n",
       " {'originalContent': 'Lets use binary classification problem motivation behind our discussion.',\n",
       "  'correctedSentence': \"Let's use binary classification problem motivation behind our discussion.\"},\n",
       " {'originalContent': 'Consider in which we want learn distinguish between malignant y1 and benign y0 tumors.',\n",
       "  'correctedSentence': 'Consider in which we want to learn to distinguish between malignant y1 and benign y0 tumours.'},\n",
       " {'originalContent': 'Given training set, an algorithm like , or initially starts with randomly initialized parameters.',\n",
       "  'correctedSentence': 'Given a training set, an algorithm like , or initially starts with randomly initialised parameters.'},\n",
       " {'originalContent': 'Over course learning, performs gradient descent straight line hyperplane decision boundary evolves until you obtain separates positivenegative examples case, Then, classify new sample either benign, it checks on side falls in, makes its prediction accordingly.',\n",
       "  'correctedSentence': 'Over the course of learning, it performs gradient descent on a straight line hyperplane decision boundary, which evolves until it separates positive and negative examples. Then, it classifies a new sample as either benign or malignant, depending on which side it falls in, and makes its prediction accordingly.'},\n",
       " {'originalContent': 'Theres class arent trying maximize likelihood looking both classes searching for separation boundary.',\n",
       "  'correctedSentence': \"There's a class that aren't trying to maximise likelihood, looking at both classes and searching for a separation boundary.\"},\n",
       " {'originalContent': 'Instead, these look one time.',\n",
       "  'correctedSentence': 'Instead, these look at one time.'},\n",
       " {'originalContent': 'First, tumors, can features what tumors like.',\n",
       "  'correctedSentence': 'First, tumours can have features that are like other tumours.'},\n",
       " {'originalContent': 'build separate Finally, tumor, match against tumor model, see whether looks more had seen set.',\n",
       "  'correctedSentence': 'Build separate models for each tumour type. Finally, match the tumour against the tumour model, and see whether it looks more like the tumours in the training set.'},\n",
       " {'originalContent': 'try directly such mapping from input space X labels are called discriminative algorithms.',\n",
       "  'correctedSentence': 'Try to directly map from the input space X to the labels, which are called discriminative algorithms.'},\n",
       " {'originalContent': 'instead Pxy Py.',\n",
       "  'correctedSentence': 'Instead, Pxy = Py.'},\n",
       " {'originalContent': 'These generative if indicates example 0 1, then Pxy0 models features, while Pxy1 features.',\n",
       "  'correctedSentence': 'These generative models indicate that, for example, Pxy0 models the features of class 0, while Pxy1 models the features of class 1.'},\n",
       " {'originalContent': 'The also learns prior Py independent probability y.',\n",
       "  'correctedSentence': 'The model also learns the prior probability Py, which is independent of y.'},\n",
       " {'originalContent': 'To illustrate concept using practical when patient walks into hospital, before doctors even them, odds their versus referred prior.',\n",
       "  'correctedSentence': 'To illustrate this concept, consider a practical example: when a patient walks into a hospital, before the doctors even see them, the odds of their having a particular disease versus not having it are referred to as the prior probability.'},\n",
       " {'originalContent': 'Thus, builds each isolation.',\n",
       "  'correctedSentence': 'Thus, the model builds each in isolation.'},\n",
       " {'originalContent': 'At test time, evaluates models, identifies matches most closely returns prediction.',\n",
       "  'correctedSentence': 'At test time, the model evaluates the models, identifies which one matches most closely, and returns the prediction.'},\n",
       " {'originalContent': 'After modeling priors Pxy, Bayes rule derive posterior x PyxPxyPyPx',\n",
       "  'correctedSentence': \"After modelling the priors Pxy, we can derive the posterior probability x using Bayes' rule: Pyx = PxyPyPx\"},\n",
       " {'originalContent': 'Here, denominator by PxPxy1Py1Pxy0Py0, function quantities',\n",
       "  'correctedSentence': 'Here, the denominator is given by Px = Pxy1Py1 + Pxy0Py0, which is a function of the quantities'},\n",
       " {'originalContent': 'Note learned part process calculating order make prediction, dont actually need calculate value Px since constant, doesnt appear there.',\n",
       "  'correctedSentence': \"Note that the learned part of the process is calculating the order to make a prediction, but we don't actually need to calculate the value of Px since it is a constant and doesn't appear in the final prediction.\"},\n",
       " {'originalContent': 'When making predictions algorithms, thus ignore computing save computation.',\n",
       "  'correctedSentence': 'When making predictions, algorithms thus ignore computing Px to save computation.'},\n",
       " {'originalContent': 'However, end goal value, would compute be able normalize numerator.',\n",
       "  'correctedSentence': 'However, the end goal is to compute the value of Px, which would allow us to normalise the numerator.'},\n",
       " {'originalContent': 'PyxPxyPyPxPxyPy above equation represents underlying framework .',\n",
       "  'correctedSentence': 'The equation Pyx = PxyPyPx represents the underlying framework.'},\n",
       " {'originalContent': 'Key takeaways Discriminative i.e., output input.',\n",
       "  'correctedSentence': 'Key takeaways: discriminative models, i.e., output is a function of input.'},\n",
       " {'originalContent': 'other words, hx0,1 directly.',\n",
       "  'correctedSentence': 'In other words, hx is 0 or 1 directly.'},\n",
       " {'originalContent': 'class, tumoridentification setting, may case them first discriminant analysis GDA, used continuousvalued say, classification.',\n",
       "  'correctedSentence': 'In the case of tumour identification, we may use discriminant analysis (GDA) for continuous-valued classification.'},\n",
       " {'originalContent': 'assume distributed according multivariate normal distribution.',\n",
       "  'correctedSentence': 'We assume that the data is distributed according to a multivariate normal distribution.'},\n",
       " {'originalContent': 'briefly properties distributions moving GDA itself.',\n",
       "  'correctedSentence': 'We will briefly discuss the properties of the distributions and then move on to GDA itself.'},\n",
       " {'originalContent': 'Multivariate Distribution generalization 1dimensional random variable ndimensional simply, nrandom variable.',\n",
       "  'correctedSentence': 'A multivariate distribution is a generalisation of a 1-dimensional random variable to an n-dimensional random variable.'},\n",
       " {'originalContent': 'rather than univariate variable, seeks multiple variables.',\n",
       "  'correctedSentence': 'Rather than a univariate variable, it seeks multiple variables.'},\n",
       " {'originalContent': 'Assume Gaussian, i.e, XRn, parameterized mean vector Rn covariance matrix Rnn, symmetric positive semidefinite.',\n",
       "  'correctedSentence': 'We assume that the data is Gaussian, i.e., X ~ Rn, parameterised by a mean vector in Rn and a covariance matrix in Rnn, which is symmetric and positive semidefinite.'},\n",
       " {'originalContent': 'Formally, written as, XN, density PDF PX,12n212exp12XT1X denotes determinant expected ExxPx,dx vectorvalued defined CovX EXET.',\n",
       "  'correctedSentence': 'Formally, this can be written as X ~ N(μ, Σ), where the density PDF is given by PX(x) = (1/√(2π)^n) * exp(-1/2 * (x-μ)^T * Σ^(-1) * (x-μ)), where μ is the mean vector, Σ is the covariance matrix, and E[x] is the expected value of x.'},\n",
       " {'originalContent': 'Covariance generalizes notion variance realvalued setting.',\n",
       "  'correctedSentence': 'Covariance generalises the notion of variance in the real-valued setting.'},\n",
       " {'originalContent': 'EEET.', 'correctedSentence': 'E[E^T] = E[E]E^T.'},\n",
       " {'originalContent': 'Since CovX.',\n",
       "  'correctedSentence': 'Since Cov(X) = E[(X-E[X])(X-E[X])^T],'},\n",
       " {'originalContent': 'explore some visualize Recall familiar',\n",
       "  'correctedSentence': \"Let's explore some visualisations. Recall the familiar\"},\n",
       " {'originalContent': 'Similarly, multivariable represented same bellshaped curve two parameters control PDF, but ndimensions.',\n",
       "  'correctedSentence': 'Similarly, a multivariate distribution can be represented by the same bell-shaped curve, but with two parameters controlling the PDF in n dimensions.'},\n",
       " {'originalContent': 'e.g., 2dimensional over 2 variables, 2dimensional, size 22.',\n",
       "  'correctedSentence': 'For example, a 2-dimensional distribution over 2 variables has a size of 2x2.'},\n",
       " {'originalContent': 'below figure shows zero 21 zerovector I 22 identity matrix.',\n",
       "  'correctedSentence': 'The figure below shows a zero-mean vector, a 2x1 zero vector, and a 2x2 identity matrix.'},\n",
       " {'originalContent': 'A standard 0.6I.',\n",
       "  'correctedSentence': 'A standard deviation of 0.6I.'},\n",
       " {'originalContent': 'essentially taken multiplied number has shrunk variance, reduced variability 2I.',\n",
       "  'correctedSentence': 'Essentially, the number has been multiplied by 0.6, which has shrunk the variance and reduced the variability. 2I.'},\n",
       " {'originalContent': 'From images, becomes larger, widershorter, smaller, compressedtaller.',\n",
       "  'correctedSentence': 'From the images, we can see that as the covariance matrix becomes larger, the distribution becomes wider and shorter, and as it becomes smaller, the distribution becomes compressed and taller.'},\n",
       " {'originalContent': 'This because always integrates area under so scales spread vs. height figures show Gaussians corresponding matrices figures, increase offdiagonal entries compressed towards 45 x1x2.',\n",
       "  'correctedSentence': 'This is because the area under the curve always integrates to 1, so as the spread increases, the height decreases, and vice versa. The figures show Gaussians corresponding to the matrices, and as the off-diagonal entries increase, the distribution becomes compressed towards the 45° line between x1 and x2.'},\n",
       " {'originalContent': 'Geometrically speaking, implies variables positively correlated.',\n",
       "  'correctedSentence': 'Geometrically speaking, this implies that the variables are positively correlated.'},\n",
       " {'originalContent': 'We clearly contours three densities 3D bumps saw above, should perfectly round circles, aspect ratio image probably little bit fatter places being',\n",
       "  'correctedSentence': 'We can clearly see the contours of the three densities, which are 3D bumps. The aspect ratio of the image is probably a little bit fatter in some places, but the contours should be perfectly round circles.'},\n",
       " {'originalContent': 'Heres another set along decreasing elements matrix, now again, opposite direction, 135 line.',\n",
       "  'correctedSentence': \"Here's another set of images, this time with decreasing elements of the matrix, and again, in the opposite direction, along the 135° line.\"},\n",
       " {'originalContent': 'Again, geometrically, endow negative correlation.',\n",
       "  'correctedSentence': 'Again, geometrically, this endows the variables with a negative correlation.'},\n",
       " {'originalContent': 'vary parameters, tend form ellipses.',\n",
       "  'correctedSentence': 'As we vary the parameters, the distribution tends to form ellipses.'},\n",
       " {'originalContent': 'By varying shift center around.',\n",
       "  'correctedSentence': 'By varying the shift of the center around.'},\n",
       " {'originalContent': 'Another values I, visualization PDFs follows carry out eigenvectors points principal axes ellipse contours.',\n",
       "  'correctedSentence': 'Another way to visualise the PDFs is to carry out the eigenvectors and points of the principal axes of the ellipse contours.'},\n",
       " {'originalContent': 'takeaway As density, change spreadheight respectively.',\n",
       "  'correctedSentence': 'The takeaway is that as the density changes, the spread and height change respectively.'},\n",
       " {'originalContent': 'Model have consider task discussion, expressed equations Pxy1, Gaussians.',\n",
       "  'correctedSentence': 'The model we have been considering for this task is expressed in terms of equations for Pxy1, which are Gaussians.'},\n",
       " {'originalContent': 'On hand, Bernoulli takes 0,1.',\n",
       "  'correctedSentence': 'On the other hand, the Bernoulli distribution takes values 0 and 1.'},\n",
       " {'originalContent': 'by, xy0N0,xy1N1,yBernoulli 0, classes, vectors 1.',\n",
       "  'correctedSentence': 'By the way, xy0 ~ N0, xy1 ~ N1, and y ~ Bernoulli(0, 1), where the classes are represented by vectors of length 1.'},\n",
       " {'originalContent': 'Put were assuming representing means.',\n",
       "  'correctedSentence': 'Put simply, we were assuming that the means are represented by'},\n",
       " {'originalContent': 'You not commonly seen.',\n",
       "  'correctedSentence': 'You may not have commonly seen this.'},\n",
       " {'originalContent': 'More section below.',\n",
       "  'correctedSentence': 'More on this in the section below.'},\n",
       " {'originalContent': 'fit your data, will define data.',\n",
       "  'correctedSentence': 'To fit your data, we will define the data.'},\n",
       " {'originalContent': 'R, 0Rn, 1Rn, Rnn.',\n",
       "  'correctedSentence': 'R, 0 in Rn, 1 in Rn, and Rnn.'},\n",
       " {'originalContent': 'Writing distributions, Pyy11yPxy012n212exp12x0T1x0Pxy112n212exp12x1T1x1',\n",
       "  'correctedSentence': 'Writing the distributions, Py(y=1) = Pxy0 ~ N(0, 1), Py(y=1) = Pxy1 ~ N(1, 1)'},\n",
       " {'originalContent': 'exponential notation similar earlier Plugging Pxy0,Pxy1,Py0 Py1 formula easily ascertain particular example.',\n",
       "  'correctedSentence': 'Using exponential notation similar to earlier, plugging in Pxy0, Pxy1, Py0, and Py1 into the formula, we can easily ascertain the particular example.'},\n",
       " {'originalContent': 'Suppose xi,yimi1.',\n",
       "  'correctedSentence': 'Suppose we have xi and yi, for i = 1 to m.'},\n",
       " {'originalContent': 'aforementioned going joint L, L,0,1,mi1Pxi,yi,0,1,mi1Pxiyi0,1,Pyi',\n",
       "  'correctedSentence': 'The aforementioned joint likelihood L is given by L = ∏[P(xi, yi) for i = 1 to m] = ∏[P(xi|yi)P(yi) for i = 1 to m]'},\n",
       " {'originalContent': 'big difference cost functions compared choose ,0,1,, Px,y,0,1,.',\n",
       "  'correctedSentence': 'There is a big difference between the cost functions. We choose to maximise the likelihood of the data, which is given by L = ∏[P(xi, yi) for i = 1 to m].'},\n",
       " {'originalContent': 'linear regression, generalized Pyx.',\n",
       "  'correctedSentence': 'In linear regression, we generalise Pyx.'},\n",
       " {'originalContent': 'loglikelihood data simply log L.',\n",
       "  'correctedSentence': 'The log-likelihood of the data is simply log L.'},\n",
       " {'originalContent': ',0,1,logmi1Pxi,yi,0,1,logmi1Pxiyi0,1,Pyi',\n",
       "  'correctedSentence': 'The log-likelihood is given by log L = ∑[log P(xi, yi) for i = 1 to m] = ∑[log P(xi|yi)P(yi) for i = 1 to m]'},\n",
       " {'originalContent': 'respect take derivative equal solve expression yields maximum estimate be, mi1yim',\n",
       "  'correctedSentence': 'Taking the derivative with respect to the parameters and setting it equal to zero, we can solve for the expression that yields the maximum likelihood estimate, which is given by μ = (1/m) * ∑[yi * xi for i = 1 to m]'},\n",
       " {'originalContent': 'An intuitive explanation around maximizes follows.',\n",
       "  'correctedSentence': 'An intuitive explanation of how this maximises the likelihood follows.'},\n",
       " {'originalContent': 'example, chance next office denoted bias coin toss fraction heads Likewise, just label y1.',\n",
       "  'correctedSentence': 'For example, the chance of the next office being occupied is denoted by the bias of a coin toss, which is the fraction of heads. Likewise, we just label y as 1.'},\n",
       " {'originalContent': 'way write indicator notation, mi11yi1m 1 argument true, otherwise 0.',\n",
       "  'correctedSentence': 'We can write this in indicator notation as 1(yi = 1) = 1 if the argument is true, and 0 otherwise.'},\n",
       " {'originalContent': 'true statement false equivalent ifstatement programming context.',\n",
       "  'correctedSentence': 'This is equivalent to a true statement being false in an if-statement in a programming context.'},\n",
       " {'originalContent': '0,1 is, 0mi11yi0ximi11yi01mi11yi1',\n",
       "  'correctedSentence': '0 or 1 is given by 0 if yi = 0, and 1 if yi = 1.'},\n",
       " {'originalContent': 'intuition expression, think all say dataset.',\n",
       "  'correctedSentence': 'The intuition behind this expression is to think about all the data points in the dataset.'},\n",
       " {'originalContent': 'reasonable examples, 0s average writing intuition.',\n",
       "  'correctedSentence': 'A reasonable example is to take the average of the 0s and 1s in the dataset.'},\n",
       " {'originalContent': 'numerator sum feature sums entire summing i1m, uses instances yi0 times xi.',\n",
       "  'correctedSentence': 'The numerator is the sum of the feature sums over the entire dataset, summing over all instances where yi = 0, and using the instances where yi = 1 times xi.'},\n",
       " {'originalContent': 'effect term whereas effectively zeroing term.',\n",
       "  'correctedSentence': 'This has the effect of zeroing out the term.'},\n",
       " {'originalContent': 'up samples yi0.',\n",
       "  'correctedSentence': 'This is because we are summing over all samples where yi = 0.'},\n",
       " {'originalContent': 'count represent Because every yi0, get extra sum, ends total examples.',\n",
       "  'correctedSentence': 'This is because every time yi = 0, we get an extra sum, which ends up being the total number of examples.'},\n",
       " {'originalContent': 'made 1mmi1xiyixiT fits means classes.',\n",
       "  'correctedSentence': 'This is made up of 1/m times the sum of xi times yi, which fits the means of the classes.'},\n",
       " {'originalContent': 'Making Predictions Using minz min z minimum any possible z.',\n",
       "  'correctedSentence': 'Making predictions using the minimum of z, where z is any possible value.'},\n",
       " {'originalContent': 'argminz argmin refers plugged expression.',\n",
       "  'correctedSentence': 'The argmin of z refers to the value of z that minimises the expression.'},\n",
       " {'originalContent': 'minzz52 attained z520 argminzz52 led z52, z5.',\n",
       "  'correctedSentence': 'The minimum of z is attained when z = 5, and the argmin of z is 5.'},\n",
       " {'originalContent': 'maxz argmaxz operators exist they deal leads having lets how go',\n",
       "  'correctedSentence': \"The max of z and the argmax of z are operators that exist, and they deal with the maximum value of z. Let's see how to go about finding the maximum value of z.\"},\n",
       " {'originalContent': 'So do benign?',\n",
       "  'correctedSentence': 'So, do we predict benign?'},\n",
       " {'originalContent': 'boils down predicting likely x, formally argmaxyPyx',\n",
       "  'correctedSentence': 'This boils down to predicting the most likely value of x, which is given by the argmax of Pyx.'},\n",
       " {'originalContent': 'Looking deeper y0,1 rule, argmaxyPyxargmaxyPxyPyPx',\n",
       "  'correctedSentence': 'Looking deeper, we can see that the rule for predicting y is given by the argmax of Pyx, which is equal to the argmax of PxyPyPx.'},\n",
       " {'originalContent': 'reasons highlighted Visualization dataset compareandcontrast operate',\n",
       "  'correctedSentence': 'The reasons for this are highlighted in the visualisation of the dataset, which shows how the model operates.'},\n",
       " {'originalContent': 'Pictorially, GDAs operation Shown been shape orientation, share point, determine put together, imply blue Py1x0.5.',\n",
       "  'correctedSentence': 'Pictorially, the operation of GDA is shown to be a shape and orientation that share a point, which determines the decision boundary. This implies that the blue line is the decision boundary, and Py1x = 0.5.'},\n",
       " {'originalContent': 'boundary, predict outcome, side, y0.',\n",
       "  'correctedSentence': 'The boundary is used to predict the outcome, and the side of the boundary determines the value of y.'},\n",
       " {'originalContent': 'Points upper right closer classifying lower left classified',\n",
       "  'correctedSentence': 'Points in the upper right are closer to being classified as y = 1, while points in the lower left are classified as y = 0.'},\n",
       " {'originalContent': 'Logistic Regression x1 x2 start whose randomly.',\n",
       "  'correctedSentence': 'Logistic regression starts with x1 and x2, whose values are randomly initialised.'},\n",
       " {'originalContent': 'Typically, initialize purpose visualization, starting off shown',\n",
       "  'correctedSentence': 'Typically, we initialise the values for the purpose of visualisation, starting off with the values shown.'},\n",
       " {'originalContent': 'Running single iteration PYX, repositions 20 iterations, converges illustrates green superimposed blue.',\n",
       "  'correctedSentence': 'Running a single iteration of the algorithm, we reposition the decision boundary, and after 20 iterations, the algorithm converges, illustrating the green line superimposed on the blue line.'},\n",
       " {'originalContent': 'arrive slightly boundaries.',\n",
       "  'correctedSentence': 'We arrive at the decision boundary, which is slightly different from the original boundary.'},\n",
       " {'originalContent': 'Why Two Separate Means, Single Matrix?',\n",
       "  'correctedSentence': 'Why do we have two separate means and a single matrix?'},\n",
       " {'originalContent': 'If lot problems.',\n",
       "  'correctedSentence': 'If we have a lot of problems.'},\n",
       " {'originalContent': 'Choosing reasonable, work fine.',\n",
       "  'correctedSentence': 'Choosing a reasonable value will work fine.'},\n",
       " {'originalContent': 'catch, though, youll roughly double isnt anymore.',\n",
       "  'correctedSentence': \"The catch, though, is that you'll roughly double the number of parameters, and it isn't anymore.\"},\n",
       " {'originalContent': 'Discussion Comparing interesting relationship.',\n",
       "  'correctedSentence': 'The discussion compares the interesting relationship between'},\n",
       " {'originalContent': 'youve ,0,1,.',\n",
       "  'correctedSentence': \"You've seen the relationship between 0 and 1.\"},\n",
       " {'originalContent': 'fixed Py1x,0,1,.',\n",
       "  'correctedSentence': 'The fixed value of Py1x is 0 or 1.'},\n",
       " {'originalContent': 'theorem, Pxy1,0,1,Pxy1,0,1,Py1Px,0,1,',\n",
       "  'correctedSentence': 'By the theorem, Pxy1 = Pxy1,0,1,Py1Px,0,1,'},\n",
       " {'originalContent': 'where, Pxy1,0,1, measure evaluating once determined optimum probability, plot Py1x so, simple few',\n",
       "  'correctedSentence': 'where Pxy1 is the measure of evaluating the optimum probability, and we plot Py1x, which is a simple function of a few variables.'},\n",
       " {'originalContent': 'Now, Mappingprojecting Xaxis, bump PXY0 PXY1.',\n",
       "  'correctedSentence': 'Now, we map the X-axis to the bump of PXY0 and PXY1.'},\n",
       " {'originalContent': 'Were Also, note split 5050 across PY1 0.5, known half',\n",
       "  'correctedSentence': 'We also note that the split is 50-50 across Py1 = 0.5, which is known as the half-way point.'},\n",
       " {'originalContent': 'Next, PY1X X.',\n",
       "  'correctedSentence': 'Next, we have Py1x = X.'},\n",
       " {'originalContent': 'unlabeled far Xaxis.',\n",
       "  'correctedSentence': 'The unlabeled points are far away from the X-axis.'},\n",
       " {'originalContent': 'infer point almost certainly came left, generating Xaxis datapoint very small.',\n",
       "  'correctedSentence': 'We can infer that the point almost certainly came from the left, generating a datapoint very close to the X-axis.'},\n",
       " {'originalContent': 'pick Its easy discern PXY10.5.',\n",
       "  'correctedSentence': 'We can easily discern that PXY1 = 0.5.'},\n",
       " {'originalContent': 'right, youd pretty sure class.',\n",
       "  'correctedSentence': \"On the right, you'd be pretty sure of the class.\"},\n",
       " {'originalContent': 'repeat exercise sweeping datapoints dense grid evaluate measure, Youll notice coming approach midpoint, increases 0.5 surpasses 0.5.',\n",
       "  'correctedSentence': \"If we repeat the exercise, sweeping the datapoints across a dense grid and evaluating the measure, you'll notice that as we approach the midpoint, the value increases to 0.5 and surpasses 0.5.\"},\n",
       " {'originalContent': 'Beyond certain tends Plotting connect dots, turns exactly view quantity Py1x,0,1,',\n",
       "  'correctedSentence': 'Beyond a certain point, plotting the connect-the-dots turns exactly into the view of the quantity Py1x, which is 0 or 1.'},\n",
       " {'originalContent': 'find form, Py1x,,0,111expTx appropriate ,,0,1.',\n",
       "  'correctedSentence': 'We find that the form of Py1x is given by the expression 1 / (1 + exp(-Tx)), which is appropriate for 0 or 1.'},\n",
       " {'originalContent': 'Py1x,,0,1.', 'correctedSentence': 'Py1x is 0 or 1.'},\n",
       " {'originalContent': 'convention redefining xis righthandside n1 dimensional adding coordinate xi01.',\n",
       "  'correctedSentence': 'By convention, we redefine the xis to be on the right-hand side, adding a coordinate xi0 = 1.'},\n",
       " {'originalContent': '1x.', 'correctedSentence': 'This is 1 times x.'},\n",
       " {'originalContent': 'While hypothesis function, hood specific choice choosing quite leading',\n",
       "  'correctedSentence': 'While the hypothesis function is a specific choice, choosing it is quite leading.'},\n",
       " {'originalContent': 'Would Prefer One Another?',\n",
       "  'correctedSentence': 'Would you prefer one or the other?'},\n",
       " {'originalContent': 'general, yield boundaries trained',\n",
       "  'correctedSentence': 'In general, the boundaries yielded by the trained models'},\n",
       " {'originalContent': 'Which better?',\n",
       "  'correctedSentence': 'Which one is better?'},\n",
       " {'originalContent': 'discuss superior viceversa.',\n",
       "  'correctedSentence': 'We discuss which one is superior, and vice versa.'},\n",
       " {'originalContent': 'assumes that, xy0 xy1 parameter logistic, governed Py1x11expTx',\n",
       "  'correctedSentence': 'It assumes that xy0 and xy1 are parameters of the logistic function, governed by Py1x = 1 / (1 + exp(-Tx)).'},\n",
       " {'originalContent': 'element x0, raised plotting pointbypoint ultimately yielded curve, shared necessarily assumptions',\n",
       "  'correctedSentence': 'The element x0 is raised to the power of the plotting point-by-point, ultimately yielding a curve that shares the necessary assumptions.'},\n",
       " {'originalContent': '3. converse, implication does Gaussian.',\n",
       "  'correctedSentence': 'The converse is also true, and the implication is that the Gaussian distribution'},\n",
       " {'originalContent': '3 2. stronger prove 2, noted property GDA.',\n",
       "  'correctedSentence': '3 implies 2, and this is a stronger result. We prove 2, and note that this is a property of GDA.'},\n",
       " {'originalContent': 'correct, better regression.',\n",
       "  'correctedSentence': 'This is correct, and better than regression.'},\n",
       " {'originalContent': 'Specifically, indeed asymptotically efficient.',\n",
       "  'correctedSentence': 'Specifically, it is indeed asymptotically efficient.'},\n",
       " {'originalContent': 'strong youre baking information algorithm, Informally, limit large sets m, there no strictly terms of, accurately',\n",
       "  'correctedSentence': \"This is a strong result, and you're baking information into the algorithm. Informally, in the limit of large sets m, there is no strictly better algorithm in terms of accuracy.\"}]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the <b> corrected </b> text\n"
     ]
    }
   ],
   "source": [
    "from difflib import ndiff\n",
    "\n",
    "def highlight_changes(original, corrected):\n",
    "    diff = list(ndiff(original.split(), corrected.split()))\n",
    "    result = []\n",
    "    in_change = False\n",
    "\n",
    "    for d in diff:\n",
    "        if d.startswith('- '):\n",
    "            continue\n",
    "        elif d.startswith('+ '):\n",
    "            if not in_change:\n",
    "                result.append('<b>')\n",
    "                in_change = True\n",
    "            result.append(d[2:])\n",
    "        elif d.startswith('  '):\n",
    "            if in_change:\n",
    "                result.append('</b>')\n",
    "                in_change = False\n",
    "            result.append(d[2:])\n",
    "    \n",
    "    if in_change:\n",
    "        result.append('</b>')\n",
    "\n",
    "    return ' '.join(result)\n",
    "\n",
    "# Test\n",
    "original_text = \"This is the original text\"\n",
    "corrected_text = \"This is the corrected text\"\n",
    "print(highlight_changes(original_text, corrected_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Back to Top CS229 Gaussian Discriminant Analysis Generative Learning Algorithms <b> We've </b> mainly looked at learning algorithms that model Pyx, the conditional distribution of y given x.\",\n",
       " 'For instance, logistic regression <b> modelled </b> Pyx as hxgTx where g is sigmoid function.',\n",
       " \"In this section, <b> we'll </b> talk about a different type <b> of </b> algorithm.\",\n",
       " \"<b> Let's </b> use binary classification problem motivation behind our discussion.\",\n",
       " 'Consider in which we want <b> to </b> learn <b> to </b> distinguish between malignant y1 and benign y0 <b> tumours. </b>',\n",
       " 'Given <b> a </b> training set, an algorithm like , or initially starts with randomly <b> initialised </b> parameters.',\n",
       " 'Over <b> the </b> course <b> of </b> learning, <b> it </b> performs gradient descent <b> on a </b> straight line hyperplane decision <b> boundary, which </b> evolves until <b> it </b> separates <b> positive and negative examples. </b> Then, <b> it classifies a </b> new sample <b> as </b> either <b> benign or malignant, depending on which side </b> it falls in, <b> and </b> makes its prediction accordingly.',\n",
       " \"<b> There's a </b> class <b> that aren't </b> trying <b> to maximise likelihood, </b> looking <b> at </b> both classes <b> and </b> searching for <b> a </b> separation boundary.\",\n",
       " 'Instead, these look <b> at </b> one time.',\n",
       " 'First, <b> tumours </b> can <b> have </b> features <b> that are like other tumours. </b>',\n",
       " '<b> Build </b> separate <b> models for each tumour type. </b> Finally, match <b> the tumour </b> against <b> the tumour </b> model, <b> and </b> see whether <b> it </b> looks more <b> like the tumours in the training </b> set.',\n",
       " '<b> Try to </b> directly <b> map </b> from <b> the </b> input space X <b> to the labels, which </b> are called discriminative algorithms.',\n",
       " '<b> Instead, </b> Pxy <b> = </b> Py.',\n",
       " 'These generative <b> models indicate that, for example, </b> Pxy0 models <b> the features of class 0, </b> while Pxy1 <b> models the features of class 1. </b>',\n",
       " 'The <b> model </b> also learns <b> the </b> prior <b> probability Py, which is </b> independent <b> of </b> y.',\n",
       " 'To illustrate <b> this concept, consider a </b> practical <b> example: </b> when <b> a </b> patient walks into <b> a </b> hospital, before <b> the </b> doctors even <b> see </b> them, <b> the </b> odds <b> of </b> their <b> having a particular disease </b> versus <b> not having it are </b> referred <b> to as the prior probability. </b>',\n",
       " 'Thus, <b> the model </b> builds each <b> in </b> isolation.',\n",
       " 'At test time, <b> the model </b> evaluates <b> the </b> models, identifies <b> which one </b> matches most <b> closely, and </b> returns <b> the </b> prediction.',\n",
       " \"After <b> modelling the </b> priors Pxy, <b> we can </b> derive <b> the </b> posterior <b> probability </b> x <b> using Bayes' rule: Pyx = PxyPyPx </b>\",\n",
       " 'Here, <b> the </b> denominator <b> is given </b> by <b> Px = Pxy1Py1 + Pxy0Py0, which is a </b> function <b> of the </b> quantities',\n",
       " \"Note <b> that the </b> learned part <b> of the </b> process <b> is </b> calculating <b> the </b> order <b> to </b> make <b> a </b> prediction, <b> but we don't </b> actually need <b> to </b> calculate <b> the </b> value <b> of </b> Px since <b> it is a constant and doesn't </b> appear <b> in the final prediction. </b>\",\n",
       " 'When making <b> predictions, algorithms </b> thus ignore computing <b> Px to </b> save computation.',\n",
       " 'However, <b> the </b> end goal <b> is to compute the value of Px, which </b> would <b> allow us to normalise the </b> numerator.',\n",
       " '<b> The </b> equation <b> Pyx = PxyPyPx </b> represents <b> the </b> underlying <b> framework. </b>',\n",
       " 'Key <b> takeaways: discriminative models, </b> i.e., output <b> is a function of </b> input.',\n",
       " '<b> In </b> other words, <b> hx is 0 or 1 </b> directly.',\n",
       " '<b> In the case of tumour identification, we </b> may <b> use </b> discriminant analysis <b> (GDA) for continuous-valued </b> classification.',\n",
       " '<b> We </b> assume <b> that the data is </b> distributed according <b> to a </b> multivariate normal distribution.',\n",
       " '<b> We will </b> briefly <b> discuss the </b> properties <b> of the </b> distributions <b> and then move on to </b> GDA itself.',\n",
       " '<b> A multivariate distribution is a generalisation of a 1-dimensional </b> random variable <b> to an n-dimensional random </b> variable.',\n",
       " '<b> Rather </b> than <b> a </b> univariate variable, <b> it </b> seeks multiple variables.',\n",
       " '<b> We assume that the data is </b> Gaussian, <b> i.e., X ~ Rn, parameterised by a </b> mean vector <b> in </b> Rn <b> and a </b> covariance matrix <b> in </b> Rnn, <b> which is </b> symmetric <b> and </b> positive semidefinite.',\n",
       " 'Formally, <b> this can be </b> written <b> as X ~ N(μ, Σ), where the </b> density PDF <b> is given by PX(x) = (1/√(2π)^n) * exp(-1/2 * (x-μ)^T * Σ^(-1) * (x-μ)), where μ is the mean vector, Σ is the covariance matrix, and E[x] is the </b> expected <b> value of x. </b>',\n",
       " 'Covariance <b> generalises the </b> notion <b> of </b> variance <b> in the real-valued </b> setting.',\n",
       " '<b> E[E^T] = E[E]E^T. </b>',\n",
       " 'Since <b> Cov(X) = E[(X-E[X])(X-E[X])^T], </b>',\n",
       " \"<b> Let's </b> explore some <b> visualisations. </b> Recall <b> the </b> familiar\",\n",
       " 'Similarly, <b> a multivariate distribution can be </b> represented <b> by the </b> same <b> bell-shaped curve, but with </b> two parameters <b> controlling the PDF in n dimensions. </b>',\n",
       " '<b> For example, a 2-dimensional distribution </b> over 2 <b> variables has a </b> size <b> of 2x2. </b>',\n",
       " '<b> The figure </b> below shows <b> a zero-mean vector, a 2x1 </b> zero <b> vector, and a 2x2 </b> identity matrix.',\n",
       " 'A standard <b> deviation of </b> 0.6I.',\n",
       " '<b> Essentially, the </b> number has <b> been multiplied by 0.6, which has </b> shrunk <b> the variance and </b> reduced <b> the variability. </b> 2I.',\n",
       " 'From <b> the </b> images, <b> we can see that as the covariance matrix </b> becomes larger, <b> the distribution becomes wider and shorter, and as it becomes </b> smaller, <b> the distribution becomes compressed and taller. </b>',\n",
       " 'This <b> is </b> because <b> the area under the curve </b> always integrates <b> to 1, </b> so <b> as the </b> spread <b> increases, the </b> height <b> decreases, and vice versa. The </b> figures show Gaussians corresponding <b> to the matrices, and as the off-diagonal </b> entries <b> increase, the distribution becomes </b> compressed towards <b> the 45° line between x1 and x2. </b>',\n",
       " 'Geometrically speaking, <b> this </b> implies <b> that the </b> variables <b> are </b> positively correlated.',\n",
       " 'We <b> can </b> clearly <b> see the </b> contours <b> of the </b> three <b> densities, which are </b> 3D <b> bumps. The </b> aspect ratio <b> of the </b> image <b> is </b> probably <b> a </b> little bit fatter <b> in some places, but the contours should be perfectly round circles. </b>',\n",
       " \"<b> Here's </b> another set <b> of images, this time with </b> decreasing elements <b> of the </b> matrix, <b> and </b> again, <b> in the </b> opposite direction, <b> along the 135° </b> line.\",\n",
       " 'Again, geometrically, <b> this endows the variables with a </b> negative correlation.',\n",
       " '<b> As we </b> vary <b> the </b> parameters, <b> the distribution tends to </b> form ellipses.',\n",
       " 'By varying <b> the </b> shift <b> of the </b> center around.',\n",
       " 'Another <b> way to visualise the </b> PDFs <b> is to </b> carry out <b> the </b> eigenvectors <b> and </b> points <b> of the </b> principal axes <b> of the </b> ellipse contours.',\n",
       " '<b> The </b> takeaway <b> is that as the density changes, the spread and height </b> change respectively.',\n",
       " '<b> The model we </b> have <b> been considering for this </b> task <b> is </b> expressed <b> in terms of </b> equations <b> for </b> Pxy1, <b> which are </b> Gaussians.',\n",
       " 'On <b> the other </b> hand, <b> the </b> Bernoulli <b> distribution </b> takes <b> values 0 and 1. </b>',\n",
       " '<b> By the way, xy0 ~ N0, xy1 ~ N1, and y ~ Bernoulli(0, 1), where the classes are represented by </b> vectors <b> of length </b> 1.',\n",
       " 'Put <b> simply, we </b> were assuming <b> that the means are represented by </b>',\n",
       " 'You <b> may </b> not <b> have </b> commonly <b> seen this. </b>',\n",
       " 'More <b> on this in the </b> section below.',\n",
       " '<b> To </b> fit your data, <b> we </b> will define <b> the </b> data.',\n",
       " 'R, <b> 0 in Rn, 1 in Rn, and </b> Rnn.',\n",
       " 'Writing <b> the </b> distributions, <b> Py(y=1) = Pxy0 ~ N(0, 1), Py(y=1) = Pxy1 ~ N(1, 1) </b>',\n",
       " '<b> Using </b> exponential notation similar <b> to earlier, plugging in Pxy0, Pxy1, Py0, and </b> Py1 <b> into the formula, we can </b> easily ascertain <b> the </b> particular example.',\n",
       " 'Suppose <b> we have xi and yi, for i = 1 to m. </b>',\n",
       " '<b> The </b> aforementioned joint <b> likelihood L is given by L = ∏[P(xi, yi) for i = 1 to m] = ∏[P(xi|yi)P(yi) for i = 1 to m] </b>',\n",
       " '<b> There is a </b> big difference <b> between the </b> cost <b> functions. We </b> choose <b> to maximise the likelihood of the data, which is given by L = ∏[P(xi, yi) for i = 1 to m]. </b>',\n",
       " '<b> In </b> linear regression, <b> we generalise </b> Pyx.',\n",
       " '<b> The log-likelihood of the </b> data <b> is </b> simply log L.',\n",
       " '<b> The log-likelihood is given by log L = ∑[log P(xi, yi) for i = 1 to m] = ∑[log P(xi|yi)P(yi) for i = 1 to m] </b>',\n",
       " '<b> Taking the derivative with </b> respect <b> to the parameters and setting it </b> equal <b> to zero, we can </b> solve <b> for the </b> expression <b> that </b> yields <b> the </b> maximum <b> likelihood estimate, which is given by μ = (1/m) * ∑[yi * xi for i = 1 to m] </b>',\n",
       " 'An intuitive explanation <b> of how this maximises the likelihood </b> follows.',\n",
       " '<b> For </b> example, <b> the </b> chance <b> of the </b> next office <b> being occupied is </b> denoted <b> by the </b> bias <b> of a </b> coin <b> toss, which is the </b> fraction <b> of heads. </b> Likewise, <b> we </b> just label <b> y as 1. </b>',\n",
       " '<b> We can </b> write <b> this in </b> indicator <b> notation as 1(yi = 1) = </b> 1 <b> if the </b> argument <b> is </b> true, <b> and 0 otherwise. </b>',\n",
       " '<b> This is equivalent to a </b> true statement <b> being </b> false <b> in an if-statement in a </b> programming context.',\n",
       " '<b> 0 or 1 is given by 0 if yi = 0, and 1 if yi = 1. </b>',\n",
       " '<b> The </b> intuition <b> behind this expression is to </b> think <b> about </b> all <b> the data points in the </b> dataset.',\n",
       " '<b> A </b> reasonable <b> example is to take the average of the </b> 0s <b> and 1s in the dataset. </b>',\n",
       " '<b> The </b> numerator <b> is the </b> sum <b> of the </b> feature sums <b> over the </b> entire <b> dataset, </b> summing <b> over all </b> instances <b> where yi = 0, and using the instances where yi = 1 </b> times xi.',\n",
       " '<b> This has the </b> effect <b> of </b> zeroing <b> out the </b> term.',\n",
       " '<b> This is because we are summing over all </b> samples <b> where yi = 0. </b>',\n",
       " '<b> This is because </b> every <b> time yi = 0, we </b> get <b> an </b> extra sum, <b> which </b> ends <b> up being the </b> total <b> number of </b> examples.',\n",
       " '<b> This is </b> made <b> up of 1/m times the sum of xi times yi, which </b> fits <b> the </b> means <b> of the </b> classes.',\n",
       " 'Making <b> predictions using the minimum of z, where </b> z <b> is </b> any possible <b> value. </b>',\n",
       " '<b> The </b> argmin <b> of z </b> refers <b> to the value of z that minimises the </b> expression.',\n",
       " '<b> The minimum of z is </b> attained <b> when z = 5, and the argmin of z is 5. </b>',\n",
       " \"<b> The max of z and the argmax of z are </b> operators <b> that exist, and </b> they deal <b> with the maximum value of z. Let's see </b> how <b> to </b> go <b> about finding the maximum value of z. </b>\",\n",
       " '<b> So, </b> do <b> we predict </b> benign?',\n",
       " '<b> This </b> boils down <b> to </b> predicting <b> the most </b> likely <b> value of </b> x, <b> which is given by the argmax of Pyx. </b>',\n",
       " 'Looking <b> deeper, we can see that the rule for predicting y is given by the argmax of Pyx, which is equal to the argmax of PxyPyPx. </b>',\n",
       " '<b> The </b> reasons <b> for this are </b> highlighted <b> in the visualisation of the dataset, which shows how the model operates. </b>',\n",
       " 'Pictorially, <b> the </b> operation <b> of GDA is shown to be a </b> shape <b> and orientation that </b> share <b> a </b> point, <b> which determines the decision boundary. This implies that the </b> blue <b> line is the decision boundary, and Py1x = 0.5. </b>',\n",
       " '<b> The boundary is used to </b> predict <b> the </b> outcome, <b> and the side of the boundary determines the value of y. </b>',\n",
       " 'Points <b> in the </b> upper right <b> are </b> closer <b> to being classified as y = 1, while points in the </b> lower left <b> are </b> classified <b> as y = 0. </b>',\n",
       " 'Logistic <b> regression starts with </b> x1 <b> and x2, </b> whose <b> values are randomly initialised. </b>',\n",
       " 'Typically, <b> we initialise the values for the </b> purpose <b> of visualisation, </b> starting off <b> with the values shown. </b>',\n",
       " 'Running <b> a </b> single iteration <b> of the algorithm, we reposition the decision boundary, and after </b> 20 iterations, <b> the algorithm converges, illustrating the </b> green <b> line </b> superimposed <b> on the blue line. </b>',\n",
       " '<b> We </b> arrive <b> at the decision boundary, which is </b> slightly <b> different from the original boundary. </b>',\n",
       " 'Why <b> do we have two separate means and a single matrix? </b>',\n",
       " 'If <b> we have a </b> lot <b> of </b> problems.',\n",
       " 'Choosing <b> a reasonable value will </b> work fine.',\n",
       " \"<b> The </b> catch, though, <b> is that you'll </b> roughly double <b> the number of parameters, and it isn't </b> anymore.\",\n",
       " '<b> The discussion compares the </b> interesting <b> relationship between </b>',\n",
       " \"<b> You've seen the relationship between 0 and 1. </b>\",\n",
       " '<b> The </b> fixed <b> value of Py1x is 0 or 1. </b>',\n",
       " '<b> By the </b> theorem, <b> Pxy1 = Pxy1,0,1,Py1Px,0,1, </b>',\n",
       " '<b> where Pxy1 is the </b> measure <b> of </b> evaluating <b> the </b> optimum probability, <b> and we </b> plot <b> Py1x, which is a </b> simple <b> function of a </b> few <b> variables. </b>',\n",
       " 'Now, <b> we map the X-axis to the </b> bump <b> of </b> PXY0 <b> and </b> PXY1.',\n",
       " '<b> We also </b> note <b> that the </b> split <b> is 50-50 </b> across <b> Py1 = </b> 0.5, <b> which is </b> known <b> as the half-way point. </b>',\n",
       " 'Next, <b> we have Py1x = </b> X.',\n",
       " '<b> The </b> unlabeled <b> points are </b> far <b> away from the X-axis. </b>',\n",
       " '<b> We can </b> infer <b> that the </b> point almost certainly came <b> from the </b> left, generating <b> a </b> datapoint very <b> close to the X-axis. </b>',\n",
       " '<b> We can easily </b> discern <b> that PXY1 = 0.5. </b>',\n",
       " \"<b> On the </b> right, <b> you'd be </b> pretty sure <b> of the </b> class.\",\n",
       " \"<b> If we </b> repeat <b> the exercise, </b> sweeping <b> the </b> datapoints <b> across a </b> dense grid <b> and evaluating the </b> measure, <b> you'll </b> notice <b> that as we </b> approach <b> the </b> midpoint, <b> the value </b> increases <b> to </b> 0.5 <b> and </b> surpasses 0.5.\",\n",
       " 'Beyond <b> a </b> certain <b> point, plotting the connect-the-dots </b> turns exactly <b> into the </b> view <b> of the </b> quantity <b> Py1x, which is 0 or 1. </b>',\n",
       " '<b> We </b> find <b> that the form of Py1x is given by the expression 1 / (1 + exp(-Tx)), which is </b> appropriate <b> for 0 or 1. </b>',\n",
       " '<b> Py1x is 0 or 1. </b>',\n",
       " '<b> By convention, we redefine the </b> xis <b> to be on the right-hand side, </b> adding <b> a </b> coordinate <b> xi0 = 1. </b>',\n",
       " '<b> This is 1 times x. </b>',\n",
       " 'While <b> the </b> hypothesis <b> function is a </b> specific <b> choice, </b> choosing <b> it is </b> quite <b> leading. </b>',\n",
       " 'Would <b> you prefer one or the other? </b>',\n",
       " '<b> In </b> general, <b> the </b> boundaries <b> yielded by the </b> trained <b> models </b>',\n",
       " 'Which <b> one is </b> better?',\n",
       " '<b> We </b> discuss <b> which one is superior, and vice versa. </b>',\n",
       " '<b> It </b> assumes <b> that </b> xy0 <b> and </b> xy1 <b> are parameters of the logistic function, </b> governed <b> by Py1x = 1 / (1 + exp(-Tx)). </b>',\n",
       " '<b> The </b> element <b> x0 is </b> raised <b> to the power of the </b> plotting <b> point-by-point, </b> ultimately <b> yielding a curve that shares the necessary assumptions. </b>',\n",
       " '<b> The converse is also true, and the </b> implication <b> is that the Gaussian distribution </b>',\n",
       " '3 <b> implies 2, and this is a </b> stronger <b> result. We </b> prove 2, <b> and note that this is a </b> property <b> of </b> GDA.',\n",
       " '<b> This is </b> correct, <b> and </b> better <b> than </b> regression.',\n",
       " 'Specifically, <b> it is </b> indeed asymptotically efficient.',\n",
       " \"<b> This is a </b> strong <b> result, and you're </b> baking information <b> into the algorithm. </b> Informally, <b> in the </b> limit <b> of </b> large sets m, there <b> is </b> no strictly <b> better algorithm in </b> terms <b> of accuracy. </b>\"]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highlighted_sentences = [highlight_changes(pair['originalContent'], pair['correctedSentence']) for pair in data]\n",
    "highlighted_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(out_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>originalContent</th>\n",
       "      <th>correctedSentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Back to Top CS229 Gaussian Discriminant Analys...</td>\n",
       "      <td>Back to Top CS229 Gaussian Discriminant Analys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For instance, logistic regression modeled Pyx ...</td>\n",
       "      <td>For instance, logistic regression modelled Pyx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In this section, well talk about a different t...</td>\n",
       "      <td>In this section, we'll talk about a different ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lets use binary classification problem motivat...</td>\n",
       "      <td>Let's use binary classification problem motiva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Consider in which we want learn distinguish be...</td>\n",
       "      <td>Consider in which we want to learn to distingu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>3. converse, implication does Gaussian.</td>\n",
       "      <td>The converse is also true, and the implication...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>3 2. stronger prove 2, noted property GDA.</td>\n",
       "      <td>3 implies 2, and this is a stronger result. We...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>correct, better regression.</td>\n",
       "      <td>This is correct, and better than regression.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Specifically, indeed asymptotically efficient.</td>\n",
       "      <td>Specifically, it is indeed asymptotically effi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>strong youre baking information algorithm, Inf...</td>\n",
       "      <td>This is a strong result, and you're baking inf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       originalContent  \\\n",
       "0    Back to Top CS229 Gaussian Discriminant Analys...   \n",
       "1    For instance, logistic regression modeled Pyx ...   \n",
       "2    In this section, well talk about a different t...   \n",
       "3    Lets use binary classification problem motivat...   \n",
       "4    Consider in which we want learn distinguish be...   \n",
       "..                                                 ...   \n",
       "125            3. converse, implication does Gaussian.   \n",
       "126         3 2. stronger prove 2, noted property GDA.   \n",
       "127                        correct, better regression.   \n",
       "128     Specifically, indeed asymptotically efficient.   \n",
       "129  strong youre baking information algorithm, Inf...   \n",
       "\n",
       "                                     correctedSentence  \n",
       "0    Back to Top CS229 Gaussian Discriminant Analys...  \n",
       "1    For instance, logistic regression modelled Pyx...  \n",
       "2    In this section, we'll talk about a different ...  \n",
       "3    Let's use binary classification problem motiva...  \n",
       "4    Consider in which we want to learn to distingu...  \n",
       "..                                                 ...  \n",
       "125  The converse is also true, and the implication...  \n",
       "126  3 implies 2, and this is a stronger result. We...  \n",
       "127       This is correct, and better than regression.  \n",
       "128  Specifically, it is indeed asymptotically effi...  \n",
       "129  This is a strong result, and you're baking inf...  \n",
       "\n",
       "[130 rows x 2 columns]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['changes'] = highlighted_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['changes'].str.contains('<b>')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_filter(df, column_name):\n",
    "    # Create a temporary column in lowercase for normalization and matching\n",
    "    df['temp_lower'] = df[column_name].str.lower()\n",
    "    \n",
    "    # Pattern to match social media names, specific extensions, and keywords like 'cookies'\n",
    "    pattern = r'\\b(company|facebook|twitter|instagram|linkedin|youtube|\\.com|\\.ad|\\.gov|\\.uk|\\.php|\\.asp|cookies|cookies|statistical|tracking|ads|script|event|policy|Cloudflare|cookie|cfduid|llc|party)\\b'\n",
    "    \n",
    "    # Filter rows where the temporary lowercase column does not contain any of the specified patterns\n",
    "    filtered_df = df[~df['temp_lower'].str.contains(pattern, flags=re.IGNORECASE, regex=True)]\n",
    "    \n",
    "    # Drop the temporary column\n",
    "    filtered_df.drop('temp_lower', axis=1, inplace=True)\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gv/v0q4rb4n3p9_ydb839lgywph0000gq/T/ipykernel_8980/3883782903.py:9: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtered_df = df[~df['temp_lower'].str.contains(pattern, flags=re.IGNORECASE, regex=True)]\n"
     ]
    }
   ],
   "source": [
    "filtered_df = normalize_and_filter(df, 'changes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Back to Top CS229 Gaussian Discriminant Analysis Generative Learning Algorithms <b> We've </b> mainly looked at learning algorithms that model Pyx, the conditional distribution of y given x.\""
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.iloc[0]['changes']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Back to Top CS229 Gaussian Discriminant Analysis Generative Learning Algorithms Weve mainly looked at learning algorithms that model Pyx, the conditional distribution of y given x.'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.iloc[0]['originalContent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"originalContent\":{\"0\":\"Back to Top CS229 Gaussian Discriminant Analysis Generative Learning Algorithms Weve mainly looked at learning algorithms that model Pyx, the conditional distribution of y given x.\",\"1\":\"For instance, logistic regression modeled Pyx as hxgTx where g is sigmoid function.\",\"2\":\"In this section, well talk about a different type algorithm.\",\"3\":\"Lets use binary classification problem motivation behind our discussion.\",\"4\":\"Consider in which we want learn distinguish between malignant y1 and benign y0 tumors.\",\"5\":\"Given training set, an algorithm like , or initially starts with randomly initialized parameters.\",\"6\":\"Over course learning, performs gradient descent straight line hyperplane decision boundary evolves until you obtain separates positivenegative examples case, Then, classify new sample either benign, it checks on side falls in, makes its prediction accordingly.\",\"7\":\"Theres class arent trying maximize likelihood looking both classes searching for separation boundary.\",\"8\":\"Instead, these look one time.\",\"9\":\"First, tumors, can features what tumors like.\",\"10\":\"build separate Finally, tumor, match against tumor model, see whether looks more had seen set.\",\"11\":\"try directly such mapping from input space X labels are called discriminative algorithms.\",\"12\":\"instead Pxy Py.\",\"13\":\"These generative if indicates example 0 1, then Pxy0 models features, while Pxy1 features.\",\"14\":\"The also learns prior Py independent probability y.\",\"15\":\"To illustrate concept using practical when patient walks into hospital, before doctors even them, odds their versus referred prior.\",\"16\":\"Thus, builds each isolation.\",\"17\":\"At test time, evaluates models, identifies matches most closely returns prediction.\",\"18\":\"After modeling priors Pxy, Bayes rule derive posterior x PyxPxyPyPx\",\"19\":\"Here, denominator by PxPxy1Py1Pxy0Py0, function quantities\",\"20\":\"Note learned part process calculating order make prediction, dont actually need calculate value Px since constant, doesnt appear there.\",\"21\":\"When making predictions algorithms, thus ignore computing save computation.\",\"22\":\"However, end goal value, would compute be able normalize numerator.\",\"23\":\"PyxPxyPyPxPxyPy above equation represents underlying framework .\",\"24\":\"Key takeaways Discriminative i.e., output input.\",\"25\":\"other words, hx0,1 directly.\",\"26\":\"class, tumoridentification setting, may case them first discriminant analysis GDA, used continuousvalued say, classification.\",\"27\":\"assume distributed according multivariate normal distribution.\",\"28\":\"briefly properties distributions moving GDA itself.\",\"29\":\"Multivariate Distribution generalization 1dimensional random variable ndimensional simply, nrandom variable.\",\"30\":\"rather than univariate variable, seeks multiple variables.\",\"31\":\"Assume Gaussian, i.e, XRn, parameterized mean vector Rn covariance matrix Rnn, symmetric positive semidefinite.\",\"32\":\"Formally, written as, XN, density PDF PX,12n212exp12XT1X denotes determinant expected ExxPx,dx vectorvalued defined CovX EXET.\",\"33\":\"Covariance generalizes notion variance realvalued setting.\",\"34\":\"EEET.\",\"35\":\"Since CovX.\",\"36\":\"explore some visualize Recall familiar\",\"37\":\"Similarly, multivariable represented same bellshaped curve two parameters control PDF, but ndimensions.\",\"38\":\"e.g., 2dimensional over 2 variables, 2dimensional, size 22.\",\"39\":\"below figure shows zero 21 zerovector I 22 identity matrix.\",\"40\":\"A standard 0.6I.\",\"41\":\"essentially taken multiplied number has shrunk variance, reduced variability 2I.\",\"42\":\"From images, becomes larger, widershorter, smaller, compressedtaller.\",\"43\":\"This because always integrates area under so scales spread vs. height figures show Gaussians corresponding matrices figures, increase offdiagonal entries compressed towards 45 x1x2.\",\"44\":\"Geometrically speaking, implies variables positively correlated.\",\"45\":\"We clearly contours three densities 3D bumps saw above, should perfectly round circles, aspect ratio image probably little bit fatter places being\",\"46\":\"Heres another set along decreasing elements matrix, now again, opposite direction, 135 line.\",\"47\":\"Again, geometrically, endow negative correlation.\",\"48\":\"vary parameters, tend form ellipses.\",\"49\":\"By varying shift center around.\",\"50\":\"Another values I, visualization PDFs follows carry out eigenvectors points principal axes ellipse contours.\",\"51\":\"takeaway As density, change spreadheight respectively.\",\"52\":\"Model have consider task discussion, expressed equations Pxy1, Gaussians.\",\"53\":\"On hand, Bernoulli takes 0,1.\",\"54\":\"by, xy0N0,xy1N1,yBernoulli 0, classes, vectors 1.\",\"55\":\"Put were assuming representing means.\",\"56\":\"You not commonly seen.\",\"57\":\"More section below.\",\"58\":\"fit your data, will define data.\",\"59\":\"R, 0Rn, 1Rn, Rnn.\",\"60\":\"Writing distributions, Pyy11yPxy012n212exp12x0T1x0Pxy112n212exp12x1T1x1\",\"61\":\"exponential notation similar earlier Plugging Pxy0,Pxy1,Py0 Py1 formula easily ascertain particular example.\",\"62\":\"Suppose xi,yimi1.\",\"63\":\"aforementioned going joint L, L,0,1,mi1Pxi,yi,0,1,mi1Pxiyi0,1,Pyi\",\"64\":\"big difference cost functions compared choose ,0,1,, Px,y,0,1,.\",\"65\":\"linear regression, generalized Pyx.\",\"66\":\"loglikelihood data simply log L.\",\"67\":\",0,1,logmi1Pxi,yi,0,1,logmi1Pxiyi0,1,Pyi\",\"68\":\"respect take derivative equal solve expression yields maximum estimate be, mi1yim\",\"69\":\"An intuitive explanation around maximizes follows.\",\"70\":\"example, chance next office denoted bias coin toss fraction heads Likewise, just label y1.\",\"71\":\"way write indicator notation, mi11yi1m 1 argument true, otherwise 0.\",\"72\":\"true statement false equivalent ifstatement programming context.\",\"73\":\"0,1 is, 0mi11yi0ximi11yi01mi11yi1\",\"74\":\"intuition expression, think all say dataset.\",\"75\":\"reasonable examples, 0s average writing intuition.\",\"76\":\"numerator sum feature sums entire summing i1m, uses instances yi0 times xi.\",\"77\":\"effect term whereas effectively zeroing term.\",\"78\":\"up samples yi0.\",\"79\":\"count represent Because every yi0, get extra sum, ends total examples.\",\"80\":\"made 1mmi1xiyixiT fits means classes.\",\"81\":\"Making Predictions Using minz min z minimum any possible z.\",\"82\":\"argminz argmin refers plugged expression.\",\"83\":\"minzz52 attained z520 argminzz52 led z52, z5.\",\"84\":\"maxz argmaxz operators exist they deal leads having lets how go\",\"85\":\"So do benign?\",\"86\":\"boils down predicting likely x, formally argmaxyPyx\",\"87\":\"Looking deeper y0,1 rule, argmaxyPyxargmaxyPxyPyPx\",\"88\":\"reasons highlighted Visualization dataset compareandcontrast operate\",\"89\":\"Pictorially, GDAs operation Shown been shape orientation, share point, determine put together, imply blue Py1x0.5.\",\"90\":\"boundary, predict outcome, side, y0.\",\"91\":\"Points upper right closer classifying lower left classified\",\"92\":\"Logistic Regression x1 x2 start whose randomly.\",\"93\":\"Typically, initialize purpose visualization, starting off shown\",\"94\":\"Running single iteration PYX, repositions 20 iterations, converges illustrates green superimposed blue.\",\"95\":\"arrive slightly boundaries.\",\"96\":\"Why Two Separate Means, Single Matrix?\",\"97\":\"If lot problems.\",\"98\":\"Choosing reasonable, work fine.\",\"99\":\"catch, though, youll roughly double isnt anymore.\",\"100\":\"Discussion Comparing interesting relationship.\",\"101\":\"youve ,0,1,.\",\"102\":\"fixed Py1x,0,1,.\",\"103\":\"theorem, Pxy1,0,1,Pxy1,0,1,Py1Px,0,1,\",\"104\":\"where, Pxy1,0,1, measure evaluating once determined optimum probability, plot Py1x so, simple few\",\"105\":\"Now, Mappingprojecting Xaxis, bump PXY0 PXY1.\",\"106\":\"Were Also, note split 5050 across PY1 0.5, known half\",\"107\":\"Next, PY1X X.\",\"108\":\"unlabeled far Xaxis.\",\"109\":\"infer point almost certainly came left, generating Xaxis datapoint very small.\",\"110\":\"pick Its easy discern PXY10.5.\",\"111\":\"right, youd pretty sure class.\",\"112\":\"repeat exercise sweeping datapoints dense grid evaluate measure, Youll notice coming approach midpoint, increases 0.5 surpasses 0.5.\",\"113\":\"Beyond certain tends Plotting connect dots, turns exactly view quantity Py1x,0,1,\",\"114\":\"find form, Py1x,,0,111expTx appropriate ,,0,1.\",\"115\":\"Py1x,,0,1.\",\"116\":\"convention redefining xis righthandside n1 dimensional adding coordinate xi01.\",\"117\":\"1x.\",\"118\":\"While hypothesis function, hood specific choice choosing quite leading\",\"119\":\"Would Prefer One Another?\",\"120\":\"general, yield boundaries trained\",\"121\":\"Which better?\",\"122\":\"discuss superior viceversa.\",\"123\":\"assumes that, xy0 xy1 parameter logistic, governed Py1x11expTx\",\"124\":\"element x0, raised plotting pointbypoint ultimately yielded curve, shared necessarily assumptions\",\"125\":\"3. converse, implication does Gaussian.\",\"126\":\"3 2. stronger prove 2, noted property GDA.\",\"127\":\"correct, better regression.\",\"128\":\"Specifically, indeed asymptotically efficient.\",\"129\":\"strong youre baking information algorithm, Informally, limit large sets m, there no strictly terms of, accurately\"},\"correctedSentence\":{\"0\":\"Back to Top CS229 Gaussian Discriminant Analysis Generative Learning Algorithms We\\'ve mainly looked at learning algorithms that model Pyx, the conditional distribution of y given x.\",\"1\":\"For instance, logistic regression modelled Pyx as hxgTx where g is sigmoid function.\",\"2\":\"In this section, we\\'ll talk about a different type of algorithm.\",\"3\":\"Let\\'s use binary classification problem motivation behind our discussion.\",\"4\":\"Consider in which we want to learn to distinguish between malignant y1 and benign y0 tumours.\",\"5\":\"Given a training set, an algorithm like , or initially starts with randomly initialised parameters.\",\"6\":\"Over the course of learning, it performs gradient descent on a straight line hyperplane decision boundary, which evolves until it separates positive and negative examples. Then, it classifies a new sample as either benign or malignant, depending on which side it falls in, and makes its prediction accordingly.\",\"7\":\"There\\'s a class that aren\\'t trying to maximise likelihood, looking at both classes and searching for a separation boundary.\",\"8\":\"Instead, these look at one time.\",\"9\":\"First, tumours can have features that are like other tumours.\",\"10\":\"Build separate models for each tumour type. Finally, match the tumour against the tumour model, and see whether it looks more like the tumours in the training set.\",\"11\":\"Try to directly map from the input space X to the labels, which are called discriminative algorithms.\",\"12\":\"Instead, Pxy = Py.\",\"13\":\"These generative models indicate that, for example, Pxy0 models the features of class 0, while Pxy1 models the features of class 1.\",\"14\":\"The model also learns the prior probability Py, which is independent of y.\",\"15\":\"To illustrate this concept, consider a practical example: when a patient walks into a hospital, before the doctors even see them, the odds of their having a particular disease versus not having it are referred to as the prior probability.\",\"16\":\"Thus, the model builds each in isolation.\",\"17\":\"At test time, the model evaluates the models, identifies which one matches most closely, and returns the prediction.\",\"18\":\"After modelling the priors Pxy, we can derive the posterior probability x using Bayes\\' rule: Pyx = PxyPyPx\",\"19\":\"Here, the denominator is given by Px = Pxy1Py1 + Pxy0Py0, which is a function of the quantities\",\"20\":\"Note that the learned part of the process is calculating the order to make a prediction, but we don\\'t actually need to calculate the value of Px since it is a constant and doesn\\'t appear in the final prediction.\",\"21\":\"When making predictions, algorithms thus ignore computing Px to save computation.\",\"22\":\"However, the end goal is to compute the value of Px, which would allow us to normalise the numerator.\",\"23\":\"The equation Pyx = PxyPyPx represents the underlying framework.\",\"24\":\"Key takeaways: discriminative models, i.e., output is a function of input.\",\"25\":\"In other words, hx is 0 or 1 directly.\",\"26\":\"In the case of tumour identification, we may use discriminant analysis (GDA) for continuous-valued classification.\",\"27\":\"We assume that the data is distributed according to a multivariate normal distribution.\",\"28\":\"We will briefly discuss the properties of the distributions and then move on to GDA itself.\",\"29\":\"A multivariate distribution is a generalisation of a 1-dimensional random variable to an n-dimensional random variable.\",\"30\":\"Rather than a univariate variable, it seeks multiple variables.\",\"31\":\"We assume that the data is Gaussian, i.e., X ~ Rn, parameterised by a mean vector in Rn and a covariance matrix in Rnn, which is symmetric and positive semidefinite.\",\"32\":\"Formally, this can be written as X ~ N(\\\\u03bc, \\\\u03a3), where the density PDF is given by PX(x) = (1\\\\/\\\\u221a(2\\\\u03c0)^n) * exp(-1\\\\/2 * (x-\\\\u03bc)^T * \\\\u03a3^(-1) * (x-\\\\u03bc)), where \\\\u03bc is the mean vector, \\\\u03a3 is the covariance matrix, and E[x] is the expected value of x.\",\"33\":\"Covariance generalises the notion of variance in the real-valued setting.\",\"34\":\"E[E^T] = E[E]E^T.\",\"35\":\"Since Cov(X) = E[(X-E[X])(X-E[X])^T],\",\"36\":\"Let\\'s explore some visualisations. Recall the familiar\",\"37\":\"Similarly, a multivariate distribution can be represented by the same bell-shaped curve, but with two parameters controlling the PDF in n dimensions.\",\"38\":\"For example, a 2-dimensional distribution over 2 variables has a size of 2x2.\",\"39\":\"The figure below shows a zero-mean vector, a 2x1 zero vector, and a 2x2 identity matrix.\",\"40\":\"A standard deviation of 0.6I.\",\"41\":\"Essentially, the number has been multiplied by 0.6, which has shrunk the variance and reduced the variability. 2I.\",\"42\":\"From the images, we can see that as the covariance matrix becomes larger, the distribution becomes wider and shorter, and as it becomes smaller, the distribution becomes compressed and taller.\",\"43\":\"This is because the area under the curve always integrates to 1, so as the spread increases, the height decreases, and vice versa. The figures show Gaussians corresponding to the matrices, and as the off-diagonal entries increase, the distribution becomes compressed towards the 45\\\\u00b0 line between x1 and x2.\",\"44\":\"Geometrically speaking, this implies that the variables are positively correlated.\",\"45\":\"We can clearly see the contours of the three densities, which are 3D bumps. The aspect ratio of the image is probably a little bit fatter in some places, but the contours should be perfectly round circles.\",\"46\":\"Here\\'s another set of images, this time with decreasing elements of the matrix, and again, in the opposite direction, along the 135\\\\u00b0 line.\",\"47\":\"Again, geometrically, this endows the variables with a negative correlation.\",\"48\":\"As we vary the parameters, the distribution tends to form ellipses.\",\"49\":\"By varying the shift of the center around.\",\"50\":\"Another way to visualise the PDFs is to carry out the eigenvectors and points of the principal axes of the ellipse contours.\",\"51\":\"The takeaway is that as the density changes, the spread and height change respectively.\",\"52\":\"The model we have been considering for this task is expressed in terms of equations for Pxy1, which are Gaussians.\",\"53\":\"On the other hand, the Bernoulli distribution takes values 0 and 1.\",\"54\":\"By the way, xy0 ~ N0, xy1 ~ N1, and y ~ Bernoulli(0, 1), where the classes are represented by vectors of length 1.\",\"55\":\"Put simply, we were assuming that the means are represented by\",\"56\":\"You may not have commonly seen this.\",\"57\":\"More on this in the section below.\",\"58\":\"To fit your data, we will define the data.\",\"59\":\"R, 0 in Rn, 1 in Rn, and Rnn.\",\"60\":\"Writing the distributions, Py(y=1) = Pxy0 ~ N(0, 1), Py(y=1) = Pxy1 ~ N(1, 1)\",\"61\":\"Using exponential notation similar to earlier, plugging in Pxy0, Pxy1, Py0, and Py1 into the formula, we can easily ascertain the particular example.\",\"62\":\"Suppose we have xi and yi, for i = 1 to m.\",\"63\":\"The aforementioned joint likelihood L is given by L = \\\\u220f[P(xi, yi) for i = 1 to m] = \\\\u220f[P(xi|yi)P(yi) for i = 1 to m]\",\"64\":\"There is a big difference between the cost functions. We choose to maximise the likelihood of the data, which is given by L = \\\\u220f[P(xi, yi) for i = 1 to m].\",\"65\":\"In linear regression, we generalise Pyx.\",\"66\":\"The log-likelihood of the data is simply log L.\",\"67\":\"The log-likelihood is given by log L = \\\\u2211[log P(xi, yi) for i = 1 to m] = \\\\u2211[log P(xi|yi)P(yi) for i = 1 to m]\",\"68\":\"Taking the derivative with respect to the parameters and setting it equal to zero, we can solve for the expression that yields the maximum likelihood estimate, which is given by \\\\u03bc = (1\\\\/m) * \\\\u2211[yi * xi for i = 1 to m]\",\"69\":\"An intuitive explanation of how this maximises the likelihood follows.\",\"70\":\"For example, the chance of the next office being occupied is denoted by the bias of a coin toss, which is the fraction of heads. Likewise, we just label y as 1.\",\"71\":\"We can write this in indicator notation as 1(yi = 1) = 1 if the argument is true, and 0 otherwise.\",\"72\":\"This is equivalent to a true statement being false in an if-statement in a programming context.\",\"73\":\"0 or 1 is given by 0 if yi = 0, and 1 if yi = 1.\",\"74\":\"The intuition behind this expression is to think about all the data points in the dataset.\",\"75\":\"A reasonable example is to take the average of the 0s and 1s in the dataset.\",\"76\":\"The numerator is the sum of the feature sums over the entire dataset, summing over all instances where yi = 0, and using the instances where yi = 1 times xi.\",\"77\":\"This has the effect of zeroing out the term.\",\"78\":\"This is because we are summing over all samples where yi = 0.\",\"79\":\"This is because every time yi = 0, we get an extra sum, which ends up being the total number of examples.\",\"80\":\"This is made up of 1\\\\/m times the sum of xi times yi, which fits the means of the classes.\",\"81\":\"Making predictions using the minimum of z, where z is any possible value.\",\"82\":\"The argmin of z refers to the value of z that minimises the expression.\",\"83\":\"The minimum of z is attained when z = 5, and the argmin of z is 5.\",\"84\":\"The max of z and the argmax of z are operators that exist, and they deal with the maximum value of z. Let\\'s see how to go about finding the maximum value of z.\",\"85\":\"So, do we predict benign?\",\"86\":\"This boils down to predicting the most likely value of x, which is given by the argmax of Pyx.\",\"87\":\"Looking deeper, we can see that the rule for predicting y is given by the argmax of Pyx, which is equal to the argmax of PxyPyPx.\",\"88\":\"The reasons for this are highlighted in the visualisation of the dataset, which shows how the model operates.\",\"89\":\"Pictorially, the operation of GDA is shown to be a shape and orientation that share a point, which determines the decision boundary. This implies that the blue line is the decision boundary, and Py1x = 0.5.\",\"90\":\"The boundary is used to predict the outcome, and the side of the boundary determines the value of y.\",\"91\":\"Points in the upper right are closer to being classified as y = 1, while points in the lower left are classified as y = 0.\",\"92\":\"Logistic regression starts with x1 and x2, whose values are randomly initialised.\",\"93\":\"Typically, we initialise the values for the purpose of visualisation, starting off with the values shown.\",\"94\":\"Running a single iteration of the algorithm, we reposition the decision boundary, and after 20 iterations, the algorithm converges, illustrating the green line superimposed on the blue line.\",\"95\":\"We arrive at the decision boundary, which is slightly different from the original boundary.\",\"96\":\"Why do we have two separate means and a single matrix?\",\"97\":\"If we have a lot of problems.\",\"98\":\"Choosing a reasonable value will work fine.\",\"99\":\"The catch, though, is that you\\'ll roughly double the number of parameters, and it isn\\'t anymore.\",\"100\":\"The discussion compares the interesting relationship between\",\"101\":\"You\\'ve seen the relationship between 0 and 1.\",\"102\":\"The fixed value of Py1x is 0 or 1.\",\"103\":\"By the theorem, Pxy1 = Pxy1,0,1,Py1Px,0,1,\",\"104\":\"where Pxy1 is the measure of evaluating the optimum probability, and we plot Py1x, which is a simple function of a few variables.\",\"105\":\"Now, we map the X-axis to the bump of PXY0 and PXY1.\",\"106\":\"We also note that the split is 50-50 across Py1 = 0.5, which is known as the half-way point.\",\"107\":\"Next, we have Py1x = X.\",\"108\":\"The unlabeled points are far away from the X-axis.\",\"109\":\"We can infer that the point almost certainly came from the left, generating a datapoint very close to the X-axis.\",\"110\":\"We can easily discern that PXY1 = 0.5.\",\"111\":\"On the right, you\\'d be pretty sure of the class.\",\"112\":\"If we repeat the exercise, sweeping the datapoints across a dense grid and evaluating the measure, you\\'ll notice that as we approach the midpoint, the value increases to 0.5 and surpasses 0.5.\",\"113\":\"Beyond a certain point, plotting the connect-the-dots turns exactly into the view of the quantity Py1x, which is 0 or 1.\",\"114\":\"We find that the form of Py1x is given by the expression 1 \\\\/ (1 + exp(-Tx)), which is appropriate for 0 or 1.\",\"115\":\"Py1x is 0 or 1.\",\"116\":\"By convention, we redefine the xis to be on the right-hand side, adding a coordinate xi0 = 1.\",\"117\":\"This is 1 times x.\",\"118\":\"While the hypothesis function is a specific choice, choosing it is quite leading.\",\"119\":\"Would you prefer one or the other?\",\"120\":\"In general, the boundaries yielded by the trained models\",\"121\":\"Which one is better?\",\"122\":\"We discuss which one is superior, and vice versa.\",\"123\":\"It assumes that xy0 and xy1 are parameters of the logistic function, governed by Py1x = 1 \\\\/ (1 + exp(-Tx)).\",\"124\":\"The element x0 is raised to the power of the plotting point-by-point, ultimately yielding a curve that shares the necessary assumptions.\",\"125\":\"The converse is also true, and the implication is that the Gaussian distribution\",\"126\":\"3 implies 2, and this is a stronger result. We prove 2, and note that this is a property of GDA.\",\"127\":\"This is correct, and better than regression.\",\"128\":\"Specifically, it is indeed asymptotically efficient.\",\"129\":\"This is a strong result, and you\\'re baking information into the algorithm. Informally, in the limit of large sets m, there is no strictly better algorithm in terms of accuracy.\"},\"changes\":{\"0\":\"Back to Top CS229 Gaussian Discriminant Analysis Generative Learning Algorithms <b> We\\'ve <\\\\/b> mainly looked at learning algorithms that model Pyx, the conditional distribution of y given x.\",\"1\":\"For instance, logistic regression <b> modelled <\\\\/b> Pyx as hxgTx where g is sigmoid function.\",\"2\":\"In this section, <b> we\\'ll <\\\\/b> talk about a different type <b> of <\\\\/b> algorithm.\",\"3\":\"<b> Let\\'s <\\\\/b> use binary classification problem motivation behind our discussion.\",\"4\":\"Consider in which we want <b> to <\\\\/b> learn <b> to <\\\\/b> distinguish between malignant y1 and benign y0 <b> tumours. <\\\\/b>\",\"5\":\"Given <b> a <\\\\/b> training set, an algorithm like , or initially starts with randomly <b> initialised <\\\\/b> parameters.\",\"6\":\"Over <b> the <\\\\/b> course <b> of <\\\\/b> learning, <b> it <\\\\/b> performs gradient descent <b> on a <\\\\/b> straight line hyperplane decision <b> boundary, which <\\\\/b> evolves until <b> it <\\\\/b> separates <b> positive and negative examples. <\\\\/b> Then, <b> it classifies a <\\\\/b> new sample <b> as <\\\\/b> either <b> benign or malignant, depending on which side <\\\\/b> it falls in, <b> and <\\\\/b> makes its prediction accordingly.\",\"7\":\"<b> There\\'s a <\\\\/b> class <b> that aren\\'t <\\\\/b> trying <b> to maximise likelihood, <\\\\/b> looking <b> at <\\\\/b> both classes <b> and <\\\\/b> searching for <b> a <\\\\/b> separation boundary.\",\"8\":\"Instead, these look <b> at <\\\\/b> one time.\",\"9\":\"First, <b> tumours <\\\\/b> can <b> have <\\\\/b> features <b> that are like other tumours. <\\\\/b>\",\"10\":\"<b> Build <\\\\/b> separate <b> models for each tumour type. <\\\\/b> Finally, match <b> the tumour <\\\\/b> against <b> the tumour <\\\\/b> model, <b> and <\\\\/b> see whether <b> it <\\\\/b> looks more <b> like the tumours in the training <\\\\/b> set.\",\"11\":\"<b> Try to <\\\\/b> directly <b> map <\\\\/b> from <b> the <\\\\/b> input space X <b> to the labels, which <\\\\/b> are called discriminative algorithms.\",\"12\":\"<b> Instead, <\\\\/b> Pxy <b> = <\\\\/b> Py.\",\"13\":\"These generative <b> models indicate that, for example, <\\\\/b> Pxy0 models <b> the features of class 0, <\\\\/b> while Pxy1 <b> models the features of class 1. <\\\\/b>\",\"14\":\"The <b> model <\\\\/b> also learns <b> the <\\\\/b> prior <b> probability Py, which is <\\\\/b> independent <b> of <\\\\/b> y.\",\"15\":\"To illustrate <b> this concept, consider a <\\\\/b> practical <b> example: <\\\\/b> when <b> a <\\\\/b> patient walks into <b> a <\\\\/b> hospital, before <b> the <\\\\/b> doctors even <b> see <\\\\/b> them, <b> the <\\\\/b> odds <b> of <\\\\/b> their <b> having a particular disease <\\\\/b> versus <b> not having it are <\\\\/b> referred <b> to as the prior probability. <\\\\/b>\",\"16\":\"Thus, <b> the model <\\\\/b> builds each <b> in <\\\\/b> isolation.\",\"17\":\"At test time, <b> the model <\\\\/b> evaluates <b> the <\\\\/b> models, identifies <b> which one <\\\\/b> matches most <b> closely, and <\\\\/b> returns <b> the <\\\\/b> prediction.\",\"18\":\"After <b> modelling the <\\\\/b> priors Pxy, <b> we can <\\\\/b> derive <b> the <\\\\/b> posterior <b> probability <\\\\/b> x <b> using Bayes\\' rule: Pyx = PxyPyPx <\\\\/b>\",\"19\":\"Here, <b> the <\\\\/b> denominator <b> is given <\\\\/b> by <b> Px = Pxy1Py1 + Pxy0Py0, which is a <\\\\/b> function <b> of the <\\\\/b> quantities\",\"20\":\"Note <b> that the <\\\\/b> learned part <b> of the <\\\\/b> process <b> is <\\\\/b> calculating <b> the <\\\\/b> order <b> to <\\\\/b> make <b> a <\\\\/b> prediction, <b> but we don\\'t <\\\\/b> actually need <b> to <\\\\/b> calculate <b> the <\\\\/b> value <b> of <\\\\/b> Px since <b> it is a constant and doesn\\'t <\\\\/b> appear <b> in the final prediction. <\\\\/b>\",\"21\":\"When making <b> predictions, algorithms <\\\\/b> thus ignore computing <b> Px to <\\\\/b> save computation.\",\"22\":\"However, <b> the <\\\\/b> end goal <b> is to compute the value of Px, which <\\\\/b> would <b> allow us to normalise the <\\\\/b> numerator.\",\"23\":\"<b> The <\\\\/b> equation <b> Pyx = PxyPyPx <\\\\/b> represents <b> the <\\\\/b> underlying <b> framework. <\\\\/b>\",\"24\":\"Key <b> takeaways: discriminative models, <\\\\/b> i.e., output <b> is a function of <\\\\/b> input.\",\"25\":\"<b> In <\\\\/b> other words, <b> hx is 0 or 1 <\\\\/b> directly.\",\"26\":\"<b> In the case of tumour identification, we <\\\\/b> may <b> use <\\\\/b> discriminant analysis <b> (GDA) for continuous-valued <\\\\/b> classification.\",\"27\":\"<b> We <\\\\/b> assume <b> that the data is <\\\\/b> distributed according <b> to a <\\\\/b> multivariate normal distribution.\",\"28\":\"<b> We will <\\\\/b> briefly <b> discuss the <\\\\/b> properties <b> of the <\\\\/b> distributions <b> and then move on to <\\\\/b> GDA itself.\",\"29\":\"<b> A multivariate distribution is a generalisation of a 1-dimensional <\\\\/b> random variable <b> to an n-dimensional random <\\\\/b> variable.\",\"30\":\"<b> Rather <\\\\/b> than <b> a <\\\\/b> univariate variable, <b> it <\\\\/b> seeks multiple variables.\",\"31\":\"<b> We assume that the data is <\\\\/b> Gaussian, <b> i.e., X ~ Rn, parameterised by a <\\\\/b> mean vector <b> in <\\\\/b> Rn <b> and a <\\\\/b> covariance matrix <b> in <\\\\/b> Rnn, <b> which is <\\\\/b> symmetric <b> and <\\\\/b> positive semidefinite.\",\"32\":\"Formally, <b> this can be <\\\\/b> written <b> as X ~ N(\\\\u03bc, \\\\u03a3), where the <\\\\/b> density PDF <b> is given by PX(x) = (1\\\\/\\\\u221a(2\\\\u03c0)^n) * exp(-1\\\\/2 * (x-\\\\u03bc)^T * \\\\u03a3^(-1) * (x-\\\\u03bc)), where \\\\u03bc is the mean vector, \\\\u03a3 is the covariance matrix, and E[x] is the <\\\\/b> expected <b> value of x. <\\\\/b>\",\"33\":\"Covariance <b> generalises the <\\\\/b> notion <b> of <\\\\/b> variance <b> in the real-valued <\\\\/b> setting.\",\"34\":\"<b> E[E^T] = E[E]E^T. <\\\\/b>\",\"35\":\"Since <b> Cov(X) = E[(X-E[X])(X-E[X])^T], <\\\\/b>\",\"36\":\"<b> Let\\'s <\\\\/b> explore some <b> visualisations. <\\\\/b> Recall <b> the <\\\\/b> familiar\",\"37\":\"Similarly, <b> a multivariate distribution can be <\\\\/b> represented <b> by the <\\\\/b> same <b> bell-shaped curve, but with <\\\\/b> two parameters <b> controlling the PDF in n dimensions. <\\\\/b>\",\"38\":\"<b> For example, a 2-dimensional distribution <\\\\/b> over 2 <b> variables has a <\\\\/b> size <b> of 2x2. <\\\\/b>\",\"39\":\"<b> The figure <\\\\/b> below shows <b> a zero-mean vector, a 2x1 <\\\\/b> zero <b> vector, and a 2x2 <\\\\/b> identity matrix.\",\"40\":\"A standard <b> deviation of <\\\\/b> 0.6I.\",\"41\":\"<b> Essentially, the <\\\\/b> number has <b> been multiplied by 0.6, which has <\\\\/b> shrunk <b> the variance and <\\\\/b> reduced <b> the variability. <\\\\/b> 2I.\",\"42\":\"From <b> the <\\\\/b> images, <b> we can see that as the covariance matrix <\\\\/b> becomes larger, <b> the distribution becomes wider and shorter, and as it becomes <\\\\/b> smaller, <b> the distribution becomes compressed and taller. <\\\\/b>\",\"43\":\"This <b> is <\\\\/b> because <b> the area under the curve <\\\\/b> always integrates <b> to 1, <\\\\/b> so <b> as the <\\\\/b> spread <b> increases, the <\\\\/b> height <b> decreases, and vice versa. The <\\\\/b> figures show Gaussians corresponding <b> to the matrices, and as the off-diagonal <\\\\/b> entries <b> increase, the distribution becomes <\\\\/b> compressed towards <b> the 45\\\\u00b0 line between x1 and x2. <\\\\/b>\",\"44\":\"Geometrically speaking, <b> this <\\\\/b> implies <b> that the <\\\\/b> variables <b> are <\\\\/b> positively correlated.\",\"45\":\"We <b> can <\\\\/b> clearly <b> see the <\\\\/b> contours <b> of the <\\\\/b> three <b> densities, which are <\\\\/b> 3D <b> bumps. The <\\\\/b> aspect ratio <b> of the <\\\\/b> image <b> is <\\\\/b> probably <b> a <\\\\/b> little bit fatter <b> in some places, but the contours should be perfectly round circles. <\\\\/b>\",\"46\":\"<b> Here\\'s <\\\\/b> another set <b> of images, this time with <\\\\/b> decreasing elements <b> of the <\\\\/b> matrix, <b> and <\\\\/b> again, <b> in the <\\\\/b> opposite direction, <b> along the 135\\\\u00b0 <\\\\/b> line.\",\"47\":\"Again, geometrically, <b> this endows the variables with a <\\\\/b> negative correlation.\",\"48\":\"<b> As we <\\\\/b> vary <b> the <\\\\/b> parameters, <b> the distribution tends to <\\\\/b> form ellipses.\",\"49\":\"By varying <b> the <\\\\/b> shift <b> of the <\\\\/b> center around.\",\"50\":\"Another <b> way to visualise the <\\\\/b> PDFs <b> is to <\\\\/b> carry out <b> the <\\\\/b> eigenvectors <b> and <\\\\/b> points <b> of the <\\\\/b> principal axes <b> of the <\\\\/b> ellipse contours.\",\"51\":\"<b> The <\\\\/b> takeaway <b> is that as the density changes, the spread and height <\\\\/b> change respectively.\",\"52\":\"<b> The model we <\\\\/b> have <b> been considering for this <\\\\/b> task <b> is <\\\\/b> expressed <b> in terms of <\\\\/b> equations <b> for <\\\\/b> Pxy1, <b> which are <\\\\/b> Gaussians.\",\"53\":\"On <b> the other <\\\\/b> hand, <b> the <\\\\/b> Bernoulli <b> distribution <\\\\/b> takes <b> values 0 and 1. <\\\\/b>\",\"54\":\"<b> By the way, xy0 ~ N0, xy1 ~ N1, and y ~ Bernoulli(0, 1), where the classes are represented by <\\\\/b> vectors <b> of length <\\\\/b> 1.\",\"55\":\"Put <b> simply, we <\\\\/b> were assuming <b> that the means are represented by <\\\\/b>\",\"56\":\"You <b> may <\\\\/b> not <b> have <\\\\/b> commonly <b> seen this. <\\\\/b>\",\"57\":\"More <b> on this in the <\\\\/b> section below.\",\"58\":\"<b> To <\\\\/b> fit your data, <b> we <\\\\/b> will define <b> the <\\\\/b> data.\",\"59\":\"R, <b> 0 in Rn, 1 in Rn, and <\\\\/b> Rnn.\",\"60\":\"Writing <b> the <\\\\/b> distributions, <b> Py(y=1) = Pxy0 ~ N(0, 1), Py(y=1) = Pxy1 ~ N(1, 1) <\\\\/b>\",\"61\":\"<b> Using <\\\\/b> exponential notation similar <b> to earlier, plugging in Pxy0, Pxy1, Py0, and <\\\\/b> Py1 <b> into the formula, we can <\\\\/b> easily ascertain <b> the <\\\\/b> particular example.\",\"62\":\"Suppose <b> we have xi and yi, for i = 1 to m. <\\\\/b>\",\"63\":\"<b> The <\\\\/b> aforementioned joint <b> likelihood L is given by L = \\\\u220f[P(xi, yi) for i = 1 to m] = \\\\u220f[P(xi|yi)P(yi) for i = 1 to m] <\\\\/b>\",\"64\":\"<b> There is a <\\\\/b> big difference <b> between the <\\\\/b> cost <b> functions. We <\\\\/b> choose <b> to maximise the likelihood of the data, which is given by L = \\\\u220f[P(xi, yi) for i = 1 to m]. <\\\\/b>\",\"65\":\"<b> In <\\\\/b> linear regression, <b> we generalise <\\\\/b> Pyx.\",\"66\":\"<b> The log-likelihood of the <\\\\/b> data <b> is <\\\\/b> simply log L.\",\"67\":\"<b> The log-likelihood is given by log L = \\\\u2211[log P(xi, yi) for i = 1 to m] = \\\\u2211[log P(xi|yi)P(yi) for i = 1 to m] <\\\\/b>\",\"68\":\"<b> Taking the derivative with <\\\\/b> respect <b> to the parameters and setting it <\\\\/b> equal <b> to zero, we can <\\\\/b> solve <b> for the <\\\\/b> expression <b> that <\\\\/b> yields <b> the <\\\\/b> maximum <b> likelihood estimate, which is given by \\\\u03bc = (1\\\\/m) * \\\\u2211[yi * xi for i = 1 to m] <\\\\/b>\",\"69\":\"An intuitive explanation <b> of how this maximises the likelihood <\\\\/b> follows.\",\"70\":\"<b> For <\\\\/b> example, <b> the <\\\\/b> chance <b> of the <\\\\/b> next office <b> being occupied is <\\\\/b> denoted <b> by the <\\\\/b> bias <b> of a <\\\\/b> coin <b> toss, which is the <\\\\/b> fraction <b> of heads. <\\\\/b> Likewise, <b> we <\\\\/b> just label <b> y as 1. <\\\\/b>\",\"71\":\"<b> We can <\\\\/b> write <b> this in <\\\\/b> indicator <b> notation as 1(yi = 1) = <\\\\/b> 1 <b> if the <\\\\/b> argument <b> is <\\\\/b> true, <b> and 0 otherwise. <\\\\/b>\",\"72\":\"<b> This is equivalent to a <\\\\/b> true statement <b> being <\\\\/b> false <b> in an if-statement in a <\\\\/b> programming context.\",\"73\":\"<b> 0 or 1 is given by 0 if yi = 0, and 1 if yi = 1. <\\\\/b>\",\"74\":\"<b> The <\\\\/b> intuition <b> behind this expression is to <\\\\/b> think <b> about <\\\\/b> all <b> the data points in the <\\\\/b> dataset.\",\"75\":\"<b> A <\\\\/b> reasonable <b> example is to take the average of the <\\\\/b> 0s <b> and 1s in the dataset. <\\\\/b>\",\"76\":\"<b> The <\\\\/b> numerator <b> is the <\\\\/b> sum <b> of the <\\\\/b> feature sums <b> over the <\\\\/b> entire <b> dataset, <\\\\/b> summing <b> over all <\\\\/b> instances <b> where yi = 0, and using the instances where yi = 1 <\\\\/b> times xi.\",\"77\":\"<b> This has the <\\\\/b> effect <b> of <\\\\/b> zeroing <b> out the <\\\\/b> term.\",\"78\":\"<b> This is because we are summing over all <\\\\/b> samples <b> where yi = 0. <\\\\/b>\",\"79\":\"<b> This is because <\\\\/b> every <b> time yi = 0, we <\\\\/b> get <b> an <\\\\/b> extra sum, <b> which <\\\\/b> ends <b> up being the <\\\\/b> total <b> number of <\\\\/b> examples.\",\"80\":\"<b> This is <\\\\/b> made <b> up of 1\\\\/m times the sum of xi times yi, which <\\\\/b> fits <b> the <\\\\/b> means <b> of the <\\\\/b> classes.\",\"81\":\"Making <b> predictions using the minimum of z, where <\\\\/b> z <b> is <\\\\/b> any possible <b> value. <\\\\/b>\",\"82\":\"<b> The <\\\\/b> argmin <b> of z <\\\\/b> refers <b> to the value of z that minimises the <\\\\/b> expression.\",\"83\":\"<b> The minimum of z is <\\\\/b> attained <b> when z = 5, and the argmin of z is 5. <\\\\/b>\",\"84\":\"<b> The max of z and the argmax of z are <\\\\/b> operators <b> that exist, and <\\\\/b> they deal <b> with the maximum value of z. Let\\'s see <\\\\/b> how <b> to <\\\\/b> go <b> about finding the maximum value of z. <\\\\/b>\",\"85\":\"<b> So, <\\\\/b> do <b> we predict <\\\\/b> benign?\",\"86\":\"<b> This <\\\\/b> boils down <b> to <\\\\/b> predicting <b> the most <\\\\/b> likely <b> value of <\\\\/b> x, <b> which is given by the argmax of Pyx. <\\\\/b>\",\"87\":\"Looking <b> deeper, we can see that the rule for predicting y is given by the argmax of Pyx, which is equal to the argmax of PxyPyPx. <\\\\/b>\",\"88\":\"<b> The <\\\\/b> reasons <b> for this are <\\\\/b> highlighted <b> in the visualisation of the dataset, which shows how the model operates. <\\\\/b>\",\"89\":\"Pictorially, <b> the <\\\\/b> operation <b> of GDA is shown to be a <\\\\/b> shape <b> and orientation that <\\\\/b> share <b> a <\\\\/b> point, <b> which determines the decision boundary. This implies that the <\\\\/b> blue <b> line is the decision boundary, and Py1x = 0.5. <\\\\/b>\",\"90\":\"<b> The boundary is used to <\\\\/b> predict <b> the <\\\\/b> outcome, <b> and the side of the boundary determines the value of y. <\\\\/b>\",\"91\":\"Points <b> in the <\\\\/b> upper right <b> are <\\\\/b> closer <b> to being classified as y = 1, while points in the <\\\\/b> lower left <b> are <\\\\/b> classified <b> as y = 0. <\\\\/b>\",\"92\":\"Logistic <b> regression starts with <\\\\/b> x1 <b> and x2, <\\\\/b> whose <b> values are randomly initialised. <\\\\/b>\",\"93\":\"Typically, <b> we initialise the values for the <\\\\/b> purpose <b> of visualisation, <\\\\/b> starting off <b> with the values shown. <\\\\/b>\",\"94\":\"Running <b> a <\\\\/b> single iteration <b> of the algorithm, we reposition the decision boundary, and after <\\\\/b> 20 iterations, <b> the algorithm converges, illustrating the <\\\\/b> green <b> line <\\\\/b> superimposed <b> on the blue line. <\\\\/b>\",\"95\":\"<b> We <\\\\/b> arrive <b> at the decision boundary, which is <\\\\/b> slightly <b> different from the original boundary. <\\\\/b>\",\"96\":\"Why <b> do we have two separate means and a single matrix? <\\\\/b>\",\"97\":\"If <b> we have a <\\\\/b> lot <b> of <\\\\/b> problems.\",\"98\":\"Choosing <b> a reasonable value will <\\\\/b> work fine.\",\"99\":\"<b> The <\\\\/b> catch, though, <b> is that you\\'ll <\\\\/b> roughly double <b> the number of parameters, and it isn\\'t <\\\\/b> anymore.\",\"100\":\"<b> The discussion compares the <\\\\/b> interesting <b> relationship between <\\\\/b>\",\"101\":\"<b> You\\'ve seen the relationship between 0 and 1. <\\\\/b>\",\"102\":\"<b> The <\\\\/b> fixed <b> value of Py1x is 0 or 1. <\\\\/b>\",\"103\":\"<b> By the <\\\\/b> theorem, <b> Pxy1 = Pxy1,0,1,Py1Px,0,1, <\\\\/b>\",\"104\":\"<b> where Pxy1 is the <\\\\/b> measure <b> of <\\\\/b> evaluating <b> the <\\\\/b> optimum probability, <b> and we <\\\\/b> plot <b> Py1x, which is a <\\\\/b> simple <b> function of a <\\\\/b> few <b> variables. <\\\\/b>\",\"105\":\"Now, <b> we map the X-axis to the <\\\\/b> bump <b> of <\\\\/b> PXY0 <b> and <\\\\/b> PXY1.\",\"106\":\"<b> We also <\\\\/b> note <b> that the <\\\\/b> split <b> is 50-50 <\\\\/b> across <b> Py1 = <\\\\/b> 0.5, <b> which is <\\\\/b> known <b> as the half-way point. <\\\\/b>\",\"107\":\"Next, <b> we have Py1x = <\\\\/b> X.\",\"108\":\"<b> The <\\\\/b> unlabeled <b> points are <\\\\/b> far <b> away from the X-axis. <\\\\/b>\",\"109\":\"<b> We can <\\\\/b> infer <b> that the <\\\\/b> point almost certainly came <b> from the <\\\\/b> left, generating <b> a <\\\\/b> datapoint very <b> close to the X-axis. <\\\\/b>\",\"110\":\"<b> We can easily <\\\\/b> discern <b> that PXY1 = 0.5. <\\\\/b>\",\"111\":\"<b> On the <\\\\/b> right, <b> you\\'d be <\\\\/b> pretty sure <b> of the <\\\\/b> class.\",\"112\":\"<b> If we <\\\\/b> repeat <b> the exercise, <\\\\/b> sweeping <b> the <\\\\/b> datapoints <b> across a <\\\\/b> dense grid <b> and evaluating the <\\\\/b> measure, <b> you\\'ll <\\\\/b> notice <b> that as we <\\\\/b> approach <b> the <\\\\/b> midpoint, <b> the value <\\\\/b> increases <b> to <\\\\/b> 0.5 <b> and <\\\\/b> surpasses 0.5.\",\"113\":\"Beyond <b> a <\\\\/b> certain <b> point, plotting the connect-the-dots <\\\\/b> turns exactly <b> into the <\\\\/b> view <b> of the <\\\\/b> quantity <b> Py1x, which is 0 or 1. <\\\\/b>\",\"114\":\"<b> We <\\\\/b> find <b> that the form of Py1x is given by the expression 1 \\\\/ (1 + exp(-Tx)), which is <\\\\/b> appropriate <b> for 0 or 1. <\\\\/b>\",\"115\":\"<b> Py1x is 0 or 1. <\\\\/b>\",\"116\":\"<b> By convention, we redefine the <\\\\/b> xis <b> to be on the right-hand side, <\\\\/b> adding <b> a <\\\\/b> coordinate <b> xi0 = 1. <\\\\/b>\",\"117\":\"<b> This is 1 times x. <\\\\/b>\",\"118\":\"While <b> the <\\\\/b> hypothesis <b> function is a <\\\\/b> specific <b> choice, <\\\\/b> choosing <b> it is <\\\\/b> quite <b> leading. <\\\\/b>\",\"119\":\"Would <b> you prefer one or the other? <\\\\/b>\",\"120\":\"<b> In <\\\\/b> general, <b> the <\\\\/b> boundaries <b> yielded by the <\\\\/b> trained <b> models <\\\\/b>\",\"121\":\"Which <b> one is <\\\\/b> better?\",\"122\":\"<b> We <\\\\/b> discuss <b> which one is superior, and vice versa. <\\\\/b>\",\"123\":\"<b> It <\\\\/b> assumes <b> that <\\\\/b> xy0 <b> and <\\\\/b> xy1 <b> are parameters of the logistic function, <\\\\/b> governed <b> by Py1x = 1 \\\\/ (1 + exp(-Tx)). <\\\\/b>\",\"124\":\"<b> The <\\\\/b> element <b> x0 is <\\\\/b> raised <b> to the power of the <\\\\/b> plotting <b> point-by-point, <\\\\/b> ultimately <b> yielding a curve that shares the necessary assumptions. <\\\\/b>\",\"125\":\"<b> The converse is also true, and the <\\\\/b> implication <b> is that the Gaussian distribution <\\\\/b>\",\"126\":\"3 <b> implies 2, and this is a <\\\\/b> stronger <b> result. We <\\\\/b> prove 2, <b> and note that this is a <\\\\/b> property <b> of <\\\\/b> GDA.\",\"127\":\"<b> This is <\\\\/b> correct, <b> and <\\\\/b> better <b> than <\\\\/b> regression.\",\"128\":\"Specifically, <b> it is <\\\\/b> indeed asymptotically efficient.\",\"129\":\"<b> This is a <\\\\/b> strong <b> result, and you\\'re <\\\\/b> baking information <b> into the algorithm. <\\\\/b> Informally, <b> in the <\\\\/b> limit <b> of <\\\\/b> large sets m, there <b> is <\\\\/b> no strictly <b> better algorithm in <\\\\/b> terms <b> of accuracy. <\\\\/b>\"},\"temp_lower\":{\"0\":\"back to top cs229 gaussian discriminant analysis generative learning algorithms <b> we\\'ve <\\\\/b> mainly looked at learning algorithms that model pyx, the conditional distribution of y given x.\",\"1\":\"for instance, logistic regression <b> modelled <\\\\/b> pyx as hxgtx where g is sigmoid function.\",\"2\":\"in this section, <b> we\\'ll <\\\\/b> talk about a different type <b> of <\\\\/b> algorithm.\",\"3\":\"<b> let\\'s <\\\\/b> use binary classification problem motivation behind our discussion.\",\"4\":\"consider in which we want <b> to <\\\\/b> learn <b> to <\\\\/b> distinguish between malignant y1 and benign y0 <b> tumours. <\\\\/b>\",\"5\":\"given <b> a <\\\\/b> training set, an algorithm like , or initially starts with randomly <b> initialised <\\\\/b> parameters.\",\"6\":\"over <b> the <\\\\/b> course <b> of <\\\\/b> learning, <b> it <\\\\/b> performs gradient descent <b> on a <\\\\/b> straight line hyperplane decision <b> boundary, which <\\\\/b> evolves until <b> it <\\\\/b> separates <b> positive and negative examples. <\\\\/b> then, <b> it classifies a <\\\\/b> new sample <b> as <\\\\/b> either <b> benign or malignant, depending on which side <\\\\/b> it falls in, <b> and <\\\\/b> makes its prediction accordingly.\",\"7\":\"<b> there\\'s a <\\\\/b> class <b> that aren\\'t <\\\\/b> trying <b> to maximise likelihood, <\\\\/b> looking <b> at <\\\\/b> both classes <b> and <\\\\/b> searching for <b> a <\\\\/b> separation boundary.\",\"8\":\"instead, these look <b> at <\\\\/b> one time.\",\"9\":\"first, <b> tumours <\\\\/b> can <b> have <\\\\/b> features <b> that are like other tumours. <\\\\/b>\",\"10\":\"<b> build <\\\\/b> separate <b> models for each tumour type. <\\\\/b> finally, match <b> the tumour <\\\\/b> against <b> the tumour <\\\\/b> model, <b> and <\\\\/b> see whether <b> it <\\\\/b> looks more <b> like the tumours in the training <\\\\/b> set.\",\"11\":\"<b> try to <\\\\/b> directly <b> map <\\\\/b> from <b> the <\\\\/b> input space x <b> to the labels, which <\\\\/b> are called discriminative algorithms.\",\"12\":\"<b> instead, <\\\\/b> pxy <b> = <\\\\/b> py.\",\"13\":\"these generative <b> models indicate that, for example, <\\\\/b> pxy0 models <b> the features of class 0, <\\\\/b> while pxy1 <b> models the features of class 1. <\\\\/b>\",\"14\":\"the <b> model <\\\\/b> also learns <b> the <\\\\/b> prior <b> probability py, which is <\\\\/b> independent <b> of <\\\\/b> y.\",\"15\":\"to illustrate <b> this concept, consider a <\\\\/b> practical <b> example: <\\\\/b> when <b> a <\\\\/b> patient walks into <b> a <\\\\/b> hospital, before <b> the <\\\\/b> doctors even <b> see <\\\\/b> them, <b> the <\\\\/b> odds <b> of <\\\\/b> their <b> having a particular disease <\\\\/b> versus <b> not having it are <\\\\/b> referred <b> to as the prior probability. <\\\\/b>\",\"16\":\"thus, <b> the model <\\\\/b> builds each <b> in <\\\\/b> isolation.\",\"17\":\"at test time, <b> the model <\\\\/b> evaluates <b> the <\\\\/b> models, identifies <b> which one <\\\\/b> matches most <b> closely, and <\\\\/b> returns <b> the <\\\\/b> prediction.\",\"18\":\"after <b> modelling the <\\\\/b> priors pxy, <b> we can <\\\\/b> derive <b> the <\\\\/b> posterior <b> probability <\\\\/b> x <b> using bayes\\' rule: pyx = pxypypx <\\\\/b>\",\"19\":\"here, <b> the <\\\\/b> denominator <b> is given <\\\\/b> by <b> px = pxy1py1 + pxy0py0, which is a <\\\\/b> function <b> of the <\\\\/b> quantities\",\"20\":\"note <b> that the <\\\\/b> learned part <b> of the <\\\\/b> process <b> is <\\\\/b> calculating <b> the <\\\\/b> order <b> to <\\\\/b> make <b> a <\\\\/b> prediction, <b> but we don\\'t <\\\\/b> actually need <b> to <\\\\/b> calculate <b> the <\\\\/b> value <b> of <\\\\/b> px since <b> it is a constant and doesn\\'t <\\\\/b> appear <b> in the final prediction. <\\\\/b>\",\"21\":\"when making <b> predictions, algorithms <\\\\/b> thus ignore computing <b> px to <\\\\/b> save computation.\",\"22\":\"however, <b> the <\\\\/b> end goal <b> is to compute the value of px, which <\\\\/b> would <b> allow us to normalise the <\\\\/b> numerator.\",\"23\":\"<b> the <\\\\/b> equation <b> pyx = pxypypx <\\\\/b> represents <b> the <\\\\/b> underlying <b> framework. <\\\\/b>\",\"24\":\"key <b> takeaways: discriminative models, <\\\\/b> i.e., output <b> is a function of <\\\\/b> input.\",\"25\":\"<b> in <\\\\/b> other words, <b> hx is 0 or 1 <\\\\/b> directly.\",\"26\":\"<b> in the case of tumour identification, we <\\\\/b> may <b> use <\\\\/b> discriminant analysis <b> (gda) for continuous-valued <\\\\/b> classification.\",\"27\":\"<b> we <\\\\/b> assume <b> that the data is <\\\\/b> distributed according <b> to a <\\\\/b> multivariate normal distribution.\",\"28\":\"<b> we will <\\\\/b> briefly <b> discuss the <\\\\/b> properties <b> of the <\\\\/b> distributions <b> and then move on to <\\\\/b> gda itself.\",\"29\":\"<b> a multivariate distribution is a generalisation of a 1-dimensional <\\\\/b> random variable <b> to an n-dimensional random <\\\\/b> variable.\",\"30\":\"<b> rather <\\\\/b> than <b> a <\\\\/b> univariate variable, <b> it <\\\\/b> seeks multiple variables.\",\"31\":\"<b> we assume that the data is <\\\\/b> gaussian, <b> i.e., x ~ rn, parameterised by a <\\\\/b> mean vector <b> in <\\\\/b> rn <b> and a <\\\\/b> covariance matrix <b> in <\\\\/b> rnn, <b> which is <\\\\/b> symmetric <b> and <\\\\/b> positive semidefinite.\",\"32\":\"formally, <b> this can be <\\\\/b> written <b> as x ~ n(\\\\u03bc, \\\\u03c3), where the <\\\\/b> density pdf <b> is given by px(x) = (1\\\\/\\\\u221a(2\\\\u03c0)^n) * exp(-1\\\\/2 * (x-\\\\u03bc)^t * \\\\u03c3^(-1) * (x-\\\\u03bc)), where \\\\u03bc is the mean vector, \\\\u03c3 is the covariance matrix, and e[x] is the <\\\\/b> expected <b> value of x. <\\\\/b>\",\"33\":\"covariance <b> generalises the <\\\\/b> notion <b> of <\\\\/b> variance <b> in the real-valued <\\\\/b> setting.\",\"34\":\"<b> e[e^t] = e[e]e^t. <\\\\/b>\",\"35\":\"since <b> cov(x) = e[(x-e[x])(x-e[x])^t], <\\\\/b>\",\"36\":\"<b> let\\'s <\\\\/b> explore some <b> visualisations. <\\\\/b> recall <b> the <\\\\/b> familiar\",\"37\":\"similarly, <b> a multivariate distribution can be <\\\\/b> represented <b> by the <\\\\/b> same <b> bell-shaped curve, but with <\\\\/b> two parameters <b> controlling the pdf in n dimensions. <\\\\/b>\",\"38\":\"<b> for example, a 2-dimensional distribution <\\\\/b> over 2 <b> variables has a <\\\\/b> size <b> of 2x2. <\\\\/b>\",\"39\":\"<b> the figure <\\\\/b> below shows <b> a zero-mean vector, a 2x1 <\\\\/b> zero <b> vector, and a 2x2 <\\\\/b> identity matrix.\",\"40\":\"a standard <b> deviation of <\\\\/b> 0.6i.\",\"41\":\"<b> essentially, the <\\\\/b> number has <b> been multiplied by 0.6, which has <\\\\/b> shrunk <b> the variance and <\\\\/b> reduced <b> the variability. <\\\\/b> 2i.\",\"42\":\"from <b> the <\\\\/b> images, <b> we can see that as the covariance matrix <\\\\/b> becomes larger, <b> the distribution becomes wider and shorter, and as it becomes <\\\\/b> smaller, <b> the distribution becomes compressed and taller. <\\\\/b>\",\"43\":\"this <b> is <\\\\/b> because <b> the area under the curve <\\\\/b> always integrates <b> to 1, <\\\\/b> so <b> as the <\\\\/b> spread <b> increases, the <\\\\/b> height <b> decreases, and vice versa. the <\\\\/b> figures show gaussians corresponding <b> to the matrices, and as the off-diagonal <\\\\/b> entries <b> increase, the distribution becomes <\\\\/b> compressed towards <b> the 45\\\\u00b0 line between x1 and x2. <\\\\/b>\",\"44\":\"geometrically speaking, <b> this <\\\\/b> implies <b> that the <\\\\/b> variables <b> are <\\\\/b> positively correlated.\",\"45\":\"we <b> can <\\\\/b> clearly <b> see the <\\\\/b> contours <b> of the <\\\\/b> three <b> densities, which are <\\\\/b> 3d <b> bumps. the <\\\\/b> aspect ratio <b> of the <\\\\/b> image <b> is <\\\\/b> probably <b> a <\\\\/b> little bit fatter <b> in some places, but the contours should be perfectly round circles. <\\\\/b>\",\"46\":\"<b> here\\'s <\\\\/b> another set <b> of images, this time with <\\\\/b> decreasing elements <b> of the <\\\\/b> matrix, <b> and <\\\\/b> again, <b> in the <\\\\/b> opposite direction, <b> along the 135\\\\u00b0 <\\\\/b> line.\",\"47\":\"again, geometrically, <b> this endows the variables with a <\\\\/b> negative correlation.\",\"48\":\"<b> as we <\\\\/b> vary <b> the <\\\\/b> parameters, <b> the distribution tends to <\\\\/b> form ellipses.\",\"49\":\"by varying <b> the <\\\\/b> shift <b> of the <\\\\/b> center around.\",\"50\":\"another <b> way to visualise the <\\\\/b> pdfs <b> is to <\\\\/b> carry out <b> the <\\\\/b> eigenvectors <b> and <\\\\/b> points <b> of the <\\\\/b> principal axes <b> of the <\\\\/b> ellipse contours.\",\"51\":\"<b> the <\\\\/b> takeaway <b> is that as the density changes, the spread and height <\\\\/b> change respectively.\",\"52\":\"<b> the model we <\\\\/b> have <b> been considering for this <\\\\/b> task <b> is <\\\\/b> expressed <b> in terms of <\\\\/b> equations <b> for <\\\\/b> pxy1, <b> which are <\\\\/b> gaussians.\",\"53\":\"on <b> the other <\\\\/b> hand, <b> the <\\\\/b> bernoulli <b> distribution <\\\\/b> takes <b> values 0 and 1. <\\\\/b>\",\"54\":\"<b> by the way, xy0 ~ n0, xy1 ~ n1, and y ~ bernoulli(0, 1), where the classes are represented by <\\\\/b> vectors <b> of length <\\\\/b> 1.\",\"55\":\"put <b> simply, we <\\\\/b> were assuming <b> that the means are represented by <\\\\/b>\",\"56\":\"you <b> may <\\\\/b> not <b> have <\\\\/b> commonly <b> seen this. <\\\\/b>\",\"57\":\"more <b> on this in the <\\\\/b> section below.\",\"58\":\"<b> to <\\\\/b> fit your data, <b> we <\\\\/b> will define <b> the <\\\\/b> data.\",\"59\":\"r, <b> 0 in rn, 1 in rn, and <\\\\/b> rnn.\",\"60\":\"writing <b> the <\\\\/b> distributions, <b> py(y=1) = pxy0 ~ n(0, 1), py(y=1) = pxy1 ~ n(1, 1) <\\\\/b>\",\"61\":\"<b> using <\\\\/b> exponential notation similar <b> to earlier, plugging in pxy0, pxy1, py0, and <\\\\/b> py1 <b> into the formula, we can <\\\\/b> easily ascertain <b> the <\\\\/b> particular example.\",\"62\":\"suppose <b> we have xi and yi, for i = 1 to m. <\\\\/b>\",\"63\":\"<b> the <\\\\/b> aforementioned joint <b> likelihood l is given by l = \\\\u220f[p(xi, yi) for i = 1 to m] = \\\\u220f[p(xi|yi)p(yi) for i = 1 to m] <\\\\/b>\",\"64\":\"<b> there is a <\\\\/b> big difference <b> between the <\\\\/b> cost <b> functions. we <\\\\/b> choose <b> to maximise the likelihood of the data, which is given by l = \\\\u220f[p(xi, yi) for i = 1 to m]. <\\\\/b>\",\"65\":\"<b> in <\\\\/b> linear regression, <b> we generalise <\\\\/b> pyx.\",\"66\":\"<b> the log-likelihood of the <\\\\/b> data <b> is <\\\\/b> simply log l.\",\"67\":\"<b> the log-likelihood is given by log l = \\\\u2211[log p(xi, yi) for i = 1 to m] = \\\\u2211[log p(xi|yi)p(yi) for i = 1 to m] <\\\\/b>\",\"68\":\"<b> taking the derivative with <\\\\/b> respect <b> to the parameters and setting it <\\\\/b> equal <b> to zero, we can <\\\\/b> solve <b> for the <\\\\/b> expression <b> that <\\\\/b> yields <b> the <\\\\/b> maximum <b> likelihood estimate, which is given by \\\\u03bc = (1\\\\/m) * \\\\u2211[yi * xi for i = 1 to m] <\\\\/b>\",\"69\":\"an intuitive explanation <b> of how this maximises the likelihood <\\\\/b> follows.\",\"70\":\"<b> for <\\\\/b> example, <b> the <\\\\/b> chance <b> of the <\\\\/b> next office <b> being occupied is <\\\\/b> denoted <b> by the <\\\\/b> bias <b> of a <\\\\/b> coin <b> toss, which is the <\\\\/b> fraction <b> of heads. <\\\\/b> likewise, <b> we <\\\\/b> just label <b> y as 1. <\\\\/b>\",\"71\":\"<b> we can <\\\\/b> write <b> this in <\\\\/b> indicator <b> notation as 1(yi = 1) = <\\\\/b> 1 <b> if the <\\\\/b> argument <b> is <\\\\/b> true, <b> and 0 otherwise. <\\\\/b>\",\"72\":\"<b> this is equivalent to a <\\\\/b> true statement <b> being <\\\\/b> false <b> in an if-statement in a <\\\\/b> programming context.\",\"73\":\"<b> 0 or 1 is given by 0 if yi = 0, and 1 if yi = 1. <\\\\/b>\",\"74\":\"<b> the <\\\\/b> intuition <b> behind this expression is to <\\\\/b> think <b> about <\\\\/b> all <b> the data points in the <\\\\/b> dataset.\",\"75\":\"<b> a <\\\\/b> reasonable <b> example is to take the average of the <\\\\/b> 0s <b> and 1s in the dataset. <\\\\/b>\",\"76\":\"<b> the <\\\\/b> numerator <b> is the <\\\\/b> sum <b> of the <\\\\/b> feature sums <b> over the <\\\\/b> entire <b> dataset, <\\\\/b> summing <b> over all <\\\\/b> instances <b> where yi = 0, and using the instances where yi = 1 <\\\\/b> times xi.\",\"77\":\"<b> this has the <\\\\/b> effect <b> of <\\\\/b> zeroing <b> out the <\\\\/b> term.\",\"78\":\"<b> this is because we are summing over all <\\\\/b> samples <b> where yi = 0. <\\\\/b>\",\"79\":\"<b> this is because <\\\\/b> every <b> time yi = 0, we <\\\\/b> get <b> an <\\\\/b> extra sum, <b> which <\\\\/b> ends <b> up being the <\\\\/b> total <b> number of <\\\\/b> examples.\",\"80\":\"<b> this is <\\\\/b> made <b> up of 1\\\\/m times the sum of xi times yi, which <\\\\/b> fits <b> the <\\\\/b> means <b> of the <\\\\/b> classes.\",\"81\":\"making <b> predictions using the minimum of z, where <\\\\/b> z <b> is <\\\\/b> any possible <b> value. <\\\\/b>\",\"82\":\"<b> the <\\\\/b> argmin <b> of z <\\\\/b> refers <b> to the value of z that minimises the <\\\\/b> expression.\",\"83\":\"<b> the minimum of z is <\\\\/b> attained <b> when z = 5, and the argmin of z is 5. <\\\\/b>\",\"84\":\"<b> the max of z and the argmax of z are <\\\\/b> operators <b> that exist, and <\\\\/b> they deal <b> with the maximum value of z. let\\'s see <\\\\/b> how <b> to <\\\\/b> go <b> about finding the maximum value of z. <\\\\/b>\",\"85\":\"<b> so, <\\\\/b> do <b> we predict <\\\\/b> benign?\",\"86\":\"<b> this <\\\\/b> boils down <b> to <\\\\/b> predicting <b> the most <\\\\/b> likely <b> value of <\\\\/b> x, <b> which is given by the argmax of pyx. <\\\\/b>\",\"87\":\"looking <b> deeper, we can see that the rule for predicting y is given by the argmax of pyx, which is equal to the argmax of pxypypx. <\\\\/b>\",\"88\":\"<b> the <\\\\/b> reasons <b> for this are <\\\\/b> highlighted <b> in the visualisation of the dataset, which shows how the model operates. <\\\\/b>\",\"89\":\"pictorially, <b> the <\\\\/b> operation <b> of gda is shown to be a <\\\\/b> shape <b> and orientation that <\\\\/b> share <b> a <\\\\/b> point, <b> which determines the decision boundary. this implies that the <\\\\/b> blue <b> line is the decision boundary, and py1x = 0.5. <\\\\/b>\",\"90\":\"<b> the boundary is used to <\\\\/b> predict <b> the <\\\\/b> outcome, <b> and the side of the boundary determines the value of y. <\\\\/b>\",\"91\":\"points <b> in the <\\\\/b> upper right <b> are <\\\\/b> closer <b> to being classified as y = 1, while points in the <\\\\/b> lower left <b> are <\\\\/b> classified <b> as y = 0. <\\\\/b>\",\"92\":\"logistic <b> regression starts with <\\\\/b> x1 <b> and x2, <\\\\/b> whose <b> values are randomly initialised. <\\\\/b>\",\"93\":\"typically, <b> we initialise the values for the <\\\\/b> purpose <b> of visualisation, <\\\\/b> starting off <b> with the values shown. <\\\\/b>\",\"94\":\"running <b> a <\\\\/b> single iteration <b> of the algorithm, we reposition the decision boundary, and after <\\\\/b> 20 iterations, <b> the algorithm converges, illustrating the <\\\\/b> green <b> line <\\\\/b> superimposed <b> on the blue line. <\\\\/b>\",\"95\":\"<b> we <\\\\/b> arrive <b> at the decision boundary, which is <\\\\/b> slightly <b> different from the original boundary. <\\\\/b>\",\"96\":\"why <b> do we have two separate means and a single matrix? <\\\\/b>\",\"97\":\"if <b> we have a <\\\\/b> lot <b> of <\\\\/b> problems.\",\"98\":\"choosing <b> a reasonable value will <\\\\/b> work fine.\",\"99\":\"<b> the <\\\\/b> catch, though, <b> is that you\\'ll <\\\\/b> roughly double <b> the number of parameters, and it isn\\'t <\\\\/b> anymore.\",\"100\":\"<b> the discussion compares the <\\\\/b> interesting <b> relationship between <\\\\/b>\",\"101\":\"<b> you\\'ve seen the relationship between 0 and 1. <\\\\/b>\",\"102\":\"<b> the <\\\\/b> fixed <b> value of py1x is 0 or 1. <\\\\/b>\",\"103\":\"<b> by the <\\\\/b> theorem, <b> pxy1 = pxy1,0,1,py1px,0,1, <\\\\/b>\",\"104\":\"<b> where pxy1 is the <\\\\/b> measure <b> of <\\\\/b> evaluating <b> the <\\\\/b> optimum probability, <b> and we <\\\\/b> plot <b> py1x, which is a <\\\\/b> simple <b> function of a <\\\\/b> few <b> variables. <\\\\/b>\",\"105\":\"now, <b> we map the x-axis to the <\\\\/b> bump <b> of <\\\\/b> pxy0 <b> and <\\\\/b> pxy1.\",\"106\":\"<b> we also <\\\\/b> note <b> that the <\\\\/b> split <b> is 50-50 <\\\\/b> across <b> py1 = <\\\\/b> 0.5, <b> which is <\\\\/b> known <b> as the half-way point. <\\\\/b>\",\"107\":\"next, <b> we have py1x = <\\\\/b> x.\",\"108\":\"<b> the <\\\\/b> unlabeled <b> points are <\\\\/b> far <b> away from the x-axis. <\\\\/b>\",\"109\":\"<b> we can <\\\\/b> infer <b> that the <\\\\/b> point almost certainly came <b> from the <\\\\/b> left, generating <b> a <\\\\/b> datapoint very <b> close to the x-axis. <\\\\/b>\",\"110\":\"<b> we can easily <\\\\/b> discern <b> that pxy1 = 0.5. <\\\\/b>\",\"111\":\"<b> on the <\\\\/b> right, <b> you\\'d be <\\\\/b> pretty sure <b> of the <\\\\/b> class.\",\"112\":\"<b> if we <\\\\/b> repeat <b> the exercise, <\\\\/b> sweeping <b> the <\\\\/b> datapoints <b> across a <\\\\/b> dense grid <b> and evaluating the <\\\\/b> measure, <b> you\\'ll <\\\\/b> notice <b> that as we <\\\\/b> approach <b> the <\\\\/b> midpoint, <b> the value <\\\\/b> increases <b> to <\\\\/b> 0.5 <b> and <\\\\/b> surpasses 0.5.\",\"113\":\"beyond <b> a <\\\\/b> certain <b> point, plotting the connect-the-dots <\\\\/b> turns exactly <b> into the <\\\\/b> view <b> of the <\\\\/b> quantity <b> py1x, which is 0 or 1. <\\\\/b>\",\"114\":\"<b> we <\\\\/b> find <b> that the form of py1x is given by the expression 1 \\\\/ (1 + exp(-tx)), which is <\\\\/b> appropriate <b> for 0 or 1. <\\\\/b>\",\"115\":\"<b> py1x is 0 or 1. <\\\\/b>\",\"116\":\"<b> by convention, we redefine the <\\\\/b> xis <b> to be on the right-hand side, <\\\\/b> adding <b> a <\\\\/b> coordinate <b> xi0 = 1. <\\\\/b>\",\"117\":\"<b> this is 1 times x. <\\\\/b>\",\"118\":\"while <b> the <\\\\/b> hypothesis <b> function is a <\\\\/b> specific <b> choice, <\\\\/b> choosing <b> it is <\\\\/b> quite <b> leading. <\\\\/b>\",\"119\":\"would <b> you prefer one or the other? <\\\\/b>\",\"120\":\"<b> in <\\\\/b> general, <b> the <\\\\/b> boundaries <b> yielded by the <\\\\/b> trained <b> models <\\\\/b>\",\"121\":\"which <b> one is <\\\\/b> better?\",\"122\":\"<b> we <\\\\/b> discuss <b> which one is superior, and vice versa. <\\\\/b>\",\"123\":\"<b> it <\\\\/b> assumes <b> that <\\\\/b> xy0 <b> and <\\\\/b> xy1 <b> are parameters of the logistic function, <\\\\/b> governed <b> by py1x = 1 \\\\/ (1 + exp(-tx)). <\\\\/b>\",\"124\":\"<b> the <\\\\/b> element <b> x0 is <\\\\/b> raised <b> to the power of the <\\\\/b> plotting <b> point-by-point, <\\\\/b> ultimately <b> yielding a curve that shares the necessary assumptions. <\\\\/b>\",\"125\":\"<b> the converse is also true, and the <\\\\/b> implication <b> is that the gaussian distribution <\\\\/b>\",\"126\":\"3 <b> implies 2, and this is a <\\\\/b> stronger <b> result. we <\\\\/b> prove 2, <b> and note that this is a <\\\\/b> property <b> of <\\\\/b> gda.\",\"127\":\"<b> this is <\\\\/b> correct, <b> and <\\\\/b> better <b> than <\\\\/b> regression.\",\"128\":\"specifically, <b> it is <\\\\/b> indeed asymptotically efficient.\",\"129\":\"<b> this is a <\\\\/b> strong <b> result, and you\\'re <\\\\/b> baking information <b> into the algorithm. <\\\\/b> informally, <b> in the <\\\\/b> limit <b> of <\\\\/b> large sets m, there <b> is <\\\\/b> no strictly <b> better algorithm in <\\\\/b> terms <b> of accuracy. <\\\\/b>\"}}'"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['changes'].str.contains('<b>')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv('a_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(filtered_df['originalContent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "caution_word_list = [\"Best\", \"Specialist\", \"Specialised\", \"Finest\", \"Most experienced\",\n",
    "                                \"Superior\", \"Principle\", \"Expert\", \"Amazing\", \"Speciality\",\n",
    "                                \"leader\", \"leaders\", \"service\", \"implantologist\"\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discuss <b>superior</b> viceversa.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Function to add <b></b> around caution words\n",
    "def highlight_caution_words(sentence):\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(word) for word in caution_word_list) + r')\\b', re.IGNORECASE)\n",
    "    highlighted_sentence = pattern.sub(r'<b>\\1</b>', sentence)\n",
    "    if highlighted_sentence != sentence:\n",
    "        return highlighted_sentence\n",
    "    return None\n",
    "\n",
    "# Apply the function to all sentences and filter out None results\n",
    "highlighted_sentences = [highlight_caution_words(sentence) for sentence in sentences if highlight_caution_words(sentence)]\n",
    "\n",
    "# Print the results\n",
    "for sentence in highlighted_sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
